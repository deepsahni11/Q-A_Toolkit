{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "\n",
    "with open(r\"E:\\\\Internships_19\\\\Internship(Summer_19)\\\\Q&A_Toolkit\\\\Dataset_analysis\\\\SQuAD\\\\glove_word_embeddings.pkl\", \"rb\") as input_file:\n",
    "    emb_matrix = pickle.load(input_file)\n",
    "    \n",
    "names = [\"validation_context\",\"train_context\",\"validation_question\",\"train_question\"]\n",
    "data_dir = \"E:\\\\Internships_19\\\\Internship(Summer_19)\\\\Q&A_Toolkit\\\\Dataset_analysis\\\\SQuAD\\\\\"\n",
    "\n",
    "word_index_padded =[os.path.join(data_dir + name + \"_word_index_padded.pkl\")  for name in names ]\n",
    "\n",
    "with open(word_index_padded[0], \"rb\") as input_file:\n",
    "    validation_context_word_index_padded = pickle.load(input_file)\n",
    "with open(word_index_padded[1], \"rb\") as input_file:\n",
    "    train_context_word_index_padded = pickle.load(input_file)\n",
    "with open(word_index_padded[2], \"rb\") as input_file:\n",
    "    validation_question_word_index_padded = pickle.load(input_file)\n",
    "with open(word_index_padded[3], \"rb\") as input_file:\n",
    "    train_question_word_index_padded = pickle.load(input_file)\n",
    "    \n",
    "validation_context_word_mask = (validation_context_word_index_padded != 0).type(torch.int32) \n",
    "train_context_word_mask = (train_context_word_index_padded != 0).type(torch.int32) \n",
    "validation_question_word_mask = (validation_question_word_index_padded != 0).type(torch.int32) \n",
    "train_question_word_mask = (train_question_word_index_padded != 0).type(torch.int32) \n",
    "\n",
    "\n",
    "def get_pretrained_embedding(embedding_matrix):\n",
    "    embedding = nn.Embedding(*embedding_matrix.shape)\n",
    "    embedding.weight = nn.Parameter(torch.from_numpy(embedding_matrix).float())\n",
    "    embedding.weight.requires_grad = False\n",
    "    return embedding\n",
    "\n",
    "\n",
    "def init_lstm_forget_bias(lstm):\n",
    "    for names in lstm._all_weights:\n",
    "        for name in names:\n",
    "            if name.startswith('bias_'):\n",
    "                # set forget bias to 1\n",
    "                bias = getattr(lstm, name)\n",
    "                n = bias.size(0)\n",
    "                start, end = n // 4, n // 2\n",
    "                bias.data.fill_(0.)\n",
    "                bias.data[start:end].fill_(1.)\n",
    "\n",
    "class Word_Level_Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_dim, embedding_matrix, dropout_ratio):\n",
    "        super(Word_Level_Encoder, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.embedding = get_pretrained_embedding(embedding_matrix)\n",
    "        self.embedding_dim = self.embedding.embedding_dim\n",
    "\n",
    "        # batch_first = True\n",
    "        # Input: has a dimension of B * m * embedding_dim\n",
    "        # Function parameters: input_size, hidden_size, num_layers_of_LSTM = 1(here)\n",
    "        self.encoder = nn.LSTM(self.embedding_dim, hidden_dim, 1, batch_first=True,\n",
    "                              bidirectional=False, dropout=dropout_ratio) \n",
    "                                     \n",
    "        init_lstm_forget_bias(self.encoder)\n",
    "        self.dropout_emb = nn.Dropout(p=dropout_ratio)\n",
    "        \n",
    "        # creates a random vector with size= hidden_dim\n",
    "        self.sentinel = nn.Parameter(torch.rand(hidden_dim,))\n",
    "\n",
    "    def forward(self, word_sequence_indexes, word_sequence_mask):\n",
    "        # stores length of per instance for context/question\n",
    "        length_per_instance = torch.sum(word_sequence_mask, 1)\n",
    "        \n",
    "        # sorts the length_per_instance vector in decreasing order\n",
    "        length_per_instance_sorted, length_per_instance_argsort = torch.sort(length_per_instance, 0, True) \n",
    "        \n",
    "        _, length_per_instance_argsort_argsort = torch.sort(length_per_instance_argsort, 0)\n",
    "        \n",
    "        # selects the word indexes from word_sequences_indexes matrix according to of length_per_instance_argsort\n",
    "        word_sequence_indexes_sorted = torch.index_select(word_sequence_indexes, 0, length_per_instance_argsort)\n",
    "\n",
    "        # returns the word_sequences_embeddings_sorted matrix with the embeddings for each token/word from word_sequence_indexes_sorted\n",
    "        word_sequence_embeddings_sorted = self.embedding(word_sequence_indexes_sorted)\n",
    "        \n",
    "        # All RNN modules accept packed sequences as inputs.\n",
    "        # Input: word_sequence_embeddings_sorted has a dimension of B x m x l (l is the size of the glove_embedding/ pre-trained embedding/embedding_dim)\n",
    "        packed_word_sequence_embeddings_sorted = pack_padded_sequence(word_sequence_embeddings_sorted, length_per_instance_sorted, batch_first=True)\n",
    "        \n",
    "        # nn.LSTM encoder gets an input of pack_padded_sequence of dimensions: B x m x l (l is the embedding_dim)\n",
    "        # since the input was a packed sequence, the output will also be a packed sequence\n",
    "        output, _ = self.encoder(packed_word_sequence_embeddings_sorted)\n",
    "        \n",
    "        # Pads a packed batch of variable length sequences.\n",
    "        # It is an inverse operation to pack_padded_sequence().\n",
    "        output_to_LSTM_padded, _ = pad_packed_sequence(output, batch_first=True)\n",
    "        \n",
    "        # Returns a contiguous tensor containing the same data as self \n",
    "        output_to_LSTM_padded = output_to_LSTM_padded.contiguous()\n",
    "        \n",
    "        # dimension:  B x m x l\n",
    "        output_to_LSTM_padded_sorted = torch.index_select(output_to_LSTM_padded, 0, length_per_instance_argsort_argsort)  \n",
    "        output_to_LSTM_padded_sorted = self.dropout_emb(output_to_LSTM_padded_sorted)\n",
    "\n",
    "        # list() creates a list of elements if an iterable is passed\n",
    "        batch_size, _ = list(word_sequence_mask.size())\n",
    "        \n",
    "        \n",
    "        sentinel_matrix = self.sentinel.unsqueeze(0).expand(batch_size, self.hidden_dim).unsqueeze(1).contiguous()  # B x 1 x l\n",
    "        length_per_instance = length_per_instance.unsqueeze(1).expand(batch_size, self.hidden_dim).unsqueeze(1)\n",
    "\n",
    "        # sentinel to be concatenated to the data\n",
    "        sentinel_zero = torch.zeros(batch_size, 1, self.hidden_dim)\n",
    "        \n",
    "        # copy sentinel vector at the end\n",
    "        output_to_LSTM_padded_sorted_with_sentinel = torch.cat([output_to_LSTM_padded_sorted, sentinel_zero], 1)  # B x (m + 1) x l\n",
    "        \n",
    "        \n",
    "        output_to_LSTM_padded_sorted_with_sentinel = output_to_LSTM_padded_sorted_with_sentinel.scatter_(1, length_per_instance, sentinel_matrix )\n",
    "\n",
    "        return output_to_LSTM_padded_sorted_with_sentinel\n",
    "    \n",
    "    \n",
    "hidden_dim = 300\n",
    "dropout_ratio = 0.2\n",
    "encoder = Word_Level_Encoder(hidden_dim, emb_matrix, dropout_ratio)\n",
    "\n",
    "e = encoder(validation_context_word_index_padded.type(torch.long)[:10],validation_context_word_mask[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicDecoder(nn.Module):\n",
    "    def __init__(self, hidden_dim, maxout_pool_size, max_number_of_iterations, dropout_ratio):\n",
    "        super(DynamicDecoder, self).__init__()\n",
    "        self.max_number_of_iterations = max_number_of_iterations\n",
    "        \n",
    "        # batch_first = True\n",
    "        # Input: has a dimension of B * m * embedding_dim\n",
    "        # Function parameters: input_size, hidden_size, num_layers_of_LSTM = 1(here)\n",
    "        self.decoder = nn.LSTM(4 * hidden_dim, hidden_dim, 1, batch_first=True, bidirectional=False)\n",
    "        init_lstm_forget_bias(self.decoder)\n",
    "\n",
    "        self.maxout_start = MaxOutHighway(hidden_dim, maxout_pool_size, dropout_ratio)\n",
    "        self.maxout_end = MaxOutHighway(hidden_dim, maxout_pool_size, dropout_ratio)\n",
    "\n",
    "    def forward(self, U, document_word_sequence_mask, answer_start,answer_end):\n",
    "        batch_size, max_word_length, _ = list(U.size()) # U has dimension : B x m x 2l\n",
    "\n",
    "        curr_mask_start,  curr_mask_end = None, None\n",
    "        results_mask_start, results_start = [], []\n",
    "        results_mask_end, results_end = [], []\n",
    "        step_losses = []\n",
    "\n",
    "        mask_matrix = (1.0 - document_word_sequence_mask.float()) * (-1e30)\n",
    "        indices = torch.arange(0, batch_size, out=torch.LongTensor(batch_size))\n",
    "\n",
    "        # initialize start_zero, end_zero: these are the initial values of start and end indices\n",
    "        # start_i_minus_1 = the first index for the context/question \n",
    "        # end_i_minus_1 = the last index for the context/question \n",
    "        start_i_minus_1 = torch.zeros(batch_size, ).long()\n",
    "        end_i_minus_1 = torch.sum(document_word_sequence_mask, 1) - 1\n",
    "\n",
    "        \n",
    "\n",
    "        # After every iteration the hidden and current state \n",
    "        # at t = length of the sequence (for the one-directional lstm) will\n",
    "        # be returned by the lstm\n",
    "        # the hidden_state_i(h_i) will serve as an input to next lstm\n",
    "        hidden_and_current_state_i = None\n",
    "        start_target = None\n",
    "        end_target = None\n",
    "        \n",
    "        # this sets the start and end target (ie. the y_label) for an answer\n",
    "        if answer_start is not None:\n",
    "            start_target = answer_start\n",
    "            end_target = answer_end\n",
    "            \n",
    "        # this is just an initialization of u_start\n",
    "        # u_start_i_minus_1 is essentially u_start_zero outside the loop\n",
    "        u_start_i_minus_1 = U[indices, start_i_minus_1, :]  # b x 2l\n",
    "        \n",
    "        # Why do we need an iterative procedure to predict the start and end indices for na answer ? \n",
    "        # Solution: there may exist several intuitive answer spans within the document, each corresponding to a\n",
    "        #local maxima. An iterative technique to select an answer span by alternating between\n",
    "        #predicting the start point and predicting the end point. This iterative procedure allows the model to\n",
    "        #recover from initial local maxima corresponding to incorrect answer spans.\n",
    "        for _ in range(self.max_number_of_iterations):\n",
    "            u_end_i_minus_1 = U[indices, end_i_minus_1, :]  # b x 2l\n",
    "            \n",
    "            # u_concatenated is fed to the lstm\n",
    "            u_concatenated = torch.cat((u_start_i_minus_1, u_end_i_minus_1), 1)  # b x 4l\n",
    "\n",
    "            # the hidden_and_current_state_i = h_i,c_i are essentially hidden and current cell states \n",
    "            # for t = length of the sequence (for the one-directional lstm) after every iteration\n",
    "            lstm_output, hidden_and_current_state_i = self.decoder(u_concatenated.unsqueeze(1), hidden_and_current_state_i)\n",
    "            h_i, c_i = hidden_and_current_state_i\n",
    "\n",
    "            start_i_minus_1, curr_mask_start, step_loss_start = self.maxout_start(h_i, U, curr_mask_start, start_i_minus_1,\n",
    "                                                                u_concatenated, mask_matrix, start_target)\n",
    "            u_start_i_minus_1 = U[indices, start_i_minus_1, :]  # b x 2l\n",
    "            u_concatenated = torch.cat((start_i_minus_1, u_end_i_minus_1), 1)  # b x 4l\n",
    "\n",
    "            end_i_minus_1, curr_mask_end, step_loss_end = self.maxout_end(h_i, U, curr_mask_end, end_i_minus_1,\n",
    "                                                              u_concatenated, mask_matrix, end_target)\n",
    "\n",
    "            if answer_start is not None:\n",
    "                step_loss = step_loss_s + step_loss_e\n",
    "                step_losses.append(step_loss)\n",
    "\n",
    "            results_mask_start.append(curr_mask_start)\n",
    "            results_start.append(start_i_minus_1)\n",
    "            results_mask_end.append(curr_mask_end)\n",
    "            results_end.append(end_i_minus_1)\n",
    "\n",
    "        result_pos_start = torch.sum(torch.stack(results_mask_start, 1), 1).long()\n",
    "        result_pos_start = result_pos_start - 1\n",
    "        index_start = torch.gather(torch.stack(results_start, 1), 1, result_pos_start.unsqueeze(1)).squeeze()\n",
    "\n",
    "        result_pos_end = torch.sum(torch.stack(results_mask_end, 1), 1).long()\n",
    "        result_pos_end = result_pos_end - 1\n",
    "        index_end = torch.gather(torch.stack(results_end, 1), 1, result_pos_end.unsqueeze(1)).squeeze()\n",
    "\n",
    "        loss = None\n",
    "\n",
    "        if span is not None:\n",
    "            sum_losses = torch.sum(torch.stack(step_losses, 1), 1)\n",
    "            batch_avg_loss = sum_losses / self.max_number_of_iterations\n",
    "            loss = torch.mean(batch_avg_loss)\n",
    "\n",
    "        return loss, index_start, index_end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Highway_Maxout_Network(nn.Module):\n",
    "    def __init__(self, hidden_dim, maxout_pool_size, dropout_ratio):\n",
    "        super(Highway_Maxout_Network, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.maxout_pool_size = maxout_pool_size\n",
    "\n",
    "        self.r = nn.Linear(5 * hidden_dim, hidden_dim, bias=False)\n",
    "       \n",
    "\n",
    "        self.max_out_layer1 = nn.Linear(3 * hidden_dim, hidden_dim*maxout_pool_size)\n",
    "        \n",
    "        self.max_out_layer2 = nn.Linear(hidden_dim, hidden_dim*maxout_pool_size)\n",
    "       \n",
    "\n",
    "        self.max_out_layer3 = nn.Linear(2 * hidden_dim, maxout_pool_size)\n",
    "\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, h_i, U, curr_mask_matrix, index_i_minus_1, u_concatenated, mask_matrix, target=None):\n",
    "        batch_size, max_word_length , _ = list(U.size())\n",
    "\n",
    "        # concatenation of ( h_i of dimension = b x l ; u_concatenated of dimension = b x 4l ) along dimension 1 = gives b x 5l\n",
    "        # self.r(b x 5l) ====> b x l (change of vector space)\n",
    "        r = F.tanh(self.r(torch.cat((h_i.view(-1, self.hidden_dim), u_concatenated), 1)))  # b x 5l => b x l\n",
    "       \n",
    "\n",
    "        # hidden_dim = l\n",
    "        r_expanded = r.unsqueeze(1).expand(b, m, self.hidden_dim).contiguous()  # b x m x l\n",
    "\n",
    "        m_t1_input = torch.cat((U, r_expanded), 2).view(-1, 3*self.hidden_dim)  # b*m x 3l\n",
    "\n",
    "        m_t1_output = self.max_out_layer1(m_t1_input)  # b*m x p*l\n",
    "        \n",
    "        m_t1_output_resized, _ = m_t1_output.view(-1, self.hidden_dim, self.maxout_pool_size).max(2) # b*m x l\n",
    "\n",
    "        # m_t2_input =  m_t1_output_resized\n",
    "        m_t2_output = self.max_out_layer2(m_t1_output_resized)  # b*m x l*p\n",
    "        \n",
    "        m_t2_output_resized, _ = m_t2_output.view(-1, self.hidden_dim, self.maxout_pool_size).max(2)  # b*m x l\n",
    "\n",
    "        m_t3_input = torch.cat((m_t1_output_resized, m_t2_output_resized), 1)  # b*m x 2l\n",
    "        alpha = self.max_out_layer3(m_t3_input)  # b * m x p\n",
    "        alpha, _ = alpha.max(1)  # b*m\n",
    "        alpha = alpha.view(-1, m) # b x m\n",
    "\n",
    "        alpha = alpha + mask_matrix  # b x m\n",
    "        alpha = F.log_softmax(alpha, 1)  # b x m\n",
    "        _, indexes_for_max_alpha = torch.max(alpha, dim=1)\n",
    "\n",
    "        if curr_mask_matrix is None:\n",
    "            curr_mask_matrix = (indexes_for_max_alpha == indexes_for_max_alpha)\n",
    "        else:\n",
    "            indexes_for_max_alpha = indexes_for_max_alpha*curr_mask_matrix.long()\n",
    "            index_i_minus_1 = index_i_minus_1*curr_mask_matrix.long()\n",
    "            curr_mask_matrix = (index_i_minus_1 != index_i_minus_1)\n",
    "\n",
    "        step_loss = None\n",
    "\n",
    "        if target is not None:\n",
    "            step_loss = self.loss(alpha, target)\n",
    "            step_loss = step_loss * curr_mask_matrix.float()\n",
    "\n",
    "        return indexes_for_max_alpha, curr_mask_matrix, step_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fusion_BiLSTM(nn.Module):\n",
    "    def __init__(self, hidden_dim, dropout_ratio):\n",
    "        super(FusionBiLSTM, self).__init__()\n",
    "         # batch_first = True\n",
    "        # Input: has a dimension of B * m * embedding_dim\n",
    "        # Function parameters: input_size, hidden_size, num_layers_of_LSTM = 1(here)\n",
    "        self.fusion_bilstm = nn.LSTM(3 * hidden_dim, hidden_dim, 1, batch_first=True,\n",
    "                                     bidirectional=True, dropout=dropout_ratio)\n",
    "        init_lstm_forget_bias(self.fusion_bilstm)\n",
    "        self.dropout = nn.Dropout(p=dropout_ratio)\n",
    "\n",
    "    def forward(self, seq, mask):\n",
    "        \n",
    "        # stores length of per instance for context/question\n",
    "        length_per_instance = torch.sum(word_sequence_mask, 1)\n",
    "        \n",
    "        # sorts the length_per_instance vector in decreasing order\n",
    "        length_per_instance_sorted, length_per_instance_argsort = torch.sort(length_per_instance, 0, True) \n",
    "        \n",
    "        _, length_per_instance_argsort_argsort = torch.sort(length_per_instance_argsort, 0)\n",
    "        \n",
    "        # selects the word indexes from word_sequences_indexes matrix according to of length_per_instance_argsort\n",
    "        word_sequence_embeddings_sorted = torch.index_select(word_sequence_embeddings, 0, length_per_instance_argsort)\n",
    "\n",
    "      \n",
    "        # All RNN modules accept packed sequences as inputs.\n",
    "        # Input: word_sequence_embeddings_sorted has a dimension of B x m x l (l is the size of the glove_embedding/ pre-trained embedding/embedding_dim)\n",
    "        packed_word_sequence_embeddings_sorted = pack_padded_sequence(word_sequence_embeddings_sorted, length_per_instance_sorted, batch_first=True)\n",
    "        \n",
    "        # nn.LSTM encoder gets an input of pack_padded_sequence of dimensions: B x m x l (l is the embedding_dim)\n",
    "        # since the input was a packed sequence, the output will also be a packed sequence\n",
    "        output, _ = self.fusion_bilstm(packed_word_sequence_embeddings_sorted)\n",
    "        \n",
    "        # Pads a packed batch of variable length sequences.\n",
    "        # It is an inverse operation to pack_padded_sequence().\n",
    "        output_to_BiLSTM_padded, _ = pad_packed_sequence(output, batch_first=True)\n",
    "        \n",
    "        # Returns a contiguous tensor containing the same data as self \n",
    "        output_to_BiLSTM_padded = output_to_BiLSTM_padded.contiguous()\n",
    "        \n",
    "        # dimension:  B x m x l\n",
    "        output_to_BiLSTM_padded_sorted = torch.index_select(output_to_BiLSTM_padded, 0, length_per_instance_argsort_argsort)  \n",
    "        output_to_BiLSTM_padded_sorted = self.dropout(output_to_BiLSTM_padded_sorted)\n",
    "\n",
    "        return output_to_BiLSTM_padded_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Coattention_Encoder(nn.Module):\n",
    "    def __init__(self, hidden_dim, maxout_pool_size, embedding_matrix, max_dec_steps, dropout_ratio):\n",
    "        super(Coattention_Encoder, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.encoder = Encoder(hidden_dim, embedding_matrix, dropout_ratio)\n",
    "\n",
    "        ## nn.Linear(input_dim, output_dim)\n",
    "        self.question_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        \n",
    "        self.fusion_bilstm = FusionBiLSTM(hidden_dim, dropout_ratio)\n",
    "        self.decoder = DynamicDecoder(hidden_dim, maxout_pool_size, max_dec_steps, dropout_ratio)\n",
    "        self.dropout = nn.Dropout(p=dropout_ratio)\n",
    "\n",
    "    def forward(self, question_word_sequence_indexes, question_word_sequence_mask, document_word_sequence_indexes, document_word_sequence_mask, span=None):\n",
    "        \n",
    "        ############## m = number of instances in document ;  n= number of instances in question ############################33\n",
    "        Q = self.encoder(question_word_sequence_indexes, question_word_sequence_mask) # B x (n + 1) x l\n",
    "        D = self.encoder(document_word_sequence_indexes, document_word_sequence_mask)  # B x (m + 1) x l\n",
    "\n",
    "        # view function is meant to reshape the tensor.(Similar to reshape function in numpy)\n",
    "        # view( row_size = -1 ,means that number of rows are unknown, column_size)\n",
    "        \n",
    "        \n",
    "        # pass the Q tensor through a non-linearity \n",
    "        Q = F.tanh(self.question_proj(Q.view(-1, self.hidden_dim))).view(Q.size()) #B x (n + 1) x l\n",
    "\n",
    "        ##################################   Co-Attention starts here  #######################################\n",
    "        \n",
    "        ########################################   Step - 1  ##################################################\n",
    "        # transpose(tensor, first_dimension to be transposed, second_dimension to be transposed)\n",
    "        Q_transpose = torch.transpose(Q, 1, 2) #dimension: B x l x (n + 1)\n",
    "        \n",
    "        # Performs a batch matrix-matrix product of matrices stored in batch1 and batch2.\n",
    "        # batch1 and batch2 must be 3-D tensors each containing the same number of matrices.\n",
    "        L = torch.bmm(D, Q_transpose) # dimension of L : B x (m + 1) x (n + 1)\n",
    "\n",
    "        ####################################### Step-2 ######################################################\n",
    "        A_Q = F.softmax(L, dim=2) # B x (m + 1) x (n + 1)\n",
    "\n",
    "\n",
    "        D_transpose = torch.transpose(D, 1, 2) #dimension: B x l x (m + 1)\n",
    "        C_Q = torch.bmm(D_transpose, A_Q) # (B x l x (m + 1)) x (B x (m + 1) x (n + 1)) => B x l x (n + 1)\n",
    "\n",
    "        ####################################### Step-3 #######################################################\n",
    "        L_tranpose = torch.transpose(L,1,2)\n",
    "        A_D = F.softmax(L_tranpose, dim=2)  # B x (n + 1) x (m + 1)\n",
    "        \n",
    "        \n",
    "        # concatenation along dimension=1:(B x l x (n + 1) ; B x l x (n + 1)  -----> B x 2l x (n + 1) ) x (B x (n + 1) x (m + 1)) ====> B x 2l x (m + 1)\n",
    "        C_D = torch.bmm(torch.cat((Q_transpose, C_Q), 1), A_D) # B x 2l x (m + 1)\n",
    "        C_D_transpose = torch.transpose(C_D, 1, 2)  # B x (m + 1) x 2l\n",
    "\n",
    "        \n",
    "        #######################################  Step-4 ##########################################################\n",
    "        #fusion BiLSTM\n",
    "        # concatenation along dimension = 2:  (B x (m + 1) x 2l ; B x (m + 1) x l  -----> B x (m + 1) x 3l )\n",
    "        bi_lstm_input = torch.cat((C_D_transpose, D), 2) # B x (m + 1) x 3l\n",
    "        bi_lstm_input = self.dropout(bi_lstm_input)\n",
    "        \n",
    "        \n",
    "       \n",
    "        U = self.fusion_bilstm(bi_lstm_input, document_word_sequence_mask) # B x m x 2l\n",
    "\n",
    "        loss, index_start, index_end = self.decoder(U, document_word_sequence_mask, answer_start, answer_end)\n",
    "        if answer_start is not None:\n",
    "            return loss, index_start, index_end\n",
    "        else:\n",
    "            return index_start, index_end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
