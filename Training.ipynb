{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "There are two ways of letting the model know your intention i.e do you want to train the model or do you want to use the model to evaluate. In case of model.train() the model knows it has to learn the layers and when we use model.eval() it indicates the model that nothing new is to be learnt and the model is used for testing. model.eval() is also necessary because in pytorch if we are using batchnorm and during test if we want to just pass a single image, pytorch throws an error if model.eval() is not specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tqdm \n",
    "import numpy as np\n",
    "import json\n",
    "import spacy\n",
    "import nltk\n",
    "\n",
    "\n",
    "class Squad_preprocessor():\n",
    "    def __init__(self,tokenizer,data_directory = \"E:\\\\Internships_19\\\\Internship(Summer_19)\\\\Q&A_Toolkit\\\\Dataset_analysis\\\\SQuAD\"):\n",
    "        self.data_directory = data_directory\n",
    "        self.glove_directory = \"E:\\\\Internships_19\\\\Internship(Summer_19)\\\\Q&A_Toolkit\\\\Dataset_analysis\\\\glove.6B\"\n",
    "        self.train_file = \"train_v2.json\"\n",
    "        self.test_file = \"test_v2.json\"\n",
    "        self.out_prefix = \"train\"\n",
    "        self.tokenizer = tokenizer\n",
    "        self.num_train_examples = 0\n",
    "        self.context_lengths = None\n",
    "        self.vocab = {}\n",
    "        \n",
    "    def load_data(self,filename = \"train_v2.json\"):\n",
    "        full_path = os.path.join(self.data_directory,filename)\n",
    "        \n",
    "        with open(full_path) as datafile:\n",
    "            self.data = json.load(datafile)\n",
    "            \n",
    "#         print(len(self.data[\"data\"]))\n",
    "            \n",
    "    def break_file(self, prefix, filename = \"train_v2.json\", count_examples = False):\n",
    "        self.load_data(filename)\n",
    "        self.out_prefix = prefix\n",
    "        \n",
    "        ##### creating data directories for different parts of the data namely:\n",
    "        ## 1) context\n",
    "        ## 2) question\n",
    "        ## 3) answer_text\n",
    "        ## 4) answer_start\n",
    "        ## 5) answer_end\n",
    "      \n",
    "        \n",
    "        ###       the SQuAD dataset has the following layout:\n",
    "        # \"data\" ---> \"title\", \"paragraphs\" \n",
    "        #                            |\n",
    "        #                            -----> \"context\" , \"qas\"\n",
    "        #                                                 |\n",
    "        #                                                 -----> \"answers\", \"id\", \"is_impossible\", \"question\"\n",
    "        #\n",
    "        #    ie. one context has several questions and their respective answers     \n",
    "\n",
    "        \n",
    "#         counter = 0\n",
    "        with open(os.path.join(self.data_directory, self.out_prefix +'.context'), 'w', encoding='utf-8') as context_file, \\\n",
    "             open(os.path.join(self.data_directory, self.out_prefix +'.question'), 'w', encoding='utf-8') as question_file, \\\n",
    "             open(os.path.join(self.data_directory, self.out_prefix + '.answer_text'), 'w', encoding= 'utf-8') as answer_text_file, \\\n",
    "             open(os.path.join(self.data_directory, self.out_prefix + '.answer_start'), 'w', encoding= 'utf-8') as answer_start_file, \\\n",
    "             open(os.path.join(self.data_directory, self.out_prefix + '.answer_end'), 'w', encoding= 'utf-8') as answer_end_file:\n",
    "             \n",
    "                   \n",
    "                    for article_idx in range(len(self.data[\"data\"])):\n",
    "                        paragraphs = self.data[\"data\"][article_idx][\"paragraphs\"] ## all the paragraphs in data directory\n",
    "\n",
    "                        for paragraph_idx in range(len(paragraphs)):\n",
    "                            context = paragraphs[paragraph_idx][\"context\"] ## each context in a given paragraph directory\n",
    "                            context = context.lower()\n",
    "                            context_tokens = self.tokenizer(context)\n",
    "                            context_tokens.insert(0,\"<sos>\")\n",
    "#                             print(context_tokens)\n",
    "# \n",
    "                            ## each context has a range of \"answers\", \"id\", \"is_impossible\", \"question\" \n",
    "\n",
    "                            qas = paragraphs[paragraph_idx][\"qas\"] ##  \"qas\" referrring to a single \"context\"\n",
    "\n",
    "                            for qas_idx in range(len(qas)):  ### disecting the \"qas\" into \"answers\", \"id\", \"is_impossible\", \"question\" \n",
    "                                question = qas[qas_idx][\"question\"]  \n",
    "                                question = question.lower()\n",
    "                                question_tokens = self.tokenizer(question)\n",
    "                                question_tokens.insert(0,\"<sos>\")\n",
    "\n",
    "                                ## we select the first answer id from the range of answers we are given for a particular question\n",
    "                                \n",
    "                                if(len(qas[qas_idx][\"answers\"]) == 0 ):\n",
    "                                    \n",
    "                                    answer_text_tokens = \"<sos>\"\n",
    "                                    word_level_answer_start = 1\n",
    "                                    word_level_answer_end = 1\n",
    "                                    \n",
    "                                    \n",
    "                                    \n",
    "                                else:\n",
    "                                    \n",
    "                                    answer_id = 0\n",
    "                                    answer_text = qas[qas_idx][\"answers\"][answer_id][\"text\"]\n",
    "#                                     print(answer_text)\n",
    "                                    \n",
    "                                    answer_text = answer_text.lower()\n",
    "                                    answer_text_tokens = self.tokenizer(answer_text) ## we atke the first option as the answer\n",
    "\n",
    "                                    char_level_answer_start = qas[qas_idx][\"answers\"][answer_id][\"answer_start\"]\n",
    "                                    word_level_answer_start = len(context[:char_level_answer_start].split())+1\n",
    "                                    word_level_answer_end = word_level_answer_start + len(answer_text.split()) \n",
    "\n",
    "\n",
    "                \n",
    "                                       \n",
    "                                    \n",
    "        \n",
    "                                    \n",
    "        \n",
    "                                    context_file.write(' '.join(token for token in context_tokens)+'\\n')\n",
    "                                    question_file.write(' '.join(token for token in question_tokens)+'\\n')\n",
    "                                    answer_text_file.write(' '.join(token for token in answer_text_tokens)+'\\n')\n",
    "\n",
    "                                    answer_start_file.write(str(word_level_answer_start)+ \"\\n\")\n",
    "                                    answer_end_file.write(str(word_level_answer_end) + \"\\n\")\n",
    "\n",
    "                                    \n",
    "                                                \n",
    "    \n",
    "    def conduct_preprocess(self):\n",
    "        self.break_file(\"train\", self.train_file, True)\n",
    "        self.break_file(\"test\", self.test_file, False)\n",
    "        \n",
    "            \n",
    "            \n",
    "    \n",
    "preprocess = Squad_preprocessor(nltk.word_tokenize,\"E:\\\\Internships_19\\\\Internship(Summer_19)\\\\Q&A_Toolkit\\\\Dataset_analysis\\\\SQuAD\")\n",
    "preprocess.conduct_preprocess()  \n",
    "\n",
    "data_directory = \"E:\\\\Internships_19\\\\Internship(Summer_19)\\\\Q&A_Toolkit\\\\Dataset_analysis\\\\SQuAD\"\n",
    "context_file = open(os.path.join(data_directory, 'train.context'), 'r', encoding='utf-8').readlines() \n",
    "question_file = open(os.path.join(data_directory, 'train.question'), 'r', encoding='utf-8').readlines() \n",
    "answer_text_file = open(os.path.join(data_directory,  'train.answer_text'), 'r', encoding= 'utf-8').readlines()  \n",
    "answer_start_file = open(os.path.join(data_directory,  'train.answer_start'), 'r', encoding= 'utf-8').readlines() \n",
    "answer_end_file = open(os.path.join(data_directory,  'train.answer_end'), 'r', encoding= 'utf-8').readlines() \n",
    "        \n",
    "with open(os.path.join(data_directory, \"validation.context\" ), 'w', encoding='utf-8') as f:\n",
    "    for item in context_file[8000:]:\n",
    "        f.write(\"%s\" % item)\n",
    "with open(os.path.join(data_directory, \"validation.question\" ), 'w', encoding='utf-8') as f:\n",
    "    for item in question_file[8000:]:\n",
    "        f.write(\"%s\" % item)\n",
    "with open(os.path.join(data_directory, \"validation.answer_text\" ), 'w', encoding='utf-8') as f:\n",
    "    for item in answer_text_file[8000:]:\n",
    "        f.write(\"%s\" % item)\n",
    "with open(os.path.join(data_directory, \"validation.answer_start\" ), 'w', encoding='utf-8') as f:\n",
    "    for item in answer_start_file[8000:]:\n",
    "        f.write(\"%s\" % item)\n",
    "with open(os.path.join(data_directory, \"validation.answer_end\" ), 'w', encoding='utf-8') as f:\n",
    "    for item in answer_end_file[8000:]:\n",
    "        f.write(\"%s\" % item)\n",
    "with open(os.path.join(data_directory, \"train.context\"), 'w', encoding='utf-8') as f:\n",
    "    for item in context_file[:8000]:\n",
    "        f.write(\"%s\" % item)\n",
    "with open(os.path.join(data_directory, \"train.question\" ), 'w', encoding='utf-8') as f:\n",
    "    for item in question_file[:8000]:\n",
    "        f.write(\"%s\" % item)\n",
    "with open(os.path.join(data_directory, \"train.answer_text\" ), 'w', encoding='utf-8') as f:\n",
    "    for item in answer_text_file[:8000]:\n",
    "        f.write(\"%s\" % item)\n",
    "with open(os.path.join(data_directory, \"train.answer_start\"), 'w', encoding='utf-8') as f:\n",
    "    for item in answer_start_file[:8000]:\n",
    "        f.write(\"%s\" % item)\n",
    "with open(os.path.join(data_directory, \"train.answer_end\" ), 'w', encoding='utf-8') as f:\n",
    "    for item in answer_end_file[:8000]:\n",
    "        f.write(\"%s\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os.path\n",
    "import operator\n",
    "import pickle\n",
    "from nltk.tokenize import WhitespaceTokenizer \n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from collections import defaultdict\n",
    "from math import sqrt\n",
    "import numpy as np \n",
    "import codecs\n",
    "import re\n",
    "import string\n",
    "import sys\n",
    "import tqdm as tqdm\n",
    "import os\n",
    "from collections import Counter\n",
    "# import Squad_processor\n",
    "import spacy\n",
    "\n",
    "import nltk\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "\n",
    "class Vocabulary():\n",
    "    \n",
    "    def __init__(self, vocab_input_files = [\"E:\\\\Internships_19\\\\Internship(Summer_19)\\\\Q&A_Toolkit\\\\Dataset_analysis\\\\SQuAD\\\\train.context\",\"E:\\\\Internships_19\\\\Internship(Summer_19)\\\\Q&A_Toolkit\\\\Dataset_analysis\\\\SQuAD\\\\train.question\"],\n",
    "                        vocab_output_filename = \"E:\\\\Internships_19\\\\Internship(Summer_19)\\\\Q&A_Toolkit\\\\Dataset_analysis\\\\SQuAD\\\\vocab.dat\"):\n",
    "        \"\"\"\n",
    "        This function works the same as contructors and is used to initilaize the parameters used in the making the model\n",
    "        \n",
    "        \"\"\"\n",
    "        self.vocab = {}\n",
    "        self.vocab_output_filename = vocab_output_filename\n",
    "        self.vocab_input_files = vocab_input_files\n",
    "        self.word_list = []\n",
    "        self.word_to_index = {} # dictionary with keys as words and values as their corresponding index number\n",
    "        self.char_to_index = {} # dictionary with keys as characters and values as their corresponding index number\n",
    "        self.word_to_index[\"<pad>\"] = 0\n",
    "        self.word_to_index[\"<sos>\"] = 1\n",
    "        self.word_to_index[\"<unk>\"] = 2\n",
    "#         self.word_to_index[\"<SOS>\"]\n",
    "        ## self.index_to_word = # dictionary with values as words and keys as their corresponding index number\n",
    "        ## self.index_to_char = # dictionary with values as characters and keys as their corresponding index number\n",
    "        \n",
    "    \n",
    "    def normalize_answer(self,s):\n",
    "        \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
    "\n",
    "        def remove_articles(text):\n",
    "            return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "\n",
    "        def white_space_fix(text):\n",
    "            return ' '.join(text.split())\n",
    "\n",
    "        def remove_punc(text):\n",
    "            exclude = set(string.punctuation)\n",
    "            return ''.join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "        def lower(text):\n",
    "            return text.lower()\n",
    "\n",
    "        return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "    \n",
    "    \n",
    "    def create_vocabulary(self,vocab_freq = 0, vocab_size = 30000, data_path=\"E:\\\\Internships_19\\\\Internship(Summer_19)\\\\Q&A_Toolkit\\\\Dataset_analysis\\\\SQuAD\"):\n",
    "        \"\"\"\n",
    "        This function creates dictionaries namely:\n",
    "        1) word_to_index\n",
    "        2) char_to_index\n",
    "        3) index_to_word\n",
    "        4) index_to_char\n",
    "        \n",
    "        and dumps them into pickle file namely: \"dictionaries.pkl\"\n",
    "        \"\"\"\n",
    "        \n",
    "\n",
    "        for filename in self.vocab_input_files:\n",
    "            with open(filename,'r', encoding = 'utf-8') as file_input:\n",
    "                \n",
    "                for line in file_input:\n",
    "                    words = self.normalize_answer(line).strip().split()\n",
    "#                     print(words)\n",
    "                    for word in words:\n",
    "                        if not (word in self.vocab):\n",
    "                            self.vocab[word] = 1\n",
    "                        else:\n",
    "                            self.vocab[word] +=1 \n",
    "\n",
    "        if vocab_freq == 0:\n",
    "            vocab_words = sorted(self.vocab,key=self.vocab.get,reverse=True)\n",
    "\n",
    "\n",
    "#         print(vocab_words)\n",
    "                    \n",
    "        temp_index = 3\n",
    "        for word in vocab_words:\n",
    "            if temp_index < vocab_size and word not in self.word_to_index:\n",
    "                self.word_to_index[word] = temp_index\n",
    "                temp_index += 1\n",
    "                \n",
    "#         print(len(self.word_to_index))\n",
    "\n",
    "        self.vocab_size = len(self.word_to_index)\n",
    "        self.index_to_word =  {v: k for k, v in self.word_to_index.items()}\n",
    "\n",
    "\n",
    "        characters = list(string.printable.lower())\n",
    "        characters.remove(' ')\n",
    "\n",
    "        char_ind = 1\n",
    "        for c in characters:\n",
    "            if c not in self.char_to_index:\n",
    "                self.char_to_index[c] = char_ind\n",
    "                char_ind += 1\n",
    "\n",
    "\n",
    "        self.index_to_char = {v: k for k,v in self.char_to_index.items()}\n",
    "\n",
    "        dict_all = {\"word_to_index\" : self.word_to_index, \"char_to_index\" : self.char_to_index,\"index_to_word\": self.index_to_word, \"index_to_char\": self.index_to_char}\n",
    "\n",
    "        pickle.dump(dict_all, open(os.path.join(data_path, \"dictionaries.pkl\"), \"wb\")) ## creates dictionaries and stores in memory as pickle files\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "vocab = Vocabulary([\"E:\\\\Internships_19\\\\Internship(Summer_19)\\\\Q&A_Toolkit\\\\Dataset_analysis\\\\SQuAD\\\\train.context\",\"E:\\\\Internships_19\\\\Internship(Summer_19)\\\\Q&A_Toolkit\\\\Dataset_analysis\\\\SQuAD\\\\train.question\"],\n",
    "                        \"E:\\\\Internships_19\\\\Internship(Summer_19)\\\\Q&A_Toolkit\\\\Dataset_analysis\\\\SQuAD\\\\vocab.dat\")\n",
    "vocab.create_vocabulary(0,30000, \"E:\\\\Internships_19\\\\Internship(Summer_19)\\\\Q&A_Toolkit\\\\Dataset_analysis\\\\SQuAD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import tqdm as tqdm\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import spacy\n",
    "# import ujson as json\n",
    "import urllib.request\n",
    "\n",
    "# from args import get_setup_args\n",
    "from codecs import open\n",
    "from collections import Counter\n",
    "from subprocess import run\n",
    "from tqdm import tqdm\n",
    "from zipfile import ZipFile\n",
    "\n",
    "\n",
    "\n",
    "class Embedding_Matrix():\n",
    "    \n",
    "    def __init__(self,embedding_dir = \"E:\\\\Internships_19\\\\Internship(Summer_19)\\\\Q&A_Toolkit\\\\Dataset_analysis\\\\SQuAD\"):\n",
    "#         embedding_dir = \"E:\\\\Internships_19\\\\Internship(Summer_19)\\\\Q&A_Toolkit\\\\Dataset_analysis\\\\SQuAD\"\n",
    "        with open(r\"E:\\\\Internships_19\\\\Internship(Summer_19)\\\\Q&A_Toolkit\\\\Dataset_analysis\\\\SQuAD\\\\dictionaries.pkl\", \"rb\") as input_file:\n",
    "            dictionaries = pickle.load(input_file)\n",
    "        self.word_to_index = dictionaries[\"word_to_index\"]\n",
    "        self.char_to_index = dictionaries[\"char_to_index\"]\n",
    "        self.index_to_word = dictionaries[\"index_to_word\"]\n",
    "        self.index_to_char = dictionaries[\"index_to_char\"]\n",
    "        \n",
    "        \n",
    "    def index_files_using_char_to_index(self, filename, _dict, max_words, max_chars):\n",
    "\n",
    "        f = open(filename, \"r\", encoding=\"utf-8\")\n",
    "        lines = f.readlines()\n",
    "        lines = [l.lower() for l in lines]\n",
    "        encoded_lines = []\n",
    "        for l in lines:\n",
    "            tokens = l.split()\n",
    "            tokens = tokens[:max_words]\n",
    "            encoded_tokens = []\n",
    "            for t in tokens:\n",
    "                l = list(t)\n",
    "                l = l[:max_chars] ## there is a max limit for the length of characters = max_chars\n",
    "                encoded_chars = []\n",
    "                for j in l:\n",
    "                    if j in _dict:\n",
    "                        encoded_chars.append(_dict[j])\n",
    "                    else:\n",
    "                        encoded_chars.append(0)  ## if the character id not in dictionary put '0' in its place\n",
    "                encoded_tokens.append(encoded_chars)\n",
    "            encoded_lines.append(encoded_tokens)\n",
    "\n",
    "        return encoded_lines\n",
    "\n",
    "    def index_files_using_word_to_index(self, filename, _dict, max_words):\n",
    "        \n",
    "        f = open(filename, \"r\", encoding=\"utf-8\")\n",
    "\n",
    "        lines = f.readlines()\n",
    "        lines  = [l.lower() for l in lines]\n",
    "        encoded_lines = []\n",
    "        for l in lines:\n",
    "            tokens = l.split()\n",
    "            tokens = tokens[:max_words]\n",
    "            temp = []\n",
    "            for t in tokens:\n",
    "                if t in _dict:\n",
    "#                     print(_dict[t])\n",
    "                    temp.append(_dict[t])\n",
    "                else:\n",
    "                    temp.append(_dict[\"<unk>\"])\n",
    "\n",
    "            encoded_lines.append(temp[:])\n",
    "#         close(filename)\n",
    "#             print(\"HEllo\")\n",
    "\n",
    "        return encoded_lines\n",
    "    \n",
    "    def index_files_to_char_level_and_word_level(self, datapath = \"E:\\\\Internships_19\\\\Internship(Summer_19)\\\\Q&A_Toolkit\\\\Dataset_analysis\\\\SQuAD\", max_words=0, max_chars=0):\n",
    "#         files = [\".context\", \".question\", \".answer_text\"]\n",
    "        files = [\".context\",\".question\", \".answer_text\"]\n",
    "\n",
    "\n",
    "        for f in files:\n",
    "            read_path_train = os.path.join(datapath, \"train\" + f)\n",
    "            write_path_train_word = os.path.join(datapath, \"train_word_index\" + f + \"_pkl.pkl\")\n",
    "#             write_path_train_char = os.path.join(datapath, \"train_char_index\" + f + \"_pkl.pkl\")\n",
    "\n",
    "            read_path_valid = os.path.join(datapath, \"validation\" + f)\n",
    "            write_path_valid_word = os.path.join(datapath, \"validation_word_index\" + f + \"_pkl.pkl\")\n",
    "#             write_path_valid_char = os.path.join(datapath, \"validation_char_index\" + f + \"_pkl.pkl\")\n",
    "\n",
    "\n",
    "\n",
    "            temp_train_word = self.index_files_using_word_to_index(read_path_train, self.word_to_index, max_words)\n",
    "            temp_valid_word = self.index_files_using_word_to_index(read_path_valid, self.word_to_index, max_words)\n",
    "\n",
    "#             temp_train_char = index_files_using_char_to_index(read_path_train, self.char_to_index, max_words,max_chars)\n",
    "#             temp_valid_char = index_files_using_char_to_index(read_path_valid, self.char_to_index, max_words,max_chars)\n",
    "\n",
    "            write_file_train_word = open(write_path_train_word, \"wb\")\n",
    "            pickle.dump(temp_train_word, write_file_train_word)\n",
    "\n",
    "#             write_file_train_char = open(write_path_train_char, \"wb\")\n",
    "#             pickle.dump(temp_train_char, write_file_train_char)\n",
    "\n",
    "            write_file_valid_word = open(write_path_valid_word, \"wb\")\n",
    "            pickle.dump(temp_valid_word, write_file_valid_word)\n",
    "\n",
    "#             write_file_valid_char = open(write_path_valid_char, \"wb\")\n",
    "#             pickle.dump(temp_valid_char, write_file_valid_char)\n",
    "\n",
    "    def get_glove_embeddings(self, word_embedding_size = 100, char_embedding_size = 20 , embedding_dir = \"E:\\\\Internships_19\\\\Internship(Summer_19)\\\\Q&A_Toolkit\\\\Dataset_analysis\\\\SQuAD\" ):\n",
    "\n",
    "\n",
    "\n",
    "        glove_embeddings = os.path.join(embedding_dir, \"glove_embeddings100.txt\")\n",
    "\n",
    "        glove_embeddings = open(glove_embeddings,'r', encoding = 'utf-8')\n",
    "\n",
    "\n",
    "\n",
    "        #     glove_embeddings = pickle.load(open(glove_embeddings))\n",
    "\n",
    "        #####################  CHECK HOW GLOVE EMBEDDINGS WORK ##############\n",
    "        temp_embeddings = []\n",
    "\n",
    "        for word in self.word_to_index:\n",
    "\n",
    "                if word in ['<pad>', '<sos>']:\n",
    "                    temp_vector = np.zeros((word_embedding_size))\n",
    "                elif word not in glove_embeddings:\n",
    "                    temp_vector = np.random.uniform(-np.sqrt(3)/np.sqrt(word_embedding_size), np.sqrt(3)/np.sqrt(word_embedding_size), word_embedding_size)\n",
    "                else:\n",
    "                    temp_vector = glove_embeddings[word]\n",
    "\n",
    "                temp_embeddings.append(temp_vector)\n",
    "\n",
    "        temp_embeddings = np.asarray(temp_embeddings)\n",
    "        temp_embeddings = temp_embeddings.astype(np.float32)\n",
    "        self.word_embeddings = temp_embeddings\n",
    "\n",
    "\n",
    "#         char_embeddings = []\n",
    "# #         print (char_embedding_size)\n",
    "#         char_embeddings.append(np.zeros((char_embedding_size)))\n",
    "\n",
    "#         for i in range(len(self.char_to_index)):\n",
    "#             temp_vector = np.random.uniform(-np.sqrt(3)/np.sqrt(char_embedding_size), np.sqrt(3)/np.sqrt(char_embedding_size), char_embedding_size)\n",
    "#             char_embeddings.append(temp_vector)\n",
    "\n",
    "#         char_embeddings = np.asarray(char_embeddings)\n",
    "#         char_embeddings = char_embeddings.astype(np.float32)\n",
    "\n",
    "#         self.char_embeddings = char_embeddings\n",
    "\n",
    "#         pickle.dump(char_embeddings, open(os.path.join(embedding_dir, \"char_embeddings\" + \".pkl\"), \"wb\")) \n",
    "        pickle.dump(temp_embeddings, open(os.path.join(embedding_dir, \"glove_word_embeddings\" + \".pkl\"), \"wb\"))\n",
    "\n",
    "\n",
    "#         return self.word_embeddings, self.char_embeddings\n",
    "\n",
    "embedding = Embedding_Matrix(embedding_dir = \"E:\\\\Internships_19\\\\Internship(Summer_19)\\\\Q&A_Toolkit\\\\Dataset_analysis\\\\SQuAD\")\n",
    "embedding.get_glove_embeddings(word_embedding_size = 100, char_embedding_size = 20 , embedding_dir = \"E:\\\\Internships_19\\\\Internship(Summer_19)\\\\Q&A_Toolkit\\\\Dataset_analysis\\\\SQuAD\" )\n",
    "embedding.index_files_to_char_level_and_word_level(datapath = \"E:\\\\Internships_19\\\\Internship(Summer_19)\\\\Q&A_Toolkit\\\\Dataset_analysis\\\\SQuAD\", max_words=700, max_chars=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Train_Model:\n",
    "\n",
    "    def __init__(self, config, model):\n",
    "        self.config = config\n",
    "        self.model = model\n",
    "        self.loss_function = torch.nn.CrossEntropyLoss()\n",
    "        self.parameters_trainable = list(filter(lambda p: p.requires_grad, self.model.parameters()))\n",
    "        self.optimizer = optim.Adam(self.parameters_trainable, lr=self.config.lr)\n",
    "\n",
    "        self.glove_path = os.path.join(config.data_dir, \"glove_word_embeddings.pkl\")\n",
    "        self.num_epochs = config.num_epochs\n",
    "        self.data_dir = config.data_dir\n",
    "        self.names = config.names\n",
    "        self.batch_size = config.batch_size\n",
    "        self.print_every = config.print_every\n",
    "        self.max_context_length = config.max_context_length\n",
    "        self.max_question_length = config.max_question_length\n",
    "        self.model_dir = config.model_dir\n",
    "        self.early_stop = config.early_stop\n",
    "        self.print_and_validate_every = config.print_and_validate_every\n",
    "        \n",
    "    def save_model(self, model, optimizer, loss, global_step, epoch ,prefix):\n",
    "        # A state_dict is simply a Python dictionary object that maps each layer to its parameter tensor\n",
    "        \n",
    "        if(prefix == \"best_model\"):\n",
    "            model_state = model.state_dict()\n",
    "            model_state = {k: v for k, v in model_state.items() if 'embedding' not in k}\n",
    "\n",
    "            state = {\n",
    "                'global_step': global_step,\n",
    "                'epoch': epoch,\n",
    "                'model': model_state,\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'current_loss': loss\n",
    "            }\n",
    "            model_save_path = os.path.join(self.model_dir, 'best_model' )\n",
    "            torch.save(state, model_save_path)\n",
    "            \n",
    "        elif(prefix == \"last_model\"):\n",
    "            model_state = model.state_dict()\n",
    "            model_state = {k: v for k, v in model_state.items() if 'embedding' not in k}\n",
    "\n",
    "            state = {\n",
    "                'global_step': global_step,\n",
    "                'epoch': epoch,\n",
    "                'model': model_state,\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'current_loss': loss\n",
    "            }\n",
    "            model_save_path = os.path.join(self.model_dir, 'last_model' )\n",
    "            torch.save(state, model_save_path)\n",
    "            \n",
    "            \n",
    "    def get_f1_em_score(self, prefix, num_samples=100):\n",
    "\n",
    "\n",
    "        f1_total = 0.\n",
    "        em_total = 0.\n",
    "        example_num = 0\n",
    "\n",
    "#         tic = time.time()\n",
    "\n",
    "        for batch in get_batch_generator(self.data_dir, self.names, self.batch_size, self.max_context_length, self.max_question_length,prefix):\n",
    "\n",
    "            _,start_pos_prediction, end_pos_prediction = self.test_one_batch(batch)\n",
    "\n",
    "            start_pos_prediction = start_pos_prediction.tolist()\n",
    "            end_pos_prediction = end_pos_prediction.tolist()\n",
    "\n",
    "            for index, (pred_answer_start, pred_answer_end, true_answer_tokens) in enumerate(zip(start_pos_prediction, end_pos_prediction, batch.answer_tokens_batch)):\n",
    "                    \n",
    "                example_num += 1\n",
    "                pred_answer_tokens = batch.context_tokens_batch[index][pred_answer_start : pred_answer_end + 1]\n",
    "                pred_answer = \" \".join(pred_answer_tokens)\n",
    "\n",
    "                true_answer = \" \".join(true_answer_tokens)\n",
    "\n",
    "                f1 = f1_score(pred_answer, true_answer)\n",
    "                em = exact_match_score(pred_answer, true_answer)\n",
    "                f1_total += f1\n",
    "                em_total += em\n",
    "\n",
    "                \n",
    "\n",
    "                if num_samples != 0 and example_num >= num_samples:\n",
    "                    break\n",
    "\n",
    "            if num_samples != 0 and example_num >= num_samples:\n",
    "                break\n",
    "\n",
    "        f1_total /= example_num\n",
    "        em_total /= example_num\n",
    "\n",
    "#         toc = time.time()\n",
    "#         logging.info(\"Calculating F1/EM for %i examples in %s set took %.2f seconds\" % (example_num, dataset, toc-tic))\n",
    "\n",
    "        return f1_total, em_total\n",
    "    def get_validation_loss(self,prefix):\n",
    "#         logging.info(\"Calculating dev loss...\")\n",
    "#         tic = time.time()\n",
    "#         loss_per_batch, batch_lengths = [], []\n",
    "        total_validation_loss = 0.0\n",
    "        validation_set_size = 0\n",
    "        for batch in get_batch_generator(self.data_dir, self.names, self.batch_size, self.max_context_length, self.max_question_length,prefix):\n",
    "\n",
    "            validation_batch_loss, _, _ = self.test_one_batch(batch)\n",
    "            validation_set_size += batch.batch_size\n",
    "            total_validation_loss += validation_batch_loss\n",
    "#             batch_lengths.append(curr_batch_size)\n",
    "#             i += 1\n",
    "#             if i == 10:\n",
    "#                 break\n",
    "#         total_num_examples = sum(batch_lengths)\n",
    "#         toc = time.time()\n",
    "#         print(validation_set_size)\n",
    "\n",
    "        validation_loss = total_validation_loss / validation_set_size\n",
    "#         print \"Computed validation loss = %f \" % (validation_loss)\n",
    "\n",
    "        return validation_loss\n",
    "\n",
    "    def get_data(self, batch, is_train=True):\n",
    "        \n",
    "        question_word_index_batch = batch.question_word_index_batch\n",
    "\n",
    "        context_word_index_batch = batch.context_word_index_batch\n",
    "        \n",
    "        span_tensor_batch = batch.span_tensor_batch\n",
    "\n",
    "        if is_train:\n",
    "            return context_word_index_batch, question_word_index_batch,span_tensor_batch\n",
    "        else:\n",
    "            return context_word_index_batch, question_word_index_batch\n",
    "      \n",
    "    def get_grad_norm(self, parameters, norm_type=2):\n",
    "        parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
    "        total_norm = 0\n",
    "        for p in parameters:\n",
    "            param_norm = p.grad.data.norm(norm_type)\n",
    "            total_norm += param_norm ** norm_type\n",
    "        total_norm = total_norm ** (1. / norm_type)\n",
    "        return total_norm\n",
    "\n",
    "    def get_param_norm(self, parameters, norm_type=2):\n",
    "        total_norm = 0\n",
    "        for p in parameters:\n",
    "            param_norm = p.data.norm(norm_type)\n",
    "            total_norm += param_norm ** norm_type\n",
    "        total_norm = total_norm ** (1. / norm_type)\n",
    "        return total_norm\n",
    "    \n",
    "    def test_one_batch(self, batch):\n",
    "        \n",
    "        self.model.eval()\n",
    "\n",
    "        context_word_index_batch, question_word_index_batch,  span_tensor_batch = self.get_data(batch)\n",
    "\n",
    "#         print(context_word_index_batch)\n",
    "\n",
    "\n",
    "        context_word_index_padded_per_batch = Variable(pad_data(context_word_index_batch))\n",
    "#         print(context_word_index_padded_per_batch)\n",
    "        context_word_index_padded_per_batch.requires_grad = False\n",
    "        question_word_index_padded_per_batch = Variable(pad_data(question_word_index_batch))\n",
    "        question_word_index_padded_per_batch.requires_grad = False\n",
    "\n",
    "\n",
    "        context_ids = np.array(context_word_index_padded_per_batch) # shape (batch_size, context_len)\n",
    "        context_mask_per_batch = (context_ids != 0).astype(np.int32) # shape (batch_size, context_len)\n",
    "        context_word_mask_per_batch_new = Variable(torch.from_numpy(context_mask_per_batch))\n",
    "        context_word_mask_per_batch_new.requires_grad = False\n",
    "\n",
    "        question_ids = np.array(question_word_index_padded_per_batch) # shape (batch_size, context_len)\n",
    "        question_mask_per_batch = (question_ids != 0).astype(np.int32) # shape (batch_size, context_len)\n",
    "        question_word_mask_per_batch_new = Variable(torch.from_numpy(question_mask_per_batch))\n",
    "        question_word_mask_per_batch_new.requires_grad = False\n",
    "\n",
    "        span_tensor_batch = Variable(span_tensor_batch)\n",
    "        \n",
    "        span_tensor_batch.requires_grad = False\n",
    "\n",
    "        start_index_prediction, end_index_prediction, loss = self.model(context_word_index_padded_per_batch,context_word_mask_per_batch_new, question_word_index_padded_per_batch, question_word_mask_per_batch_new, span_tensor_batch)\n",
    "    \n",
    "        self.model.train()\n",
    "\n",
    "        return loss.item(),start_index_prediction, end_index_prediction\n",
    "        \n",
    "    def train_one_batch(self, batch):\n",
    "\n",
    "\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        context_word_index_batch, question_word_index_batch,  span_tensor_batch = self.get_data(batch)\n",
    "\n",
    "\n",
    "\n",
    "        context_word_index_padded_per_batch = Variable(pad_data(context_word_index_batch))\n",
    "        context_word_index_padded_per_batch.requires_grad = False\n",
    "        question_word_index_padded_per_batch = Variable(pad_data(question_word_index_batch))\n",
    "        question_word_index_padded_per_batch.requires_grad = False\n",
    "\n",
    "\n",
    "        context_ids = np.array(context_word_index_padded_per_batch) # shape (batch_size, context_len)\n",
    "        context_mask_per_batch = (context_ids != 0).astype(np.int32) # shape (batch_size, context_len)\n",
    "        context_word_mask_per_batch_new = Variable(torch.from_numpy(context_mask_per_batch))\n",
    "        context_word_mask_per_batch_new.requires_grad = False\n",
    "\n",
    "        question_ids = np.array(question_word_index_padded_per_batch) # shape (batch_size, context_len)\n",
    "        question_mask_per_batch = (question_ids != 0).astype(np.int32) # shape (batch_size, context_len)\n",
    "        question_word_mask_per_batch_new = Variable(torch.from_numpy(question_mask_per_batch))\n",
    "        question_word_mask_per_batch_new.requires_grad = False\n",
    "\n",
    "        span_tensor_batch = Variable(span_tensor_batch)\n",
    "        \n",
    "        span_tensor_batch.requires_grad = False\n",
    "\n",
    "        _, _, loss = self.model(context_word_index_padded_per_batch,context_word_mask_per_batch_new, question_word_index_padded_per_batch, question_word_mask_per_batch_new, span_tensor_batch)\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "#         l2_reg = None\n",
    "#         for W in self.parameters:\n",
    "            \n",
    "#             if l2_reg is None:\n",
    "#                 l2_reg = W.norm(2)\n",
    "#             else:\n",
    "#                 l2_reg = l2_reg + W.norm(2)\n",
    "#         loss = loss + config.reg_lambda * l2_reg\n",
    "        \n",
    "\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "\n",
    "        \n",
    "        param_norm = self.get_param_norm(self.parameters_trainable)\n",
    "        grad_norm = self.get_grad_norm(self.parameters_trainable)\n",
    "        \n",
    "        \n",
    "#         clip_grad_norm_(parameters, config.max_grad_norm)\n",
    "        self.optimizer.step()\n",
    "    \n",
    "\n",
    "        return loss.item(), param_norm, grad_norm\n",
    "    \n",
    "    \n",
    "    def train(self):\n",
    "\n",
    "\n",
    "        num_parameters = sum(p.numel() for p in self.parameters_trainable)\n",
    "\n",
    "        best_validation_f1, best_validation_em = None, None\n",
    "        best_validation_epoch = 0\n",
    "        epoch = 0\n",
    "        global_step = 0\n",
    "\n",
    "        loss_array = []\n",
    "        logging.info(\"Beginning training loop...\")\n",
    "        for epoch in range(200):\n",
    "            total_loss = 0.0\n",
    "            epoch_tic = time.time()\n",
    "            for batch in get_batch_generator(self.data_dir, self.names, self.batch_size, self.max_context_length, self.max_question_length,\"train\"):\n",
    "                \n",
    "                global_step += 1\n",
    "                iter_tic = time.time()\n",
    "                \n",
    "                train_batch_loss, param_norm, grad_norm = self.train_one_batch(batch)\n",
    "\n",
    "#                 total_loss = total_loss + loss\n",
    "#                 loss_array.append(total_loss)\n",
    "    \n",
    "                iter_toc = time.time()\n",
    "                iter_time = iter_toc - iter_tic\n",
    "            \n",
    "                if global_step % self.print_and_validate_every == 0:\n",
    "                    \n",
    "#                     print(self.get_validation_loss(\"validation\"))\n",
    "                    validation_batch_loss = self.get_validation_loss(\"validation\")\n",
    "                    \n",
    "#                     print()\n",
    "\n",
    "\n",
    "#                 if global_step % config.save_every == 0:\n",
    "#                     logging.info(\"Saving to %s...\" % model_dir)\n",
    "#                     self.save_model(model, optimizer, loss, global_step, epoch, model_dir)\n",
    "\n",
    "#                 if global_step % config.validate_every == 0:\n",
    "                    \n",
    "#                     logging.info(\"Epoch %d, Iter %d, dev loss: %f\" % (epoch, global_step, dev_loss))\n",
    "#                     write_summary(dev_loss, \"dev/loss\", summary_writer, global_step)\n",
    "\n",
    "                    train_batch_f1, train_batch_em = self.get_f1_em_score(\"train\", num_samples=100)\n",
    "#                     logging.info(\"Epoch %d, Iter %d, Train F1 score: %f, Train EM score: %f\" % (\n",
    "#                         epoch, global_step, train_f1, train_em))\n",
    "#                     write_summary(train_f1, \"train/F1\", summary_writer, global_step)\n",
    "#                     write_summary(train_em, \"train/EM\", summary_writer, global_step)\n",
    "\n",
    "                    validation_batch_f1, validation_batch_em = self.get_f1_em_score(\"validation\", num_samples=100)\n",
    "#                     logging.info(\n",
    "#                         \"Epoch %d, Iter %d, Dev F1 score: %f, Dev EM score: %f\" % (epoch, global_step, dev_f1, dev_em))\n",
    "#                     write_summary(dev_f1, \"dev/F1\", summary_writer, global_step)\n",
    "#                     write_summary(dev_em, \"dev/EM\", summary_writer, global_step)\n",
    "\n",
    "\n",
    "\n",
    "                    if best_validation_f1 is None or validation_batch_f1 > best_validation_f1:\n",
    "                        best_validation_f1 = validation_batch_f1\n",
    "\n",
    "                    if best_validation_em is None or validation_batch_em > best_validation_em:\n",
    "                        best_validation_em = validation_batch_em\n",
    "                        best_validation_epoch = epoch+1\n",
    "#                         logging.info(\"Saving to %s...\" % bestmodel_dir)\n",
    "                        self.save_model(self.model, self.optimizer, validation_batch_loss, global_step, epoch, \"best_model\")\n",
    "                        \n",
    "                    print (\"Epoch : {} Step : {} Train_batch Loss : {} Validation_batch Loss :{} \" .format(epoch+1, global_step, train_batch_loss, validation_batch_loss))\n",
    "            \n",
    "                    print(\"Train_batch F1:{} Train_batch EM:{} Validation_batch_F1: {} Best_validation_batch F1:{} Best_validation_batch EM :{} \".format(train_batch_f1,train_batch_em,validation_batch_f1,best_validation_f1,best_validation_em))\n",
    "\n",
    "                \n",
    "            if (epoch - best_validation_epoch > self.early_stop):\n",
    "                break\n",
    "            self.save_model(self.model, self.optimizer, train_batch_loss, global_step, epoch+1 ,\"last_model\")\n",
    "#             self.save_model(model, optimizer, loss, global_step, epoch, bestmodel_dir)\n",
    "#             torch.save(self.model.state_dict(), open(os.path.join(outdir, \"last_model\"), \"wb\"))\n",
    "#             print(\"total loss for epoch number = \" + str(epoch+1) + \" = \" + str(total_loss))\n",
    "\n",
    "#             epoch_toc = time.time()\n",
    "            print(\"End of epoch %i.\" % (epoch+1))\n",
    "\n",
    "        sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:54: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.15 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1 Step : 2 Train_batch Loss : 10.03243350982666 Validation_batch Loss :0.4990212440490723 \n",
      "Train_batch F1:0.057028985507246374 Train_batch EM:0.0 Validation_batch_F1: 0.1113545089448704 Best_validation_batch F1:0.1113545089448704 Best_validation_batch EM :0.0 \n",
      "End of epoch 1.\n",
      "Epoch : 2 Step : 4 Train_batch Loss : 10.027390480041504 Validation_batch Loss :0.49893956184387206 \n",
      "Train_batch F1:0.057028985507246374 Train_batch EM:0.0 Validation_batch_F1: 0.08417220262152483 Best_validation_batch F1:0.1113545089448704 Best_validation_batch EM :0.0 \n",
      "End of epoch 2.\n",
      "Epoch : 3 Step : 6 Train_batch Loss : 10.017292976379395 Validation_batch Loss :0.49881811141967775 \n",
      "Train_batch F1:0.11199974574116449 Train_batch EM:0.0 Validation_batch_F1: 0.026486555874212576 Best_validation_batch F1:0.1113545089448704 Best_validation_batch EM :0.0 \n",
      "End of epoch 3.\n",
      "Epoch : 4 Step : 8 Train_batch Loss : 9.998069763183594 Validation_batch Loss :0.4986070156097412 \n",
      "Train_batch F1:0.11199974574116449 Train_batch EM:0.0 Validation_batch_F1: 0.026486555874212576 Best_validation_batch F1:0.1113545089448704 Best_validation_batch EM :0.0 \n",
      "End of epoch 4.\n",
      "Epoch : 5 Step : 10 Train_batch Loss : 9.958771705627441 Validation_batch Loss :0.4982137203216553 \n",
      "Train_batch F1:0.11199974574116449 Train_batch EM:0.0 Validation_batch_F1: 0.026486555874212576 Best_validation_batch F1:0.1113545089448704 Best_validation_batch EM :0.0 \n",
      "End of epoch 5.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-294b5e6bfa55>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mtrain_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrain_Model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0mtrain_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-38-46acb3093a9b>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    292\u001b[0m \u001b[1;31m#                     write_summary(dev_loss, \"dev/loss\", summary_writer, global_step)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 294\u001b[1;33m                     \u001b[0mtrain_batch_f1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_batch_em\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_f1_em_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"train\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    295\u001b[0m \u001b[1;31m#                     logging.info(\"Epoch %d, Iter %d, Train F1 score: %f, Train EM score: %f\" % (\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    296\u001b[0m \u001b[1;31m#                         epoch, global_step, train_f1, train_em))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-38-46acb3093a9b>\u001b[0m in \u001b[0;36mget_f1_em_score\u001b[1;34m(self, prefix, num_samples)\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mget_batch_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_context_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_question_length\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mprefix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m             \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstart_pos_prediction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend_pos_prediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[0mstart_pos_prediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstart_pos_prediction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-38-46acb3093a9b>\u001b[0m in \u001b[0;36mtest_one_batch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    180\u001b[0m         \u001b[0mspan_tensor_batch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequires_grad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 182\u001b[1;33m         \u001b[0mstart_index_prediction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend_index_prediction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontext_word_index_padded_per_batch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcontext_word_mask_per_batch_new\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquestion_word_index_padded_per_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquestion_word_mask_per_batch_new\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mspan_tensor_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    183\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    492\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 493\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    494\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-21-9b02cbf09c9a>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, context_word_indexes, context_word_mask, question_word_indexes, question_word_mask, span_tensor)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m         \u001b[0mU_matrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoattention_encoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquestion_representation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpassage_representation\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcontext_word_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex_start\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex_end\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mU_matrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontext_word_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mspan_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-24-742324998791>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, question_representation, context_representation, document_word_sequence_mask)\u001b[0m\n\u001b[0;32m     66\u001b[0m         \u001b[0mbi_lstm_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mC_D_transpose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mD\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# B x (m + 1) x 3l\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m         \u001b[0mU\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfusion_bilstm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbi_lstm_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdocument_word_sequence_mask\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# B x m x 2l\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[1;31m#         print(\"size of U.(U is output of Co-attention encoder) ==  \" + str(U.size()))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    492\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 493\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    494\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-24-742324998791>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, word_sequence_embeddings, word_sequence_mask)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m         \u001b[1;31m# since the input was a packed sequence, the output will also be a packed sequence\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 105\u001b[1;33m         \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfusion_bilstm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpacked_word_sequence_embeddings\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minitial_hidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m         \u001b[1;31m# Pads a packed batch of variable length sequences.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    492\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 493\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    494\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    555\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    556\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mPackedSequence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 557\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward_packed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    558\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    559\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mforward_packed\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    548\u001b[0m         \u001b[0mmax_batch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_batch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m         \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_batch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    551\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_packed_sequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munsorted_indices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mforward_impl\u001b[1;34m(self, input, hx, batch_sizes, max_batch_size, sorted_indices)\u001b[0m\n\u001b[0;32m    523\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    524\u001b[0m             result = _VF.lstm(input, batch_sizes, hx, self._get_flat_weights(), self.bias,\n\u001b[1;32m--> 525\u001b[1;33m                               self.num_layers, self.dropout, self.training, self.bidirectional)\n\u001b[0m\u001b[0;32m    526\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    527\u001b[0m         \u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\traceback.py\u001b[0m in \u001b[0;36mformat_stack\u001b[1;34m(f, limit)\u001b[0m\n\u001b[0;32m    191\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m         \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_back\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 193\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mformat_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mextract_stack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\traceback.py\u001b[0m in \u001b[0;36mextract_stack\u001b[1;34m(f, limit)\u001b[0m\n\u001b[0;32m    205\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m         \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_back\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 207\u001b[1;33m     \u001b[0mstack\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mStackSummary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextract\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwalk_stack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    208\u001b[0m     \u001b[0mstack\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreverse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mstack\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\traceback.py\u001b[0m in \u001b[0;36mextract\u001b[1;34m(klass, frame_gen, limit, lookup_lines, capture_locals)\u001b[0m\n\u001b[0;32m    352\u001b[0m                 filename, lineno, name, lookup_line=False, locals=f_locals))\n\u001b[0;32m    353\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfnames\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 354\u001b[1;33m             \u001b[0mlinecache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheckcache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    355\u001b[0m         \u001b[1;31m# If immediate lookup was desired, trigger lookups now.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    356\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlookup_lines\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\IPython\\core\\compilerop.py\u001b[0m in \u001b[0;36mcheck_linecache_ipython\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m    138\u001b[0m     \"\"\"\n\u001b[0;32m    139\u001b[0m     \u001b[1;31m# First call the orignal checkcache as intended\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 140\u001b[1;33m     \u001b[0mlinecache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_checkcache_ori\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    141\u001b[0m     \u001b[1;31m# Then, update back the cache with our data, so that tracebacks related\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m     \u001b[1;31m# to our compiled codes can be produced.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\linecache.py\u001b[0m in \u001b[0;36mcheckcache\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[1;32mcontinue\u001b[0m   \u001b[1;31m# no-op for files loaded via a __loader__\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m             \u001b[0mstat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfullname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m             \u001b[1;32mdel\u001b[0m \u001b[0mcache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torchviz import make_dot\n",
    "hidden_dim = 100\n",
    "dropout_ratio = 0.15\n",
    "maxout_pool_size=16\n",
    "max_number_of_iterations = 3\n",
    "with open(r\"E:\\\\Internships_19\\\\Internship(Summer_19)\\\\Q&A_Toolkit\\\\Dataset_analysis\\\\SQuAD\\\\glove_word_embeddings.pkl\", \"rb\") as input_file:\n",
    "    embedding_matrix = pickle.load(input_file)\n",
    "    \n",
    "with autograd.set_detect_anomaly(True):\n",
    "    model = DCN_Model(hidden_dim, embedding_matrix, dropout_ratio, maxout_pool_size, max_number_of_iterations)\n",
    "\n",
    "    \n",
    "    # model = model.cpu()\n",
    "    train_model = Train_Model(config, model)\n",
    "\n",
    "    train_model.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm as tqdm\n",
    "import torch\n",
    "import random\n",
    "# import nltk\n",
    "import numpy as np\n",
    "import pickle\n",
    "import sys\n",
    "import copy\n",
    "import os.path\n",
    "# import tqdm as tqdm\n",
    "\n",
    "datapath = \"E:\\\\Internships_19\\\\Internship(Summer_19)\\\\Q&A_Toolkit\\\\Dataset_analysis\\\\SQuAD\\\\\"\n",
    "\n",
    "def find_max_length(data):\n",
    "\n",
    "    \"\"\" Finds the maximum sequence length for data \n",
    "        Args:\n",
    "            data: The data from which sequences will be chosen\n",
    "    \"\"\"\n",
    "    temp = 0\n",
    "    index = 0\n",
    "    for i, _ in enumerate(data):\n",
    "\n",
    "        if (len(data[i]) > temp):\n",
    "            temp = len(data[i])\n",
    "            index = i\n",
    "    return temp,index\n",
    "\n",
    "\n",
    "def pad_data(data):\n",
    "\n",
    "    \"\"\" Pad the data to max_length given\n",
    "        Args: \n",
    "            data: Data that needs to be padded\n",
    "            max_length : The length to be achieved with padding\n",
    "        Returns:\n",
    "            padded_data : Each sequence is padded to make it of length\n",
    "                          max_length.\n",
    "    \"\"\"\n",
    "    padded_data = []\n",
    "    max_length,index =  find_max_length(data)\n",
    "\n",
    "    for lines in data:\n",
    "        if (len(lines) < max_length):\n",
    "            temp = np.lib.pad(lines, (0,max_length - len(lines)),\n",
    "                'constant', constant_values=0)\n",
    "        else:\n",
    "            temp = lines[:max_length]\n",
    "        padded_data.append(temp)\n",
    "\n",
    "    padded_data = torch.from_numpy(np.array(padded_data)).type(torch.int64)\n",
    "\n",
    "    return padded_data\n",
    "\n",
    "\n",
    "def index_files_using_word_to_index(filename, _dict, max_words):\n",
    "    \n",
    "    f = open(filename, \"r\", encoding=\"utf-8\")\n",
    "\n",
    "    lines = f.readlines()\n",
    "    lines  = [l.lower() for l in lines]\n",
    "    encoded_lines = []\n",
    "    for l in lines:\n",
    "        tokens = l.split()\n",
    "        tokens = tokens[:max_words]\n",
    "        temp = []\n",
    "        for t in tokens:\n",
    "            if t in _dict:\n",
    "                temp.append(_dict[t])\n",
    "            else:\n",
    "                temp.append(1)\n",
    "\n",
    "        encoded_lines.append(temp[:])\n",
    "\n",
    "    return encoded_lines\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "import sys\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def normalize_answer(s):\n",
    "    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
    "\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "\n",
    "def f1_score(prediction, ground_truth):\n",
    "    prediction_tokens = normalize_answer(prediction).split()\n",
    "    ground_truth_tokens = normalize_answer(ground_truth).split()\n",
    "    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
    "    num_same = sum(common.values())\n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "    precision = 1.0 * num_same / len(prediction_tokens)\n",
    "    recall = 1.0 * num_same / len(ground_truth_tokens)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "\n",
    "def exact_match_score(prediction, ground_truth):\n",
    "    return (normalize_answer(prediction) == normalize_answer(ground_truth))\n",
    "    \n",
    "with open(r\"E:\\\\Internships_19\\\\Internship(Summer_19)\\\\Q&A_Toolkit\\\\Dataset_analysis\\\\SQuAD\\\\dictionaries.pkl\", \"rb\") as input_file:\n",
    "    dictionaries = pickle.load(input_file)\n",
    "word_to_index = dictionaries[\"word_to_index\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "\n",
    "import torch\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "import os\n",
    "import pickle\n",
    "import tqdm as tqdm\n",
    "\n",
    "import numpy as np\n",
    "from six.moves import xrange\n",
    "\n",
    "class Batch():\n",
    "    \"\"\"A class to hold the information needed for a training batch\"\"\"\n",
    "    def __init__(self,names,context_word_index_batch,question_word_index_batch, span_tensor_batch,context_tokens_batch,questions_tokens_batch,answer_tokens_batch):\n",
    "        \n",
    "        self.names = names\n",
    "        self.context_word_index_batch = context_word_index_batch\n",
    "\n",
    "        self.question_word_index_batch = question_word_index_batch\n",
    "        self.span_tensor_batch = span_tensor_batch\n",
    "        self.context_tokens_batch = context_tokens_batch\n",
    "        self.questions_tokens_batch = questions_tokens_batch\n",
    "        self.answer_tokens_batch = answer_tokens_batch\n",
    "        self.batch_size = len(self.context_word_index_batch)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def refill_batches(batches,batch_size,names, max_context_length, max_question_length,context_word_index,question_word_index,span_tensor,context_tokens,question_tokens,answer_tokens):\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    Adds more batches into the \"batches\" list.\n",
    "    Inputs:\n",
    "      batches: list to add batches to\n",
    "\n",
    "      names: list containing strings of file names [\"train_context\",\"train_question\"] or [\"validation_context\",\"validation_question\"]\n",
    "      data_dir : paths to {train/dev}.{context/question/answer} data files\n",
    "      batch_size: integer ==> how big to make the batches\n",
    "      max_context_length, max_question_length: max length of context and question respectively\n",
    "\n",
    "    \"\"\"\n",
    "#     print (\"Refilling batches...\")\n",
    "    tic = time.time()\n",
    "    examples = [] \n",
    "\n",
    "\n",
    "\n",
    "        # add to examples\n",
    "    examples.append((context_word_index, question_word_index, span_tensor,context_tokens,question_tokens,answer_tokens))\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "    # Make into batches and append to the list batches\n",
    "    for batch_start in xrange(0, len(examples[0][0]), batch_size):\n",
    "\n",
    "        # Note: each of these is a list length batch_size of lists of ints (except on last iter when it might be less than batch_size)\n",
    "        context_word_index_batch = examples[0][0][batch_start:batch_start+batch_size]\n",
    "        question_word_index_batch = examples[0][1][batch_start:batch_start+batch_size]\n",
    "        span_tensor_batch = examples[0][2][batch_start:batch_start+batch_size]\n",
    "        context_tokens_batch = examples[0][3][batch_start:batch_start+batch_size]\n",
    "        questions_tokens_batch = examples[0][4][batch_start:batch_start+batch_size]\n",
    "        answer_tokens_batch = examples[0][5][batch_start:batch_start+batch_size]\n",
    "\n",
    "#         print(\"Batch \" + str(batch_start + 1) + \" loaded\")\n",
    "        \n",
    "        batches.append((context_word_index_batch, question_word_index_batch,span_tensor_batch,context_tokens_batch,questions_tokens_batch,answer_tokens_batch))\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    # shuffle the batches\n",
    "    random.shuffle(batches)\n",
    "\n",
    "    toc = time.time()\n",
    "#     print (\"Refilling batches took %.2f seconds\" % (toc-tic))\n",
    "    return batches\n",
    "\n",
    "\n",
    "def get_batch_generator(data_dir,names, batch_size, max_context_length, max_question_length,prefix):\n",
    "    \"\"\"\n",
    "    This function returns a generator object that yields batches.\n",
    "    The last batch in the dataset will be a partial batch.\n",
    "    Read this to understand generators and the yield keyword in Python: https://stackoverflow.com/questions/231767/what-does-the-yield-keyword-do\n",
    "    Inputs:\n",
    "      names: list containing strings of file names = [\"train_context\",\"train_question\"] or [\"validation_context\",\"validation_question\"]\n",
    "      data_dir : paths to {train/dev}.{context/question/answer} data files\n",
    "      batch_size: integer ==> how big to make the batches\n",
    "      max_context_length, max_question_length: max length of context and question respectively\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "#     if(prefix == \"train\"):\n",
    "\n",
    "    context_path_train = os.path.join(data_dir, prefix + \".context\")\n",
    "    question_path_train = os.path.join(data_dir, prefix +  \".question\")\n",
    "    answer_path_train = os.path.join(data_dir, prefix +  \".answer_text\")\n",
    "#     print(os.path.join(data_dir, prefix + \".context\"))\n",
    "\n",
    "    context_tokens = open(context_path_train, \"r\", encoding=\"utf-8\").readlines()\n",
    "    question_tokens =  open(question_path_train, \"r\", encoding=\"utf-8\").readlines()\n",
    "    answer_tokens = open(answer_path_train, \"r\", encoding=\"utf-8\").readlines()\n",
    "#         print(question_tokens)\n",
    "\n",
    "#     lines = f.readlines()\n",
    "#     lines  = [l.lower() for l in lines]\n",
    "\n",
    "    context_word_index_old = index_files_using_word_to_index(context_path_train, word_to_index, max_context_length)\n",
    "    question_word_index_old = index_files_using_word_to_index(question_path_train, word_to_index, max_question_length)\n",
    "    \n",
    "    answer_start_path = os.path.join(data_dir  + prefix +  \".answer_start\")\n",
    "    answer_start_list = open(answer_start_path, \"r\", encoding=\"utf-8\").readlines()\n",
    "    \n",
    "    answer_end_path = os.path.join(data_dir + \"//\" + prefix +  \".answer_end\")\n",
    "    answer_end_list = open(answer_end_path, \"r\", encoding=\"utf-8\").readlines()\n",
    "\n",
    "#     with open(data_dir + \"//\" + prefix +  \"answer_end\", \"r\") as input_file:\n",
    "#         answer_end_pkl = pickle.load(input_file)\n",
    "#         print(\"answer_end_pkl\")\n",
    "#         print(answer_end_pkl)\n",
    "#     with open(data_dir + \"//\" + prefix +  \"answer_start\", \"r\") as input_file:\n",
    "#         answer_start_pkl = pickle.load(input_file)\n",
    "            \n",
    "#     elif(prefix == \"validation\"):\n",
    "#         context_path_train = os.path.join(datapath, \"validation.context\")\n",
    "#         question_path_train = os.path.join(datapath, \"validation.question\")\n",
    "#         answer_path_train = os.path.join(datapath, \"validation.answer_text\")\n",
    "\n",
    "#         context_tokens = open(context_path_train, \"r\", encoding=\"utf-8\").readlines()\n",
    "#         question_tokens =  open(question_path_train, \"r\", encoding=\"utf-8\").readlines()\n",
    "#         answer_tokens = open(answer_path_train, \"r\", encoding=\"utf-8\").readlines()\n",
    "\n",
    "\n",
    "#         context_word_index_old = index_files_using_word_to_index(context_path_train, word_to_index, max_context_length)\n",
    "#         question_word_index_old = index_files_using_word_to_index(question_path_train, word_to_index, max_question_length)\n",
    "\n",
    "\n",
    "#         ### check for answers if they are validation or train\n",
    "#         with open(data_dir + \"//\" + \"answer_end_pkl.pkl\", \"rb\") as input_file:\n",
    "#             answer_end_pkl = pickle.load(input_file)\n",
    "#         with open(data_dir + \"//\" + \"answer_start_pkl.pkl\", \"rb\") as input_file:\n",
    "#             answer_start_pkl = pickle.load(input_file)\n",
    "\n",
    "\n",
    "            \n",
    "    context_tokens = context_tokens[30:50]\n",
    "    question_tokens = question_tokens[30:50]\n",
    "    answer_tokens = answer_tokens[30:50]\n",
    "\n",
    "    answer_end = torch.from_numpy(np.array([int(i) for i in answer_end_list])).long()\n",
    "    answer_start = torch.from_numpy(np.array([int(i) for i in answer_start_list])).long()              \n",
    "    answer_start = torch.unsqueeze(answer_start, 1)\n",
    "    answer_end = torch.unsqueeze(answer_end, 1)\n",
    "\n",
    "    span_tensor_old = torch.cat((answer_start, answer_end), 1)\n",
    "    span_tensor = span_tensor_old[30:50]\n",
    "    context_word_index = context_word_index_old[30:50]\n",
    "    question_word_index = question_word_index_old[30:50]\n",
    "\n",
    "\n",
    "\n",
    "    batches = []\n",
    "    count = 0\n",
    "\n",
    "    while (True):\n",
    "        count = count + 1\n",
    "        if len(batches) == 0: # add more batches\n",
    "            if(count > 2):\n",
    "                break\n",
    "            batches = refill_batches(batches,batch_size,names, max_context_length, max_question_length,context_word_index,question_word_index,span_tensor,context_tokens,question_tokens,answer_tokens)\n",
    "        if len(batches) == 0:\n",
    "            break\n",
    "\n",
    "        # Get next batch. These are all lists length batch_size\n",
    "        (context_word_index_batch, question_word_index_batch,span_tensor_batch,context_tokens,question_tokens,answer_tokens) = batches.pop(0)\n",
    "        \n",
    "\n",
    "        if(len(context_word_index_batch) == 0):\n",
    "            break\n",
    "            \n",
    "\n",
    "\n",
    "        # Make into a Batch object\n",
    "        batch = Batch(names,context_word_index_batch, question_word_index_batch, span_tensor_batch,context_tokens,question_tokens,answer_tokens)\n",
    "\n",
    "        yield batch\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import logging\n",
    "import code\n",
    "import pickle\n",
    "import os\n",
    "from torch import autograd\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import Embedding\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "\n",
    "class DCN_Model(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_dim, embedding_matrix, dropout_ratio, maxout_pool_size, max_number_of_iterations):\n",
    "        super(DCN_Model, self).__init__()\n",
    "\n",
    "        self.encoder = Word_Level_Encoder(hidden_dim, emb_matrix, dropout_ratio)\n",
    "        self.coattention_encoder = Coattention_Encoder(hidden_dim, maxout_pool_size, embedding_matrix, max_number_of_iterations, dropout_ratio)\n",
    "        self.decoder = Dynamic_Decoder(hidden_dim, maxout_pool_size, max_number_of_iterations, dropout_ratio)\n",
    "\n",
    "    def forward(self, context_word_indexes, context_word_mask, question_word_indexes, question_word_mask,span_tensor):\n",
    "        passage_representation = self.encoder.forward(context_word_indexes, context_word_mask)\n",
    "\n",
    "        question_representation = self.encoder.forward(question_word_indexes, question_word_mask)\n",
    "       \n",
    "\n",
    "        U_matrix = self.coattention_encoder.forward(question_representation, passage_representation,context_word_mask)\n",
    "\n",
    "        loss, index_start, index_end = self.decoder.forward(U_matrix, context_word_mask, span_tensor)\n",
    "\n",
    "        return index_start, index_end, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "\n",
    "with open(r\"E:\\\\Internships_19\\\\Internship(Summer_19)\\\\Q&A_Toolkit\\\\Dataset_analysis\\\\SQuAD\\\\glove_word_embeddings.pkl\", \"rb\") as input_file:\n",
    "    emb_matrix = pickle.load(input_file)\n",
    "    \n",
    "names = [\"validation_context\",\"train_context\",\"validation_question\",\"train_question\"]\n",
    "data_dir = \"E:\\\\Internships_19\\\\Internship(Summer_19)\\\\Q&A_Toolkit\\\\Dataset_analysis\\\\SQuAD\\\\\"\n",
    "\n",
    "\n",
    "\n",
    "def get_pretrained_embedding(embedding_matrix):\n",
    "    embedding = nn.Embedding(*embedding_matrix.shape)\n",
    "    embedding.weight = nn.Parameter(torch.from_numpy(embedding_matrix).float())\n",
    "    embedding.weight.requires_grad = False\n",
    "    return embedding\n",
    "\n",
    "\n",
    "class Word_Level_Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_dim, embedding_matrix, dropout_ratio):\n",
    "        super(Word_Level_Encoder, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.embedding = get_pretrained_embedding(embedding_matrix)\n",
    "        self.embedding_dim = self.embedding.embedding_dim\n",
    "\n",
    "        # batch_first = True\n",
    "        # Input: has a dimension of B x m x embedding_dim\n",
    "        # Function parameters: input_size, hidden_size, num_layers_of_LSTM = 1(here)\n",
    "        self.encoder = nn.LSTM(self.embedding_dim, self.hidden_dim, 1, batch_first=True,\n",
    "                              bidirectional=False, dropout=dropout_ratio) \n",
    "                                     \n",
    "#         self.dropout_emb = nn.Dropout(p=dropout_ratio)\n",
    "        \n",
    "        # creates a random vector with size= hidden_dim\n",
    "        self.sentinel = nn.Parameter(torch.rand(hidden_dim,))\n",
    "        \n",
    "    def initHidden(self,batch_size):\n",
    "        h0 = Variable(torch.zeros(1, batch_size, self.hidden_dim), requires_grad = False) # Initial hidden state\n",
    "        c0 = Variable(torch.zeros(1, batch_size, self.hidden_dim), requires_grad = False) # Initial cell state\n",
    "        return h0, c0\n",
    "\n",
    "\n",
    "    def forward(self, word_sequence_indexes, word_sequence_mask):\n",
    "        \n",
    "        # stores length of per instance for context/question\n",
    "        # tensor of size = B\n",
    "        length_per_instance = torch.sum(word_sequence_mask, 1)\n",
    "\n",
    "\n",
    "        initial_hidden_states = self.initHidden(len(length_per_instance))\n",
    "        # returns the word_sequences_embeddings with the embeddings for each token/word from word_sequence_indexes\n",
    "        # word_sequence_embeddings is a tensor of dimension of B x m x l\n",
    "        word_sequence_embeddings = self.embedding(word_sequence_indexes)\n",
    "        \n",
    "        # All RNN modules accept packed sequences as inputs.\n",
    "        # Input: word_sequence_embeddings has a dimension of B x m x l (l is the size of the glove_embedding/ pre-trained embedding/embedding_dim)\n",
    "        packed_word_sequence_embeddings = pack_padded_sequence(word_sequence_embeddings,length_per_instance,batch_first=True,enforce_sorted=False)\n",
    "\n",
    "        \n",
    "        \n",
    "        # nn.LSTM encoder gets an input of pack_padded_sequence of dimensions\n",
    "        # since the input was a packed sequence, the output will also be a packed sequence\n",
    "        output, _ = self.encoder(packed_word_sequence_embeddings,initial_hidden_states)\n",
    "       \n",
    "        \n",
    "        # Pads a packed batch of variable length sequences.\n",
    "        # It is an inverse operation to pack_padded_sequence().\n",
    "        # dimension:  B x m x l\n",
    "        output_to_LSTM_padded, _ = pad_packed_sequence(output, batch_first=True)\n",
    "        \n",
    "        \n",
    "\n",
    "        # list() creates a list of elements if an iterable is passed\n",
    "        # batch_size is a scalar which stores the value of batch size. (batch_size = B)\n",
    "        batch_size, _ = list(word_sequence_mask.size())\n",
    "        \n",
    "        \n",
    "        # dimension of sentinel matrix =  B x 1 x l (replicates or expands along given dimension)\n",
    "        length_per_instance_new_dim = length_per_instance.unsqueeze(1).expand(batch_size, self.hidden_dim).unsqueeze(1)\n",
    "        \n",
    "\n",
    "        # sentinel to be concatenated to the data\n",
    "        # dimension of sentinel_zero =  B x 1 x l\n",
    "        sentinel_zero = torch.zeros(batch_size, 1, self.hidden_dim)\n",
    "        \n",
    "        # copy sentinel vector at the end\n",
    "        # dimension of output_to_LSTM_padded_with_sentinel =  B x (m + 1) x l\n",
    "        output_to_LSTM_padded_with_sentinel = torch.cat([output_to_LSTM_padded, sentinel_zero], 1)  \n",
    "        \n",
    "        \n",
    "        \n",
    "        return output_to_LSTM_padded_with_sentinel\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "class Highway_Maxout_Network(nn.Module):\n",
    "    def __init__(self, hidden_dim, maxout_pool_size, dropout_ratio):\n",
    "        super(Highway_Maxout_Network, self).__init__()\n",
    "        self.hidden_dim = hidden_dim # l\n",
    "        self.maxout_pool_size = maxout_pool_size # p\n",
    "\n",
    "        # Affine mapping from 5l ==> l\n",
    "        self.r = nn.Linear(5 * hidden_dim, hidden_dim, bias=False) \n",
    "       \n",
    "\n",
    "        # Affine mapping from 3*l ==> l*p\n",
    "        self.max_out_layer1 = nn.Linear(3 * hidden_dim, hidden_dim*maxout_pool_size)\n",
    "        \n",
    "        # Affine mapping from l ==> l*p\n",
    "        self.max_out_layer2 = nn.Linear(hidden_dim, hidden_dim*maxout_pool_size)\n",
    "       \n",
    "        # Affine mapping from 2*l ==> p\n",
    "        self.max_out_layer3 = nn.Linear(2 * hidden_dim, maxout_pool_size)\n",
    "\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "        \n",
    "    \n",
    "    def forward(self, h_i, U, curr_mask_vector, index_i_minus_1, u_concatenated, mask_matrix, target=None):\n",
    "        batch_size, max_word_length , _ = list(U.size())\n",
    "\n",
    "        # concatenation of ( h_i of dimension = b x l ; u_concatenated of dimension = b x 4l ) along dimension 1 = gives b x 5l\n",
    "        # self.r(b x 5l) ====> b x l (change of vector space)\n",
    "        r = torch.tanh(self.r(torch.cat((h_i.view(-1, self.hidden_dim), u_concatenated), 1)))  # b x 5l => b x l\n",
    "       \n",
    "\n",
    "        # hidden_dim = l\n",
    "        r_expanded = r.unsqueeze(1).expand(batch_size, max_word_length, self.hidden_dim).contiguous()  # b x m x l\n",
    "\n",
    "        m_t1_input = torch.cat((U, r_expanded), 2).view(-1, 3*self.hidden_dim)  # b*m x 3l\n",
    "\n",
    "        m_t1_output = self.max_out_layer1(m_t1_input)  # b*m x p*l\n",
    "        \n",
    "        m_t1_output_resized, _ = m_t1_output.view(-1, self.hidden_dim, self.maxout_pool_size).max(2) # b*m x l\n",
    "\n",
    "        # m_t2_input =  m_t1_output_resized\n",
    "        m_t2_output = self.max_out_layer2(m_t1_output_resized)  # b*m x l*p\n",
    "        \n",
    "        m_t2_output_resized, _ = m_t2_output.view(-1, self.hidden_dim, self.maxout_pool_size).max(2)  # b*m x l\n",
    "\n",
    "        m_t3_input = torch.cat((m_t1_output_resized, m_t2_output_resized), 1)  # b*m x 2l\n",
    "        alpha1 = self.max_out_layer3(m_t3_input)  # b * m x p\n",
    "        alpha2, _ = alpha1.max(1)  # b*m\n",
    "        alpha3 = alpha2.view(-1, max_word_length) # b x m\n",
    "\n",
    "#         print(\"alpha \" + str(alpha3.size()) )\n",
    "#         print(\"mask matrix \" + str(mask_matrix.size()))\n",
    "        alpha3 = alpha3 + mask_matrix  # b x m\n",
    "#         print(\"alpha3\")\n",
    "#         print(alpha3)\n",
    "        \n",
    "        # alpha can be treated as probabilities that assign probability masses todifferent words in context. The word with\n",
    "        # maximum weight(probability) becomes the index(start/end)\n",
    "        alpha4 = F.softmax(alpha3, 1)  # b x m\n",
    "        _, index_i = torch.max(alpha4, dim=1) # b\n",
    "\n",
    "        if curr_mask_vector is None:\n",
    "            curr_mask_vector = (index_i == index_i) # b\n",
    "        else:\n",
    "            index_i = index_i*curr_mask_vector.long()  # b\n",
    "            index_i_minus_1 = index_i_minus_1*curr_mask_vector.long()  # b\n",
    "            curr_mask_vector = (index_i != index_i_minus_1) # b\n",
    "\n",
    "        step_loss = None\n",
    "        \n",
    "        \n",
    "\n",
    "#         target[target < 0] = 0\n",
    "        \n",
    "        \n",
    "        ## loss is only calculated only on that the predicted index at i_th time-step which varies \n",
    "        ## from the predicted index at time-step (i-1)_th time-step\n",
    "#         print(target)\n",
    "        if target is not None:\n",
    "            step_loss = self.loss(alpha3, target)  # b\n",
    "#             print(\"step_loss\")\n",
    "#             print(step_loss)\n",
    "# #             step_loss1 = step_loss * curr_mask_vector.float() # b\n",
    "#             print(\"step_loss1\")\n",
    "#             print(step_loss1)\n",
    "\n",
    "        return index_i, curr_mask_vector, step_loss # all have dimension: b\n",
    "\n",
    "class Dynamic_Decoder(nn.Module):\n",
    "    def __init__(self, hidden_dim, maxout_pool_size, max_number_of_iterations, dropout_ratio):\n",
    "        super(Dynamic_Decoder, self).__init__()\n",
    "        self.max_number_of_iterations = max_number_of_iterations\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        # batch_first = True\n",
    "        # Input: has a dimension of B * m * embedding_dim\n",
    "        # Function parameters: input_size, hidden_size, num_layers_of_LSTM = 1(here)\n",
    "        self.decoder = nn.LSTM(4 * hidden_dim, hidden_dim, 1, batch_first=True, bidirectional=False)\n",
    "\n",
    "        self.maxout_start = Highway_Maxout_Network(hidden_dim, maxout_pool_size, dropout_ratio)\n",
    "        self.maxout_end = Highway_Maxout_Network(hidden_dim, maxout_pool_size, dropout_ratio)\n",
    "        \n",
    "    def initHidden(self,batch_size):\n",
    "        h0 = Variable(torch.zeros(1, batch_size, self.hidden_dim), requires_grad = False) # Initial hidden state\n",
    "        c0 = Variable(torch.zeros(1, batch_size, self.hidden_dim), requires_grad = False) # Initial cell state\n",
    "        return h0, c0\n",
    "\n",
    "\n",
    "    def forward(self, U, document_word_sequence_mask,span_tensor):\n",
    "        batch_size, max_word_length, _ = list(U.size()) # U has dimension : B x m x 2l\n",
    "\n",
    "        curr_mask_start,  curr_mask_end = None, None\n",
    "        results_mask_start, results_start = [], []\n",
    "        results_mask_end, results_end = [], []\n",
    "        step_losses = []\n",
    "        \n",
    "\n",
    "        # dimension = B x m\n",
    "        mask_matrix = (1.0 - document_word_sequence_mask.float()) * (-1e30)\n",
    "        \n",
    "        # dimension = B\n",
    "        indices = torch.arange(0, batch_size)\n",
    "\n",
    "        \n",
    "        # initialize start_i_minus_1, end_i_minus_1: these are the initial values of start and end indices\n",
    "        # start_i_minus_1 = the first index for the context/question \n",
    "        # end_i_minus_1 = the last index for the context/question \n",
    "        \n",
    "        # dimension = B\n",
    "        start_i_minus_1 = torch.zeros(batch_size).long()\n",
    "        \n",
    "        # dimension = B\n",
    "        end_i_minus_1 = torch.sum(document_word_sequence_mask, 1) - 1\n",
    "\n",
    "        \n",
    "\n",
    "        # After every iteration the hidden and current state \n",
    "        # at t = length of the sequence (for the one-directional lstm) will\n",
    "        # be returned by the lstm\n",
    "        # the hidden_state_i(h_i) will serve as an input to next lstm\n",
    "        hidden_and_current_state_i = self.initHidden(batch_size)\n",
    "        start_target = None\n",
    "        end_target = None\n",
    "        \n",
    "        # this sets the start and end target (ie. the y_label) for an answer\n",
    "        if span_tensor is not None:\n",
    "            # Dimension = B\n",
    "            start_target = span_tensor[:,0]\n",
    "            \n",
    "            \n",
    "            # Dimension = B\n",
    "            end_target = span_tensor[:,1]\n",
    "            \n",
    "    \n",
    "            \n",
    "        # this is just an initialization of u_start_i_minus_1\n",
    "        # u_start_i_minus_1 is essentially u_start_zero outside the loop\n",
    "        u_start_i_minus_1 = U[indices, start_i_minus_1, :]  # B x 2l\n",
    "        \n",
    "        # Why do we need an iterative procedure to predict the start and end indices for an answer ? \n",
    "        # Solution: there may exist several intuitive answer spans within the document, each corresponding to a\n",
    "        # local maxima. An iterative technique to select an answer span by alternating between\n",
    "        # predicting the start point and predicting the end point. This iterative procedure allows the model to\n",
    "        # recover from initial local maxima corresponding to incorrect answer spans.\n",
    "        for _ in range(self.max_number_of_iterations):\n",
    "            u_end_i_minus_1 = U[indices, end_i_minus_1, :]  # B x 2l\n",
    "            \n",
    "            # u_concatenated is fed to the lstm\n",
    "            u_concatenated = torch.cat((u_start_i_minus_1, u_end_i_minus_1), 1)  # B x 4l\n",
    "            \n",
    "        \n",
    "\n",
    "            # the hidden_and_current_state_i = h_i,c_i are essentially hidden and current cell states \n",
    "            # for t = length of the sequence (for the one-directional lstm) after every iteration\n",
    "            # u_concatenated.unsqueeze(1) has a dimension : B x 1 x 4l\n",
    "            lstm_output, hidden_and_current_state_i = self.decoder(u_concatenated.unsqueeze(1), hidden_and_current_state_i)\n",
    "            \n",
    "            # h_i has dimension = 1 x B x l\n",
    "            # c_i has dimension = 1 x B x l\n",
    "            h_i, c_i = hidden_and_current_state_i\n",
    "            \n",
    "            \n",
    "\n",
    "            # Inputs to the Highway_Maxout_Network(to find start index) are: hidden_state_i(h_i), start_i_minus_1(index), u_concatenated ==>(u_start_i_minus_1;u_end_i_minus_1) \n",
    "            start_i_minus_1, curr_mask_start, step_loss_start = self.maxout_start(h_i, U, curr_mask_start, start_i_minus_1,\n",
    "                                                                u_concatenated, mask_matrix, start_target)\n",
    "            \n",
    "            \n",
    "            \n",
    "            u_start_i_minus_1 = U[indices, start_i_minus_1, :]  # B x 2l\n",
    "\n",
    "            u_concatenated = torch.cat((u_start_i_minus_1, u_end_i_minus_1), 1)  # b x 4l\n",
    "\n",
    "            # Inputs to the Highway_Maxout_Network(to find end index) are: hidden_state_i(h_i), end_i_minus_1(index), u_concatenated ==>(u_start_i_minus_1;u_end_i_minus_1) \n",
    "            end_i_minus_1, curr_mask_end, step_loss_end = self.maxout_end(h_i, U, curr_mask_end, end_i_minus_1,\n",
    "                                                              u_concatenated, mask_matrix, end_target)\n",
    "\n",
    "            # we minimize the cumulative softmax cross entropy of the start and end points across all iterations.\n",
    "            if span_tensor is not None:\n",
    "                step_loss = step_loss_start + step_loss_end\n",
    "#                 print(step_loss)\n",
    "                step_losses.append(step_loss)\n",
    "            \n",
    "            results_mask_start.append(curr_mask_start) # appends all the curr_mask_start ==> dimension: num_iterations x B\n",
    "            results_start.append(start_i_minus_1) # appends all the start_indexes ==> dimension: num_iterations x B\n",
    "            results_mask_end.append(curr_mask_end) # appends all the curr_mask_end ==> dimension: num_iterations x B\n",
    "            results_end.append(end_i_minus_1) # appends all the end_indexes ==> dimension: num_iterations x B\n",
    "\n",
    "        \n",
    "        \n",
    "        # Dimension = B\n",
    "        result_pos_start1 = torch.sum(torch.stack(results_mask_start, 1), 1).long()\n",
    "        result_pos_start = result_pos_start1 - 1\n",
    "        \n",
    "        # Dimension = B\n",
    "        index_start = torch.gather(torch.stack(results_start, 1), 1, result_pos_start.unsqueeze(1)).squeeze()\n",
    "\n",
    "        # Dimension = B\n",
    "        result_pos_end1 = torch.sum(torch.stack(results_mask_end, 1), 1).long()\n",
    "        result_pos_end = result_pos_end1 - 1\n",
    "        \n",
    "        # Dimension = B\n",
    "        index_end = torch.gather(torch.stack(results_end, 1), 1, result_pos_end.unsqueeze(1)).squeeze()\n",
    "\n",
    "        loss = None\n",
    "\n",
    "#         print(\"step_losses\")\n",
    "#         print(sum(step_losses))\n",
    "        if span_tensor is not None:\n",
    "            # step losses has dimension = num_iterations x B\n",
    "            sum_losses = sum(step_losses)\n",
    "            batch_avg_loss = sum_losses / self.max_number_of_iterations\n",
    "            loss = batch_avg_loss\n",
    "\n",
    "            \n",
    "        return loss, index_start, index_end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "\n",
    "class Coattention_Encoder(nn.Module):\n",
    "    def __init__(self, hidden_dim, maxout_pool_size, embedding_matrix, max_number_of_iterations, dropout_ratio):\n",
    "        super(Coattention_Encoder, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        ## nn.Linear(input_dim, output_dim)\n",
    "        # Affine mapping from l ==> l\n",
    "        self.question_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        \n",
    "        self.fusion_bilstm = Fusion_BiLSTM(hidden_dim, dropout_ratio)\n",
    "#         self.dropout = nn.Dropout(p=dropout_ratio)\n",
    "\n",
    "    def forward(self, question_representation, context_representation,document_word_sequence_mask):\n",
    "        \n",
    "        ############## m = max length of instances in one batch of document ;  n= max length of instances in one batch of question ############################33\n",
    "        Q = question_representation # B x (n + 1) x l\n",
    "        D = context_representation  # B x (m + 1) x l\n",
    "        \n",
    "#         print(\"question_representation.(Output to Encoder Layer) ==  \" + str(Q.size()))\n",
    "#         print(\"context_representation. (Output to Encoder Layer)  ==  \" + str(D.size()))\n",
    "\n",
    "        # view function is meant to reshape the tensor.(Similar to reshape function in numpy)\n",
    "        # view( row_size = -1 ,means that number of rows are unknown, column_size)\n",
    "        # pass the Q tensor through a non-linearity \n",
    "        Q2 = torch.tanh(self.question_proj(Q.view(-1, self.hidden_dim))).view(Q.size()) #B x (n + 1) x l\n",
    "\n",
    "        ##################################   Co-Attention starts here  #######################################\n",
    "        \n",
    "        ########################################   Step - 1  ##################################################\n",
    "        # transpose(tensor, first_dimension to be transposed, second_dimension to be transposed)\n",
    "        Q_transpose = torch.transpose(Q2, 1, 2) #dimension: B x l x (n + 1)\n",
    "        \n",
    "        # Performs a batch matrix-matrix product of matrices stored in batch1 and batch2.\n",
    "        # batch1 and batch2 must be 3-D tensors each containing the same number of matrices.\n",
    "        L = torch.bmm(D, Q_transpose) # dimension of L : B x (m + 1) x (n + 1)\n",
    "\n",
    "        ####################################### Step-2 ######################################################\n",
    "        A_Q = F.softmax(L, dim=2) # B x (m + 1) x (n + 1)\n",
    "\n",
    "\n",
    "        D_transpose = torch.transpose(D, 1, 2) #dimension: B x l x (m + 1)\n",
    "        C_Q = torch.bmm(D_transpose, A_Q) # (B x l x (m + 1)) x (B x (m + 1) x (n + 1)) => B x l x (n + 1)\n",
    "\n",
    "        ####################################### Step-3 #######################################################\n",
    "        L_tranpose = torch.transpose(L,1,2)\n",
    "        A_D = F.softmax(L_tranpose, dim=2)  # B x (n + 1) x (m + 1)\n",
    "        \n",
    "        \n",
    "        # concatenation along dimension=1:(B x l x (n + 1) ; B x l x (n + 1)  -----> B x 2l x (n + 1) ) x (B x (n + 1) x (m + 1)) ====> B x 2l x (m + 1)\n",
    "        C_D = torch.bmm(torch.cat((Q_transpose, C_Q), 1), A_D) # B x 2l x (m + 1)\n",
    "        C_D_transpose = torch.transpose(C_D, 1, 2)  # B x (m + 1) x 2l\n",
    "\n",
    "        \n",
    "        #######################################  Step-4 ##########################################################\n",
    "        #fusion BiLSTM\n",
    "        # concatenation along dimension = 2:  (B x (m + 1) x 2l ; B x (m + 1) x l  -----> B x (m + 1) x 3l )\n",
    "        bi_lstm_input = torch.cat((C_D_transpose, D), 2) # B x (m + 1) x 3l\n",
    "       \n",
    "        U = self.fusion_bilstm(bi_lstm_input, document_word_sequence_mask) # B x m x 2l\n",
    "        \n",
    "#         print(\"size of U.(U is output of Co-attention encoder) ==  \" + str(U.size()))\n",
    "        \n",
    "        return U\n",
    "\n",
    "\n",
    "class Fusion_BiLSTM(nn.Module):\n",
    "    def __init__(self, hidden_dim, dropout_ratio):\n",
    "        super(Fusion_BiLSTM, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "         # batch_first = True\n",
    "        # Input: has a dimension of B * m * embedding_dim\n",
    "        # Function parameters: input_size, hidden_size, num_layers_of_LSTM = 1(here)\n",
    "        self.fusion_bilstm = nn.LSTM(3 * hidden_dim, hidden_dim, 1, batch_first=True,\n",
    "                                     bidirectional=True, dropout=dropout_ratio)\n",
    "        \n",
    "#         self.dropout = nn.Dropout(p=dropout_ratio)\n",
    "\n",
    "    def initHidden(self,batch_size):\n",
    "        h0 = Variable(torch.zeros(2, batch_size, self.hidden_dim), requires_grad = False) # Initial hidden state\n",
    "        c0 = Variable(torch.zeros(2, batch_size, self.hidden_dim), requires_grad = False) # Initial cell state\n",
    "        return h0, c0\n",
    "\n",
    "    def forward(self, word_sequence_embeddings, word_sequence_mask):\n",
    "        \n",
    "        # stores length of per instance for context/question\n",
    "        length_per_instance = torch.sum(word_sequence_mask, 1)\n",
    "        \n",
    "        initial_hidden_states = self.initHidden(len(length_per_instance))\n",
    "      \n",
    "        # All RNN modules accept packed sequences as inputs.\n",
    "        # Input: word_sequence_embeddings has a dimension of B x m+1 x 3l (l is the size of the glove_embedding/ pre-trained embedding/embedding_dim)\n",
    "        packed_word_sequence_embeddings = pack_padded_sequence(word_sequence_embeddings, length_per_instance, batch_first=True,enforce_sorted=False)\n",
    "        \n",
    "        # since the input was a packed sequence, the output will also be a packed sequence\n",
    "        output, _ = self.fusion_bilstm(packed_word_sequence_embeddings,initial_hidden_states)\n",
    "        \n",
    "        # Pads a packed batch of variable length sequences.\n",
    "        # It is an inverse operation to pack_padded_sequence().\n",
    "        # dimension:  B x m x 2l\n",
    "        output_to_BiLSTM_padded, _ = pad_packed_sequence(output, batch_first=True)\n",
    "\n",
    "\n",
    "        return output_to_BiLSTM_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "class Config(object):\n",
    "    pass\n",
    "\n",
    "config = Config()\n",
    "config.data_dir = \"E:\\\\Internships_19\\\\Internship(Summer_19)\\\\Q&A_Toolkit\\\\Dataset_analysis\\\\SQuAD\\\\\"\n",
    "config.word_embedding_size = 100\n",
    "config.hidden_dim = 300\n",
    "config.dropout_ratio = 0.15\n",
    "config.max_context_length = 600\n",
    "config.max_question_length = 30\n",
    "config.print_and_validate_every = 2\n",
    "\n",
    "#vector with zeros for unknown words\n",
    "config.num_iterations = 2\n",
    "config.maxout_pool_size=16\n",
    "\n",
    "config.lr = 0.001\n",
    "config.dropout_ratio = 0.15\n",
    "config.early_stop = 10\n",
    "config.vocab_size = 50000\n",
    "\n",
    "config.max_grad_norm = 5.0\n",
    "config.batch_size = 20\n",
    "config.num_epochs = 2\n",
    "config.model_dir = \"E:\\\\Internships_19\\\\Internship(Summer_19)\\\\Q&A_Toolkit\\\\Dynamic_Coattention_Networks\\\\Models\\\\saved_models\"\n",
    "\n",
    "# config.print_every = 100\n",
    "# config.save_every = 50000000\n",
    "# config.eval_every = 1000\n",
    "\n",
    "# config.model_type = 'co-attention'\n",
    "config.reg_lambda = 0.00007\n",
    "config.names = [\"train_context\",\"train_question\"]\n",
    "config.print_every = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FINAL TO_DOs\n",
    "1)keep a cap on vocabulary(50k), initialize other words randomly\n",
    "2)integrate the code to pickle vocab and answer/context/question tokens/indices in the end-to-end model\n",
    "3) break train file into train and validation, and use validation file as test file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
