{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file just processeses the SQuAD Dataset and stores it in a file format. The five files are :\n",
    "\n",
    "1) context\n",
    "\n",
    "2) question\n",
    "\n",
    "3) answer_text\n",
    "\n",
    "4) answer_start\n",
    "\n",
    "5) answer_end\n",
    "\n",
    "Our next aim should be tokenize each and store in memory as pickle files.( Along with creating a vocabulary using the contexts and questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tqdm \n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "\n",
    "class Squad_preprocessor():\n",
    "    def __init__(self,tokenizer,data_directory = \"E:\\\\Internships_19\\\\Internship(Summer_19)\\\\Q&A_Toolkit\\\\Dataset_analysis\\\\SQuAD\"):\n",
    "        self.data_directory = data_directory\n",
    "        self.glove_directory = \"E:\\\\Internships_19\\\\Internship(Summer_19)\\\\Q&A_Toolkit\\\\Dataset_analysis\\\\glove.6B\"\n",
    "        self.train_file = \"train_v2.json\"\n",
    "        self.validation_file = \"validation_v2.json\"\n",
    "        self.out_prefix = \"train\"\n",
    "        self.tokenizer = tokenizer\n",
    "        self.num_train_examples = 0\n",
    "        self.context_lengths = None\n",
    "        self.vocab = {}\n",
    "        \n",
    "    def load_data(self,filename = \"train_v2.json\"):\n",
    "        full_path = os.path.join(self.data_directory,filename)\n",
    "        \n",
    "        with open(full_path) as datafile:\n",
    "            self.data = json.load(datafile)\n",
    "            \n",
    "#         print(len(self.data[\"data\"]))\n",
    "            \n",
    "    def break_file(self, prefix, filename = \"train_v2.json\", count_examples = False):\n",
    "        self.load_data(filename)\n",
    "        self.out_prefix = prefix\n",
    "        \n",
    "        ##### creating data directories for different parts of the data namely:\n",
    "        ## 1) context\n",
    "        ## 2) question\n",
    "        ## 3) answer_text\n",
    "        ## 4) answer_start\n",
    "        ## 5) answer_end\n",
    "      \n",
    "        \n",
    "        ###       the SQuAD dataset has the following layout:\n",
    "        # \"data\" ---> \"title\", \"paragraphs\" \n",
    "        #                            |\n",
    "        #                            -----> \"context\" , \"qas\"\n",
    "        #                                                 |\n",
    "        #                                                 -----> \"answers\", \"id\", \"is_impossible\", \"question\"\n",
    "        #\n",
    "        #    ie. one context has several questions and their respective answers     \n",
    "\n",
    "        \n",
    "        with open(os.path.join(self.data_directory, self.out_prefix +'.context'), 'w', encoding='utf-8') as context_file, \\\n",
    "             open(os.path.join(self.data_directory, self.out_prefix +'.question'), 'w', encoding='utf-8') as question_file, \\\n",
    "             open(os.path.join(self.data_directory, self.out_prefix + '.answer_text'), 'w', encoding= 'utf-8') as answer_text_file, \\\n",
    "             open(os.path.join(self.data_directory, self.out_prefix + '.answer_start'), 'w', encoding= 'utf-8') as answer_start_file, \\\n",
    "             open(os.path.join(self.data_directory, self.out_prefix + '.answer_end'), 'w', encoding= 'utf-8') as answer_end_file:\n",
    "                   \n",
    "                    for article_idx in tqdm.tqdm(range(len(self.data[\"data\"]))):\n",
    "                        paragraphs = self.data[\"data\"][article_idx][\"paragraphs\"] ## all the paragraphs in data directory\n",
    "\n",
    "                        for paragraph_idx in range(len(paragraphs)):\n",
    "                            context = paragraphs[paragraph_idx][\"context\"] ## each context in a given paragraph directory\n",
    "                            context = context.lower()\n",
    "                            context_tokens = self.tokenizer(context)\n",
    "\n",
    "                            ## each context has a range of \"answers\", \"id\", \"is_impossible\", \"question\" \n",
    "\n",
    "                            qas = paragraphs[paragraph_idx][\"qas\"] ##  \"qas\" referrring to a single \"context\"\n",
    "\n",
    "                            for qas_idx in range(len(qas)):  ### disecting the \"qas\" into \"answers\", \"id\", \"is_impossible\", \"question\" \n",
    "                                question = qas[qas_idx][\"question\"]  \n",
    "                                question = question.lower()\n",
    "                                question_tokens = self.tokenizer(question)\n",
    "\n",
    "                                ## we select the first answer id from the range of answers we are given for a particular question\n",
    "                                \n",
    "                                if(len(qas[qas_idx][\"answers\"]) == 0 ):\n",
    "                                    \n",
    "                                    answer_text_tokens = \"<unk>\"\n",
    "                                    word_level_answer_start = -1\n",
    "                                    word_level_answer_end = -1\n",
    "                                    \n",
    "                                    \n",
    "                                    \n",
    "                                else:\n",
    "                                    \n",
    "                                    answer_id = 0\n",
    "                                    answer_text = qas[qas_idx][\"answers\"][answer_id][\"text\"]\n",
    "#                                     print(answer_text)\n",
    "                                    \n",
    "                                    answer_text = answer_text.lower()\n",
    "                                    answer_text_tokens = self.tokenizer(answer_text) ## we atke the first option as the answer\n",
    "\n",
    "                                    char_level_answer_start = qas[qas_idx][\"answers\"][answer_id][\"answer_start\"]\n",
    "                                    word_level_answer_start = len(context[:char_level_answer_start].split())\n",
    "                                    word_level_answer_end = word_level_answer_start + len(answer_text.split()) - 1\n",
    "\n",
    "\n",
    "                                context_file.write(' '.join(token for token in context_tokens)+'\\n')\n",
    "                                question_file.write(' '.join(token for token in question_tokens)+'\\n')\n",
    "                                answer_text_file.write(' '.join(token for token in answer_text_tokens)+'\\n')\n",
    "                                answer_start_file.write(str(word_level_answer_start)+ \"\\n\")\n",
    "                                answer_end_file.write(str(word_level_answer_end) + \"\\n\")\n",
    "\n",
    "    \n",
    "    def conduct_preprocess(self):\n",
    "        self.break_file(\"train\", self.train_file, True)\n",
    "        self.break_file(\"validation\", self.validation_file, False)\n",
    "        \n",
    "            \n",
    "            \n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
