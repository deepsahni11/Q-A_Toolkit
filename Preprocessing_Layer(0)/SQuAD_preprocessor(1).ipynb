{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file just processeses the SQuAD Dataset and stores it in a file format. The five files are :\n",
    "\n",
    "1) context\n",
    "\n",
    "2) question\n",
    "\n",
    "3) answer_text\n",
    "\n",
    "4) answer_start\n",
    "\n",
    "5) answer_end\n",
    "\n",
    "Our next aim should be tokenize each and store in memory as pickle files.( Along with creating a vocabulary using the contexts and questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tqdm \n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "\n",
    "class Squad_preprocessor():\n",
    "    def __init__(self,tokenizer,data_directory = \"E:\\\\Internships_19\\\\Internship(Summer_19)\\\\Q&A_Toolkit\\\\Dataset_analysis\\\\SQuAD\"):\n",
    "        self.data_directory = data_directory\n",
    "        self.glove_directory = \"E:\\\\Internships_19\\\\Internship(Summer_19)\\\\Q&A_Toolkit\\\\Dataset_analysis\\\\glove.6B\"\n",
    "        self.train_file = \"train_v2.json\"\n",
    "        self.validation_file = \"validation_v2.json\"\n",
    "        self.out_prefix = \"train\"\n",
    "        self.tokenizer = tokenizer\n",
    "        self.num_train_examples = 0\n",
    "        self.context_lengths = None\n",
    "        self.vocab = {}\n",
    "        \n",
    "    def load_data(self,filename = \"train_v2.json\"):\n",
    "        full_path = os.path.join(self.data_directory,filename)\n",
    "        \n",
    "        with open(full_path) as datafile:\n",
    "            self.data = json.load(datafile)\n",
    "            \n",
    "#         print(len(self.data[\"data\"]))\n",
    "            \n",
    "    def break_file(self, prefix, filename = \"train_v2.json\", count_examples = False):\n",
    "        self.load_data(filename)\n",
    "        self.out_prefix = prefix\n",
    "        \n",
    "        ##### creating data directories for different parts of the data namely:\n",
    "        ## 1) context\n",
    "        ## 2) question\n",
    "        ## 3) answer_text\n",
    "        ## 4) answer_start\n",
    "        ## 5) answer_end\n",
    "      \n",
    "        \n",
    "        ###       the SQuAD dataset has the following layout:\n",
    "        # \"data\" ---> \"title\", \"paragraphs\" \n",
    "        #                            |\n",
    "        #                            -----> \"context\" , \"qas\"\n",
    "        #                                                 |\n",
    "        #                                                 -----> \"answers\", \"id\", \"is_impossible\", \"question\"\n",
    "        #\n",
    "        #    ie. one context has several questions and their respective answers     \n",
    "\n",
    "        \n",
    "        with open(os.path.join(self.data_directory, self.out_prefix +'.context'), 'w', encoding='utf-8') as context_file, \\\n",
    "             open(os.path.join(self.data_directory, self.out_prefix +'.question'), 'w', encoding='utf-8') as question_file, \\\n",
    "             open(os.path.join(self.data_directory, self.out_prefix + '.answer_text'), 'w', encoding= 'utf-8') as answer_text_file, \\\n",
    "             open(os.path.join(self.data_directory, self.out_prefix + '.answer_start'), 'w', encoding= 'utf-8') as answer_start_file, \\\n",
    "             open(os.path.join(self.data_directory, self.out_prefix + '.answer_end'), 'w', encoding= 'utf-8') as answer_end_file:\n",
    "                   \n",
    "                    for article_idx in tqdm.tqdm(range(len(self.data[\"data\"]))):\n",
    "                        paragraphs = self.data[\"data\"][article_idx][\"paragraphs\"] ## all the paragraphs in data directory\n",
    "\n",
    "                        for paragraph_idx in range(len(paragraphs)):\n",
    "                            context = paragraphs[paragraph_idx][\"context\"] ## each context in a given paragraph directory\n",
    "                            context = context.lower()\n",
    "                            context_tokens = self.tokenizer(context)\n",
    "\n",
    "                            ## each context has a range of \"answers\", \"id\", \"is_impossible\", \"question\" \n",
    "\n",
    "                            qas = paragraphs[paragraph_idx][\"qas\"] ##  \"qas\" referrring to a single \"context\"\n",
    "\n",
    "                            for qas_idx in range(len(qas)):  ### disecting the \"qas\" into \"answers\", \"id\", \"is_impossible\", \"question\" \n",
    "                                question = qas[qas_idx][\"question\"]  \n",
    "                                question = question.lower()\n",
    "                                question_tokens = self.tokenizer(question)\n",
    "\n",
    "                                ## we select the first answer id from the range of answers we are given for a particular question\n",
    "                                \n",
    "                                if(len(qas[qas_idx][\"answers\"]) == 0 ):\n",
    "                                    \n",
    "                                    answer_text_tokens = \"<unk>\"\n",
    "                                    word_level_answer_start = -1\n",
    "                                    word_level_answer_end = -1\n",
    "                                    \n",
    "                                    \n",
    "                                    \n",
    "                                else:\n",
    "                                    \n",
    "                                    answer_id = 0\n",
    "                                    answer_text = qas[qas_idx][\"answers\"][answer_id][\"text\"]\n",
    "#                                     print(answer_text)\n",
    "                                    \n",
    "                                    answer_text = answer_text.lower()\n",
    "                                    answer_text_tokens = self.tokenizer(answer_text) ## we atke the first option as the answer\n",
    "\n",
    "                                    char_level_answer_start = qas[qas_idx][\"answers\"][answer_id][\"answer_start\"]\n",
    "                                    word_level_answer_start = len(context[:char_level_answer_start].split())\n",
    "                                    word_level_answer_end = word_level_answer_start + len(answer_text.split()) - 1\n",
    "\n",
    "\n",
    "                                context_file.write(' '.join(token for token in context_tokens)+'\\n')\n",
    "                                question_file.write(' '.join(token for token in question_tokens)+'\\n')\n",
    "                                answer_text_file.write(' '.join(token for token in answer_text_tokens)+'\\n')\n",
    "                                answer_start_file.write(str(word_level_answer_start)+ \"\\n\")\n",
    "                                answer_end_file.write(str(word_level_answer_end) + \"\\n\")\n",
    "\n",
    "    \n",
    "    def conduct_preprocess(self):\n",
    "        self.break_file(\"train\", self.train_file, True)\n",
    "        self.break_file(\"validation\", self.validation_file, False)\n",
    "        \n",
    "            \n",
    "            \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "from six.moves import xrange\n",
    "\n",
    "class Batch(object):\n",
    "    \"\"\"\n",
    "    This class contains all the information required to train a batch\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,context_word_indexes,context_word_mask,context_word_tokens,question_word_indexes,question_word_mask,question_word_tokens,answer_tokens,answer_start,answer_end):\n",
    "         \"\"\"\n",
    "        Inputs:\n",
    "        \n",
    "          {context/question}_word_indexes: Numpy arrays.\n",
    "            Shape (batch_size, {context_len/question_len}). Contains padding.\n",
    "          {context/question}_mask: Numpy arrays, same shape as _word_indexes.\n",
    "            Contains 1s where there is real data, 0s where there is padding.\n",
    "          {context/question/answer}_tokens: Lists length batch_size, containing lists (unpadded) of tokens (strings)\n",
    "         \n",
    "        \"\"\"\n",
    "            \n",
    "            # ans_span: numpy array, shape (batch_size, 2)\n",
    "            \n",
    "            self.context_word_indexes = context_word_indexes\n",
    "            self.context_word_mask = context_word_mask\n",
    "            self.context_word_tokens = context_word_tokens\n",
    "            self.question_word_indexes = question_word_indexes\n",
    "            self.question_word_mask = question_word_mask\n",
    "            self.question_word_tokens = question_word_tokens\n",
    "            self.answer_tokens = answer_tokens\n",
    "            self.answer_start = answer_start\n",
    "            self.answer_end = answer_end       \n",
    "            \n",
    "         \n",
    "\n",
    "    def generate_batches(word_to_index, context_path, question_path, answer_path, batch_size, max_context_len, max_question_len, discard_long):\n",
    "        \"\"\"\n",
    "        This function returns a generator object that yields batches.\n",
    "        The last batch in the dataset will be a partial batch.\n",
    "        Read this to understand generators and the yield keyword in Python: https://stackoverflow.com/questions/231767/what-does-the-yield-keyword-do\n",
    "        \n",
    "        Inputs:\n",
    "        \n",
    "          word_to_index: dictionary mapping word (string) to index (int)\n",
    "          context_tokens_file, question_tokens_file, answer_tokens_file: paths to {train/validation}.{context/question/answer} data files\n",
    "          batch_size: integer, specifies how big the batches should be\n",
    "          max_context_length, max_question_length: max length of context and question respectively\n",
    "          discard_long: If True, discard any examples that are longer than context_len or question_len.\n",
    "                        If False, truncate those exmaples instead.\n",
    "        \"\"\"\n",
    "        context_file, question_file, answer_file = open(context_path), open(question_path), open(answer_path)\n",
    "        batches = []\n",
    "\n",
    "        while True:\n",
    "            if len(batches) == 0: # add more batches\n",
    "                refill_batches(batches, word_to_index, context_file, question_file, answer_file, batch_size, max_context_len, max_question_len, discard_long)\n",
    "            if len(batches) == 0:\n",
    "                break\n",
    "\n",
    "            # Get next batch. These are all lists length batch_size\n",
    "            (context_word_indexes, context_word_tokens, question_word_indexes, question_word_tokens, answer_start,answer_end, answer_tokens) = batches.pop(0)\n",
    "\n",
    "            # Pad context_ids and qn_ids\n",
    "            qn_ids = padded(qn_ids, question_len) # pad questions to length question_len\n",
    "            context_ids = padded(context_ids, context_len) # pad contexts to length context_len\n",
    "\n",
    "            # Make qn_ids into a np array and create qn_mask\n",
    "            qn_ids = np.array(qn_ids) # shape (batch_size, question_len)\n",
    "            qn_mask = (qn_ids != PAD_ID).astype(np.int32) # shape (batch_size, question_len)\n",
    "\n",
    "            # Make context_ids into a np array and create context_mask\n",
    "            context_ids = np.array(context_ids) # shape (batch_size, context_len)\n",
    "            context_mask = (context_ids != PAD_ID).astype(np.int32) # shape (batch_size, context_len)\n",
    "\n",
    "            # Make ans_span into a np array\n",
    "            ans_span = np.array(ans_span) # shape (batch_size, 2)\n",
    "\n",
    "            # Make into a Batch object\n",
    "            batch = Batch(context_ids, context_mask, context_tokens, qn_ids, qn_mask, qn_tokens, ans_span, ans_tokens)\n",
    "\n",
    "            yield batch\n",
    "\n",
    "        return\n",
    "    \n",
    "    def refill_batches(batches, word2id, context_file, qn_file, ans_file, batch_size, context_len, question_len, discard_long):\n",
    "        \"\"\"\n",
    "        Adds more batches into the \"batches\" list.\n",
    "        Inputs:\n",
    "          batches: list to add batches to\n",
    "          word2id: dictionary mapping word (string) to word id (int)\n",
    "          context_file, qn_file, ans_file: paths to {train/dev}.{context/question/answer} data files\n",
    "          batch_size: int. how big to make the batches\n",
    "          context_len, question_len: max length of context and question respectively\n",
    "          discard_long: If True, discard any examples that are longer than context_len or question_len.\n",
    "            If False, truncate those exmaples instead.\n",
    "        \"\"\"\n",
    "        print \"Refilling batches...\"\n",
    "        tic = time.time()\n",
    "        examples = [] # list of (qn_ids, context_ids, ans_span, ans_tokens) triples\n",
    "        context_line, qn_line, ans_line = context_file.readline(), qn_file.readline(), ans_file.readline() # read the next line from each\n",
    "\n",
    "        while context_line and qn_line and ans_line: # while you haven't reached the end\n",
    "\n",
    "            # Convert tokens to word ids\n",
    "            context_tokens, context_ids = sentence_to_token_ids(context_line, word2id)\n",
    "            qn_tokens, qn_ids = sentence_to_token_ids(qn_line, word2id)\n",
    "            ans_span = intstr_to_intlist(ans_line)\n",
    "\n",
    "            # read the next line from each file\n",
    "            context_line, qn_line, ans_line = context_file.readline(), qn_file.readline(), ans_file.readline()\n",
    "\n",
    "            # get ans_tokens from ans_span\n",
    "            assert len(ans_span) == 2\n",
    "            if ans_span[1] < ans_span[0]:\n",
    "                print \"Found an ill-formed gold span: start=%i end=%i\" % (ans_span[0], ans_span[1])\n",
    "                continue\n",
    "            ans_tokens = context_tokens[ans_span[0] : ans_span[1]+1] # list of strings\n",
    "\n",
    "            # discard or truncate too-long questions\n",
    "            if len(qn_ids) > question_len:\n",
    "                if discard_long:\n",
    "                    continue\n",
    "                else: # truncate\n",
    "                    qn_ids = qn_ids[:question_len]\n",
    "\n",
    "            # discard or truncate too-long contexts\n",
    "            if len(context_ids) > context_len:\n",
    "                if discard_long:\n",
    "                    continue\n",
    "                else: # truncate\n",
    "                    context_ids = context_ids[:context_len]\n",
    "\n",
    "            # add to examples\n",
    "            examples.append((context_ids, context_tokens, qn_ids, qn_tokens, ans_span, ans_tokens))\n",
    "\n",
    "            # stop refilling if you have 160 batches\n",
    "            if len(examples) == batch_size * 160:\n",
    "                break\n",
    "\n",
    "        # Once you've either got 160 batches or you've reached end of file:\n",
    "\n",
    "        # Sort by question length\n",
    "        # Note: if you sort by context length, then you'll have batches which contain the same context many times (because each context appears several times, with different questions)\n",
    "        examples = sorted(examples, key=lambda e: len(e[2]))\n",
    "\n",
    "        # Make into batches and append to the list batches\n",
    "        for batch_start in xrange(0, len(examples), batch_size):\n",
    "\n",
    "            # Note: each of these is a list length batch_size of lists of ints (except on last iter when it might be less than batch_size)\n",
    "            context_ids_batch, context_tokens_batch, qn_ids_batch, qn_tokens_batch, ans_span_batch, ans_tokens_batch = zip(*examples[batch_start:batch_start+batch_size])\n",
    "\n",
    "            batches.append((context_ids_batch, context_tokens_batch, qn_ids_batch, qn_tokens_batch, ans_span_batch, ans_tokens_batch))\n",
    "\n",
    "        # shuffle the batches\n",
    "        random.shuffle(batches)\n",
    "\n",
    "        toc = time.time()\n",
    "        print \"Refilling batches took %.2f seconds\" % (toc-tic)\n",
    "        return"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
