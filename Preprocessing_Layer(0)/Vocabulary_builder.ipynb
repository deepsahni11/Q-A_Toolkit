{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os.path\n",
    "import operator\n",
    "import pickle\n",
    "from nltk.tokenize import WhitespaceTokenizer \n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from collections import defaultdict\n",
    "from math import sqrt\n",
    "import numpy as np \n",
    "import codecs\n",
    "import re\n",
    "import string\n",
    "import sys\n",
    "import tqdm as tqdm\n",
    "import os\n",
    "from collections import Counter\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "\n",
    "class Vocabulary():\n",
    "    def __init__(self, vocab_input_files = [\"E:\\\\Internships_19\\\\Internship(Summer_19)\\\\Q&A_Toolkit\\\\Dataset_analysis\\\\SQuAD\\\\train.context\",\"E:\\\\Internships_19\\\\Internship(Summer_19)\\\\Q&A_Toolkit\\\\Dataset_analysis\\\\SQuAD\\\\train.question\"],\n",
    "                        vocab_output_filename = \"E:\\\\Internships_19\\\\Internship(Summer_19)\\\\Q&A_Toolkit\\\\Dataset_analysis\\\\SQuAD\\\\vocab.dat\"):\n",
    "        \"\"\"\n",
    "        This function works the same as contructors and is used to initilaize the parameters used in the making the model\n",
    "        \n",
    "        \"\"\"\n",
    "        self.vocab = {}\n",
    "        self.vocab_output_filename = vocab_output_filename\n",
    "        self.vocab_input_files = vocab_input_files\n",
    "        self.word_list = []\n",
    "        self.word_to_index = {} # dictionary with keys as words and values as their corresponding index number\n",
    "        self.char_to_index = {} # dictionary with keys as characters and values as their corresponding index number\n",
    "        self.word_to_index[\"<pad>\"] = 0\n",
    "        self.word_to_index[\"<unk>\"] = 1\n",
    "        ## self.index_to_word = # dictionary with values as words and keys as their corresponding index number\n",
    "        ## self.index_to_char = # dictionary with values as characters and keys as their corresponding index number\n",
    "        \n",
    "    \n",
    "     \n",
    "    \n",
    "    \n",
    "    def create_vocabulary(self,vocab_freq = 0, vocab_size = 30000, data_path=\"E:\\\\Internships_19\\\\Internship(Summer_19)\\\\Q&A_Toolkit\\\\Dataset_analysis\\\\SQuAD\"):\n",
    "        \"\"\"\n",
    "        This function creates dictionaries namely:\n",
    "        1) word_to_index\n",
    "        2) char_to_index\n",
    "        3) index_to_word\n",
    "        4) index_to_char\n",
    "        \n",
    "        and dumps them into pickle file namely: \"dictionaries.pkl\"\n",
    "        \"\"\"\n",
    "        \n",
    "\n",
    "        for filename in self.vocab_input_files:\n",
    "            with open(filename,'r', encoding = 'utf-8') as file_input:\n",
    "                \n",
    "                for line in tqdm.tqdm(file_input):\n",
    "                    words = line.strip().split()\n",
    "#                     print(words)\n",
    "                    for word in words:\n",
    "                        if not (word in self.vocab):\n",
    "                            self.vocab[word] = 1\n",
    "                        else:\n",
    "                            self.vocab[word] +=1 \n",
    "\n",
    "        if vocab_freq == 0:\n",
    "            vocab_words = sorted(self.vocab,key=self.vocab.get,reverse=True)\n",
    "\n",
    "\n",
    "                    \n",
    "        temp_index = 2\n",
    "        for word in vocab_words:\n",
    "            if word not in self.word_to_index:\n",
    "                self.word_to_index[word] = temp_index\n",
    "                temp_index += 1\n",
    "                \n",
    "#         print(len(self.word_to_index))\n",
    "\n",
    "        self.vocab_size = len(self.word_to_index)\n",
    "        self.index_to_word =  {v: k for k, v in self.word_to_index.items()}\n",
    "\n",
    "\n",
    "        characters = list(string.printable.lower())\n",
    "        characters.remove(' ')\n",
    "\n",
    "        char_ind = 1\n",
    "        for c in characters:\n",
    "            if c not in self.char_to_index:\n",
    "                self.char_to_index[c] = char_ind\n",
    "                char_ind += 1\n",
    "\n",
    "\n",
    "        self.index_to_char = {v: k for k,v in self.char_to_index.items()}\n",
    "\n",
    "        dict_all = {\"word_to_index\" : self.word_to_index, \"char_to_index\" : self.char_to_index,\"index_to_word\": self.index_to_word, \"index_to_char\": self.index_to_char}\n",
    "\n",
    "        pickle.dump(dict_all, open(os.path.join(data_path, \"dictionaries.pkl\"), \"wb\")) ## creates dictionaries and stores in memory as pickle files\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\n",
      "1373it [00:00, 13696.67it/s]\n",
      "3009it [00:00, 14390.88it/s]\n",
      "4813it [00:00, 15311.26it/s]\n",
      "5897it [00:00, 13288.71it/s]\n",
      "6965it [00:00, 12350.04it/s]\n",
      "8539it [00:00, 13178.90it/s]\n",
      "10323it [00:00, 14274.59it/s]\n",
      "12468it [00:00, 15841.04it/s]\n",
      "14724it [00:00, 17365.18it/s]\n",
      "17007it [00:01, 18676.33it/s]\n",
      "19176it [00:01, 19476.02it/s]\n",
      "21395it [00:01, 20177.50it/s]\n",
      "23460it [00:01, 18383.20it/s]\n",
      "25362it [00:01, 15089.78it/s]\n",
      "27310it [00:01, 16181.13it/s]\n",
      "29050it [00:01, 15592.13it/s]\n",
      "30699it [00:01, 14943.56it/s]\n",
      "32262it [00:01, 15088.84it/s]\n",
      "33860it [00:02, 15313.33it/s]\n",
      "35426it [00:02, 14986.06it/s]\n",
      "37032it [00:02, 15260.98it/s]\n",
      "38578it [00:02, 13778.75it/s]\n",
      "39997it [00:02, 12958.84it/s]\n",
      "41551it [00:02, 13613.58it/s]\n",
      "42984it [00:02, 13791.69it/s]\n",
      "44389it [00:02, 13365.54it/s]\n",
      "45747it [00:02, 13400.06it/s]\n",
      "47172it [00:03, 13615.69it/s]\n",
      "48568it [00:03, 13687.29it/s]\n",
      "50221it [00:03, 14416.76it/s]\n",
      "51679it [00:03, 14285.46it/s]\n",
      "53119it [00:03, 12173.77it/s]\n",
      "54729it [00:03, 13126.91it/s]\n",
      "56419it [00:03, 14043.46it/s]\n",
      "57887it [00:03, 14076.36it/s]\n",
      "59339it [00:03, 13430.73it/s]\n",
      "60872it [00:04, 13921.43it/s]\n",
      "62435it [00:04, 14364.44it/s]\n",
      "63897it [00:04, 14282.59it/s]\n",
      "65498it [00:04, 14730.61it/s]\n",
      "66987it [00:04, 11890.79it/s]\n",
      "68516it [00:04, 12723.77it/s]\n",
      "69875it [00:04, 12852.89it/s]\n",
      "71393it [00:04, 13463.87it/s]\n",
      "72812it [00:04, 13644.91it/s]\n",
      "74212it [00:05, 11667.35it/s]\n",
      "75454it [00:05, 11691.53it/s]\n",
      "76868it [00:05, 12308.60it/s]\n",
      "78145it [00:05, 10677.41it/s]\n",
      "79739it [00:05, 11831.95it/s]\n",
      "81333it [00:05, 12800.82it/s]\n",
      "82814it [00:05, 13317.31it/s]\n",
      "84330it [00:05, 13793.52it/s]\n",
      "85794it [00:05, 14007.54it/s]\n",
      "87231it [00:06, 13194.20it/s]\n",
      "88607it [00:06, 13322.60it/s]\n",
      "90356it [00:06, 14322.54it/s]\n",
      "91827it [00:06, 12453.11it/s]\n",
      "93221it [00:06, 12838.84it/s]\n",
      "94750it [00:06, 13461.35it/s]\n",
      "96141it [00:06, 13179.37it/s]\n",
      "97492it [00:06, 12376.75it/s]\n",
      "98763it [00:07, 11957.53it/s]\n",
      "99985it [00:07, 11869.58it/s]\n",
      "101191it [00:07, 11236.61it/s]\n",
      "102635it [00:07, 12015.79it/s]\n",
      "103866it [00:07, 10807.63it/s]\n",
      "105372it [00:07, 11787.58it/s]\n",
      "107005it [00:07, 12843.07it/s]\n",
      "108510it [00:07, 13424.08it/s]\n",
      "110110it [00:07, 14078.38it/s]\n",
      "111677it [00:07, 14492.38it/s]\n",
      "113242it [00:08, 14790.62it/s]\n",
      "114748it [00:08, 13522.77it/s]\n",
      "116140it [00:08, 13610.34it/s]\n",
      "117529it [00:08, 12420.60it/s]\n",
      "118820it [00:08, 12536.74it/s]\n",
      "120131it [00:08, 12676.24it/s]\n",
      "121419it [00:08, 12709.10it/s]\n",
      "122704it [00:08, 10303.62it/s]\n",
      "123818it [00:09, 10401.66it/s]\n",
      "124939it [00:09, 10624.29it/s]\n",
      "126081it [00:09, 10828.48it/s]\n",
      "127438it [00:09, 11505.73it/s]\n",
      "128622it [00:09, 10153.14it/s]\n",
      "130181it [00:09, 11321.32it/s]\n",
      "130319it [00:09, 13573.25it/s]\n",
      "0it [00:00, ?it/s]\n",
      "11370it [00:00, 112874.73it/s]\n",
      "22509it [00:00, 112177.81it/s]\n",
      "39383it [00:00, 124651.70it/s]\n",
      "54743it [00:00, 131873.83it/s]\n",
      "68726it [00:00, 133882.20it/s]\n",
      "80519it [00:00, 123140.83it/s]\n",
      "93460it [00:00, 124867.38it/s]\n",
      "105290it [00:00, 101861.45it/s]\n",
      "121582it [00:00, 114556.56it/s]\n",
      "130319it [00:01, 123951.79it/s]"
     ]
    }
   ],
   "source": [
    "import Squad_processor\n",
    "import spacy\n",
    "\n",
    "import nltk\n",
    "# nltk.download('popular')\n",
    "# tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# preprocess = Squad_preprocessor(nltk.word_tokenize,\"E:\\\\Internships_19\\\\Internship(Summer_19)\\\\Q&A_Toolkit\\\\Dataset_analysis\\\\SQuAD\")\n",
    "# preprocess.conduct_preprocess()\n",
    "vocab = Vocabulary([\"E:\\\\Internships_19\\\\Internship(Summer_19)\\\\Q&A_Toolkit\\\\Dataset_analysis\\\\SQuAD\\\\train.context\",\"E:\\\\Internships_19\\\\Internship(Summer_19)\\\\Q&A_Toolkit\\\\Dataset_analysis\\\\SQuAD\\\\train.question\"],\n",
    "                        \"E:\\\\Internships_19\\\\Internship(Summer_19)\\\\Q&A_Toolkit\\\\Dataset_analysis\\\\SQuAD\\\\vocab.dat\")\n",
    "vocab.create_vocabulary(0,30000, \"E:\\\\Internships_19\\\\Internship(Summer_19)\\\\Q&A_Toolkit\\\\Dataset_analysis\\\\SQuAD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
