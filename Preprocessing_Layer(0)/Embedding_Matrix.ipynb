{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-c66bc433caab>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;31m# import ujson as json\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0murllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\spacy\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mthinc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mneural\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mprefer_gpu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequire_gpu\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mcli\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcli_info\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mglossary\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mexplain\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mabout\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\spacy\\cli\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mpackage\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpackage\u001b[0m  \u001b[1;31m# noqa: F401\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mprofile\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mprofile\u001b[0m  \u001b[1;31m# noqa: F401\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain\u001b[0m  \u001b[1;31m# noqa: F401\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mpretrain\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpretrain\u001b[0m  \u001b[1;31m# noqa: F401\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mdebug_data\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdebug_data\u001b[0m  \u001b[1;31m# noqa: F401\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\spacy\\cli\\train.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ml\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcreate_default_optimizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattrs\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPROB\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIS_OOV\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCLUSTER\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLANG\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgold\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mGoldCorpus\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\spacy\\_ml.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m     \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[0mthinc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextra\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrappers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPyTorchWrapperRNN\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[1;32mdel\u001b[0m \u001b[0m_dl_flags\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m __all__ += [name for name in dir(_C)\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36mmodule_from_spec\u001b[1;34m(spec)\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36mcreate_module\u001b[1;34m(self, spec)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import tqdm as tqdm\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import spacy\n",
    "# import ujson as json\n",
    "import urllib.request\n",
    "\n",
    "# from args import get_setup_args\n",
    "from codecs import open\n",
    "from collections import Counter\n",
    "from subprocess import run\n",
    "from tqdm import tqdm\n",
    "from zipfile import ZipFile\n",
    "\n",
    "\n",
    "\n",
    "class Embedding_Matrix():\n",
    "    def __init__(self,embedding_dir = \"E:\\\\Internships_19\\\\Internship(Summer_19)\\\\Q&A_Toolkit\\\\Dataset_analysis\\\\SQuAD\"):\n",
    "#         embedding_dir = \"E:\\\\Internships_19\\\\Internship(Summer_19)\\\\Q&A_Toolkit\\\\Dataset_analysis\\\\SQuAD\"\n",
    "        with open(r\"E:\\\\Internships_19\\\\Internship(Summer_19)\\\\Q&A_Toolkit\\\\Dataset_analysis\\\\SQuAD\\\\dictionaries.pkl\", \"rb\") as input_file:\n",
    "            dictionaries = pickle.load(input_file)\n",
    "        self.word_to_index = dictionaries[\"word_to_index\"]\n",
    "        self.char_to_index = dictionaries[\"char_to_index\"]\n",
    "        self.index_to_word = dictionaries[\"index_to_word\"]\n",
    "        self.index_to_char = dictionaries[\"index_to_char\"]\n",
    "        \n",
    "        \n",
    "    def index_files_using_char_to_index(self, filename, _dict, max_words, max_chars):\n",
    "\n",
    "        f = codecs.open(filename, \"r\", encoding=\"utf-8\")\n",
    "        lines = f.readlines()\n",
    "        lines = [l.lower() for l in lines]\n",
    "        encoded_lines = []\n",
    "        for l in lines:\n",
    "            tokens = l.split()\n",
    "            tokens = tokens[:max_words]\n",
    "            encoded_tokens = []\n",
    "            for t in tokens:\n",
    "                l = list(t)\n",
    "                l = l[:max_chars] ## there is a max limit for the length of characters = max_chars\n",
    "                encoded_chars = []\n",
    "                for j in l:\n",
    "                    if j in _dict:\n",
    "                        encoded_chars.append(_dict[j])\n",
    "                    else:\n",
    "                        encoded_chars.append(0)  ## if the character id not in dictionary put '0' in its place\n",
    "                encoded_tokens.append(encoded_chars)\n",
    "            encoded_lines.append(encoded_tokens)\n",
    "\n",
    "        return encoded_lines\n",
    "\n",
    "    def index_files_using_word_to_index(self, filename, _dict, max_words):\n",
    "        f = codecs.open(filename, \"r\", encoding=\"utf-8\")\n",
    "\n",
    "        lines = f.readlines()\n",
    "        lines  = [l.lower() for l in lines]\n",
    "        encoded_lines = []\n",
    "        for l in lines:\n",
    "            tokens = l.split()\n",
    "            tokens = tokens[:max_words]\n",
    "            temp = []\n",
    "            for t in tokens:\n",
    "                if t in _dict:\n",
    "                    temp.append(_dict[t])\n",
    "                else:\n",
    "                    temp.append(1)\n",
    "\n",
    "            encoded_lines.append(temp[:])\n",
    "\n",
    "        return encoded_lines\n",
    "    def index_files_to_char_level_and_word_level(self, datapath = \"E:\\\\Internships_19\\\\Internship(Summer_19)\\\\Q&A_Toolkit\\\\Dataset_analysis\\\\SQuAD\", max_words=0, max_chars=0):\n",
    "#         files = [\".context\", \".question\", \".answer_text\"]\n",
    "        files = [\".question\", \".answer_text\"]\n",
    "\n",
    "\n",
    "        for f in files:\n",
    "            read_path_train = os.path.join(datapath, \"train\" + f)\n",
    "            write_path_train_word = os.path.join(datapath, \"train_word_index\" + f + \"_pkl.pkl\")\n",
    "            write_path_train_char = os.path.join(datapath, \"train_char_index\" + f + \"_pkl.pkl\")\n",
    "\n",
    "            read_path_valid = os.path.join(datapath, \"validation\" + f)\n",
    "            write_path_valid_word = os.path.join(datapath, \"validation_word_index\" + f + \"_pkl.pkl\")\n",
    "            write_path_valid_char = os.path.join(datapath, \"validation_char_index\" + f + \"_pkl.pkl\")\n",
    "\n",
    "\n",
    "\n",
    "            temp_train_word = index_files_using_word_to_index(self, read_path_train, self.word_to_index, max_words)\n",
    "            temp_valid_word = index_files_using_word_to_index(self, read_path_valid, self.word_to_index, max_words)\n",
    "\n",
    "            temp_train_char = index_files_using_char_to_index(self, read_path_train, self.char_to_index, max_words,max_chars)\n",
    "            temp_valid_char = index_files_using_char_to_index(self, read_path_valid, self.char_to_index, max_words,max_chars)\n",
    "\n",
    "            write_file_train_word = open(write_path_train_word, \"wb\")\n",
    "            pickle.dump(temp_train_word, write_file_train_word)\n",
    "\n",
    "            write_file_train_char = open(write_path_train_char, \"wb\")\n",
    "            pickle.dump(temp_train_char, write_file_train_char)\n",
    "\n",
    "            write_file_valid_word = open(write_path_valid_word, \"wb\")\n",
    "            pickle.dump(temp_valid_word, write_file_valid_word)\n",
    "\n",
    "            write_file_valid_char = open(write_path_valid_char, \"wb\")\n",
    "            pickle.dump(temp_valid_char, write_file_valid_char)\n",
    "\n",
    "    def get_glove_embeddings(self, word_embedding_size = 100, char_embedding_size = 20 , embedding_dir = \"E:\\\\Internships_19\\\\Internship(Summer_19)\\\\Q&A_Toolkit\\\\Dataset_analysis\\\\SQuAD\" ):\n",
    "\n",
    "\n",
    "\n",
    "        glove_embeddings = os.path.join(embedding_dir, \"glove_embeddings100.txt\")\n",
    "\n",
    "        glove_embeddings = open(glove_embeddings,'r', encoding = 'utf-8')\n",
    "\n",
    "\n",
    "\n",
    "        #     glove_embeddings = pickle.load(open(glove_embeddings))\n",
    "\n",
    "        #####################  CHECK HOW GLOVE EMBEDDINGS WORK ##############\n",
    "        temp_embeddings = []\n",
    "\n",
    "        for word in self.word_to_index:\n",
    "\n",
    "                if word in ['<pad>', '<s>', '<eos>','<unk>']:\n",
    "                    temp_vector = np.zeros((word_embedding_size))\n",
    "                elif word not in glove_embeddings:\n",
    "                    temp_vector = np.random.uniform(-np.sqrt(3)/np.sqrt(word_embedding_size), np.sqrt(3)/np.sqrt(word_embedding_size), word_embedding_size)\n",
    "                else:\n",
    "                    temp_vector = glove_embeddings[word]\n",
    "\n",
    "                temp_embeddings.append(temp_vector)\n",
    "\n",
    "        temp_embeddings = np.asarray(temp_embeddings)\n",
    "        temp_embeddings = temp_embeddings.astype(np.float32)\n",
    "        self.word_embeddings = temp_embeddings\n",
    "\n",
    "\n",
    "        char_embeddings = []\n",
    "        print (char_embedding_size)\n",
    "        char_embeddings.append(np.zeros((char_embedding_size)))\n",
    "\n",
    "        for i in range(len(self.char_to_index)):\n",
    "            temp_vector = np.random.uniform(-np.sqrt(3)/np.sqrt(char_embedding_size), np.sqrt(3)/np.sqrt(char_embedding_size), char_embedding_size)\n",
    "            char_embeddings.append(temp_vector)\n",
    "\n",
    "        char_embeddings = np.asarray(char_embeddings)\n",
    "        char_embeddings = char_embeddings.astype(np.float32)\n",
    "\n",
    "        self.char_embeddings = char_embeddings\n",
    "\n",
    "        pickle.dump(char_embeddings, open(os.path.join(embedding_dir, \"char_embeddings\" + \".pkl\"), \"wb\")) \n",
    "        pickle.dump(temp_embeddings, open(os.path.join(embedding_dir, \"glove_word_embeddings\" + \".pkl\"), \"wb\"))\n",
    "\n",
    "\n",
    "#         return self.word_embeddings, self.char_embeddings\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = Embedding_Matrix(embedding_dir = \"E:\\\\Internships_19\\\\Internship(Summer_19)\\\\Q&A_Toolkit\\\\Dataset_analysis\\\\SQuAD\")\n",
    "embedding.get_glove_embeddings(word_embedding_size = 100, char_embedding_size = 20 , embedding_dir = \"E:\\\\Internships_19\\\\Internship(Summer_19)\\\\Q&A_Toolkit\\\\Dataset_analysis\\\\SQuAD\" )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding.index_files_to_char_level_and_word_level(datapath = \"E:\\\\Internships_19\\\\Internship(Summer_19)\\\\Q&A_Toolkit\\\\Dataset_analysis\\\\SQuAD\", max_words=400, max_chars=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dir = \"E:\\\\Internships_19\\\\Internship(Summer_19)\\\\Q&A_Toolkit\\\\Dataset_analysis\\\\SQuAD\"\n",
    "with open(r\"E:\\\\Internships_19\\\\Internship(Summer_19)\\\\Q&A_Toolkit\\\\Dataset_analysis\\\\SQuAD\\\\glove_word_embeddings.pkl\", \"rb\") as input_file:\n",
    "    glove = pickle.load(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.06258035,  0.0150595 ,  0.02575215,  0.17270675,  0.07960177,\n",
       "       -0.00596011,  0.13488224, -0.03397342, -0.1348676 ,  0.07859553,\n",
       "        0.09202604, -0.03717313, -0.09934395,  0.08046655,  0.16119549,\n",
       "       -0.02862258, -0.11719763,  0.160727  ,  0.17106102, -0.07843148,\n",
       "       -0.15625611, -0.10382988,  0.13577586,  0.05128137,  0.11930855,\n",
       "        0.01463436,  0.1415507 ,  0.04997107, -0.10924181, -0.05069717,\n",
       "        0.15108474,  0.11705878, -0.05256043,  0.01571849,  0.12796202,\n",
       "       -0.09618449,  0.1120216 , -0.00436207,  0.06399671, -0.00043768,\n",
       "       -0.0093321 ,  0.0866193 , -0.14117904,  0.09678099,  0.14233519,\n",
       "       -0.15919808, -0.07660368, -0.06428248,  0.10527738, -0.04710912,\n",
       "       -0.07718737,  0.06490774,  0.02479386,  0.14717807,  0.12930167,\n",
       "        0.01818467,  0.1511378 , -0.06462082, -0.06716185, -0.08174136,\n",
       "        0.02943872, -0.0541234 ,  0.08440569, -0.11220339, -0.09587555,\n",
       "       -0.09138144,  0.17025383, -0.167808  ,  0.07598063, -0.05774475,\n",
       "       -0.05262013, -0.07135279,  0.09696224, -0.01491713, -0.09321778,\n",
       "        0.1066835 , -0.15052016, -0.00859144,  0.06090625, -0.11218899,\n",
       "        0.14394672,  0.14250956, -0.03652666, -0.11909122,  0.15308909,\n",
       "       -0.03285783, -0.07330446,  0.07225059, -0.03349517, -0.12004445,\n",
       "        0.01436969, -0.00827436, -0.07883133,  0.07400639,  0.028842  ,\n",
       "       -0.00635713, -0.00534394,  0.03012634, -0.01025914,  0.03824801],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove[100]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
