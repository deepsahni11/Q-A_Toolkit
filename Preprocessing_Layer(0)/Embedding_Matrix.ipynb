{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import tqdm as tqdm\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import spacy\n",
    "# import ujson as json\n",
    "import urllib.request\n",
    "\n",
    "# from args import get_setup_args\n",
    "from codecs import open\n",
    "from collections import Counter\n",
    "from subprocess import run\n",
    "from tqdm import tqdm\n",
    "from zipfile import ZipFile\n",
    "\n",
    "\n",
    "\n",
    "class Embedding_Matrix():\n",
    "    def __init__(self,embedding_dir = \"E:\\\\Internships_19\\\\Internship(Summer_19)\\\\Q&A_Toolkit\\\\Dataset_analysis\\\\SQuAD\"):\n",
    "#         embedding_dir = \"E:\\\\Internships_19\\\\Internship(Summer_19)\\\\Q&A_Toolkit\\\\Dataset_analysis\\\\SQuAD\"\n",
    "        with open(r\"E:\\\\Internships_19\\\\Internship(Summer_19)\\\\Q&A_Toolkit\\\\Dataset_analysis\\\\SQuAD\\\\dictionaries.pkl\", \"rb\") as input_file:\n",
    "            dictionaries = pickle.load(input_file)\n",
    "        self.word_to_index = dictionaries[\"word_to_index\"]\n",
    "        self.char_to_index = dictionaries[\"char_to_index\"]\n",
    "        self.index_to_word = dictionaries[\"index_to_word\"]\n",
    "        self.index_to_char = dictionaries[\"index_to_char\"]\n",
    "        \n",
    "        \n",
    "    def index_files_using_char_to_index(self, filename, _dict, max_words, max_chars):\n",
    "\n",
    "        f = codecs.open(filename, \"r\", encoding=\"utf-8\")\n",
    "        lines = f.readlines()\n",
    "        lines = [l.lower() for l in lines]\n",
    "        encoded_lines = []\n",
    "        for l in lines:\n",
    "            tokens = l.split()\n",
    "            tokens = tokens[:max_words]\n",
    "            encoded_tokens = []\n",
    "            for t in tokens:\n",
    "                l = list(t)\n",
    "                l = l[:max_chars] ## there is a max limit for the length of characters = max_chars\n",
    "                encoded_chars = []\n",
    "                for j in l:\n",
    "                    if j in _dict:\n",
    "                        encoded_chars.append(_dict[j])\n",
    "                    else:\n",
    "                        encoded_chars.append(0)  ## if the character id not in dictionary put '0' in its place\n",
    "                encoded_tokens.append(encoded_chars)\n",
    "            encoded_lines.append(encoded_tokens)\n",
    "\n",
    "        return encoded_lines\n",
    "\n",
    "    def index_files_using_word_to_index(self, filename, _dict, max_words):\n",
    "        f = codecs.open(filename, \"r\", encoding=\"utf-8\")\n",
    "\n",
    "        lines = f.readlines()\n",
    "        lines  = [l.lower() for l in lines]\n",
    "        encoded_lines = []\n",
    "        for l in lines:\n",
    "            tokens = l.split()\n",
    "            tokens = tokens[:max_words]\n",
    "            temp = []\n",
    "            for t in tokens:\n",
    "                if t in _dict:\n",
    "                    temp.append(_dict[t])\n",
    "                else:\n",
    "                    temp.append(1)\n",
    "\n",
    "            encoded_lines.append(temp[:])\n",
    "\n",
    "        return encoded_lines\n",
    "    def index_files_to_char_level_and_word_level(self, datapath = \"E:\\\\Internships_19\\\\Internship(Summer_19)\\\\Q&A_Toolkit\\\\Dataset_analysis\\\\SQuAD\", max_words=0, max_chars=0):\n",
    "#         files = [\".context\", \".question\", \".answer_text\"]\n",
    "        files = [\".question\", \".answer_text\"]\n",
    "\n",
    "\n",
    "        for f in files:\n",
    "            read_path_train = os.path.join(datapath, \"train\" + f)\n",
    "            write_path_train_word = os.path.join(datapath, \"train_word_index\" + f + \"_pkl.pkl\")\n",
    "            write_path_train_char = os.path.join(datapath, \"train_char_index\" + f + \"_pkl.pkl\")\n",
    "\n",
    "            read_path_valid = os.path.join(datapath, \"validation\" + f)\n",
    "            write_path_valid_word = os.path.join(datapath, \"validation_word_index\" + f + \"_pkl.pkl\")\n",
    "            write_path_valid_char = os.path.join(datapath, \"validation_char_index\" + f + \"_pkl.pkl\")\n",
    "\n",
    "\n",
    "\n",
    "            temp_train_word = index_files_using_word_to_index(self, read_path_train, self.word_to_index, max_words)\n",
    "            temp_valid_word = index_files_using_word_to_index(self, read_path_valid, self.word_to_index, max_words)\n",
    "\n",
    "            temp_train_char = index_files_using_char_to_index(self, read_path_train, self.char_to_index, max_words,max_chars)\n",
    "            temp_valid_char = index_files_using_char_to_index(self, read_path_valid, self.char_to_index, max_words,max_chars)\n",
    "\n",
    "            write_file_train_word = open(write_path_train_word, \"wb\")\n",
    "            pickle.dump(temp_train_word, write_file_train_word)\n",
    "\n",
    "            write_file_train_char = open(write_path_train_char, \"wb\")\n",
    "            pickle.dump(temp_train_char, write_file_train_char)\n",
    "\n",
    "            write_file_valid_word = open(write_path_valid_word, \"wb\")\n",
    "            pickle.dump(temp_valid_word, write_file_valid_word)\n",
    "\n",
    "            write_file_valid_char = open(write_path_valid_char, \"wb\")\n",
    "            pickle.dump(temp_valid_char, write_file_valid_char)\n",
    "\n",
    "    def get_glove_embeddings(self, word_embedding_size = 100, char_embedding_size = 20 , embedding_dir = \"E:\\\\Internships_19\\\\Internship(Summer_19)\\\\Q&A_Toolkit\\\\Dataset_analysis\\\\SQuAD\" ):\n",
    "\n",
    "\n",
    "\n",
    "        glove_embeddings = os.path.join(embedding_dir, \"glove_embeddings100.txt\")\n",
    "\n",
    "        glove_embeddings = open(glove_embeddings,'r', encoding = 'utf-8')\n",
    "\n",
    "\n",
    "\n",
    "        #     glove_embeddings = pickle.load(open(glove_embeddings))\n",
    "\n",
    "        #####################  CHECK HOW GLOVE EMBEDDINGS WORK ##############\n",
    "        temp_embeddings = []\n",
    "\n",
    "        for word in self.word_to_index:\n",
    "\n",
    "                if word in ['<pad>', '<s>', '<eos>']:\n",
    "                    temp_vector = np.zeros((word_embedding_size))\n",
    "                elif word not in glove_embeddings:\n",
    "                    temp_vector = np.random.uniform(-np.sqrt(3)/np.sqrt(word_embedding_size), np.sqrt(3)/np.sqrt(word_embedding_size), word_embedding_size)\n",
    "                else:\n",
    "                    temp_vector = glove_embeddings[word]\n",
    "\n",
    "                temp_embeddings.append(temp_vector)\n",
    "\n",
    "        temp_embeddings = np.asarray(temp_embeddings)\n",
    "        temp_embeddings = temp_embeddings.astype(np.float32)\n",
    "        self.word_embeddings = temp_embeddings\n",
    "\n",
    "\n",
    "        char_embeddings = []\n",
    "        print (char_embedding_size)\n",
    "        char_embeddings.append(np.zeros((char_embedding_size)))\n",
    "\n",
    "        for i in range(len(self.char_to_index)):\n",
    "            temp_vector = np.random.uniform(-np.sqrt(3)/np.sqrt(char_embedding_size), np.sqrt(3)/np.sqrt(char_embedding_size), char_embedding_size)\n",
    "            char_embeddings.append(temp_vector)\n",
    "\n",
    "        char_embeddings = np.asarray(char_embeddings)\n",
    "        char_embeddings = char_embeddings.astype(np.float32)\n",
    "\n",
    "        self.char_embeddings = char_embeddings\n",
    "\n",
    "        pickle.dump(char_embeddings, open(os.path.join(embedding_dir, \"char_embeddings\" + \".pkl\"), \"wb\")) \n",
    "        pickle.dump(temp_embeddings, open(os.path.join(embedding_dir, \"glove_word_embeddings\" + \".pkl\"), \"wb\"))\n",
    "\n",
    "\n",
    "#         return self.word_embeddings, self.char_embeddings\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "embedding = Embedding_Matrix(embedding_dir = \"E:\\\\Internships_19\\\\Internship(Summer_19)\\\\Q&A_Toolkit\\\\Dataset_analysis\\\\SQuAD\")\n",
    "embedding.get_glove_embeddings(word_embedding_size = 100, char_embedding_size = 20 , embedding_dir = \"E:\\\\Internships_19\\\\Internship(Summer_19)\\\\Q&A_Toolkit\\\\Dataset_analysis\\\\SQuAD\" )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding.index_files_to_char_level_and_word_level(datapath = \"E:\\\\Internships_19\\\\Internship(Summer_19)\\\\Q&A_Toolkit\\\\Dataset_analysis\\\\SQuAD\", max_words=400, max_chars=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
