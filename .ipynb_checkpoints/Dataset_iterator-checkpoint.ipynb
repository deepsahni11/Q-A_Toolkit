{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset iterator file creates batches of training data and yields a batch: (\"yields\" returns a generator element where \"generators\" are iterables which can loop over its elements only once and then they are destroyed from memory.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "import os\n",
    "import pickle\n",
    "import tqdm as tqdm\n",
    "\n",
    "import numpy as np\n",
    "from six.moves import xrange\n",
    "# from data_util.vocab import PAD_ID, UNK_ID\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This file contains code to read tokenized data from file,\n",
    "truncate, pad and process it into batches ready for training\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Batch(object):\n",
    "    \"\"\"A class to hold the information needed for a training batch\"\"\"\n",
    "\n",
    "    def __init__(self,names, context_word_index_padded, context_word_mask, question_word_index_padded, question_word_mask, answer_start, answer_end):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "          {context/qn}_ids: Numpy arrays.\n",
    "            Shape (batch_size, {context_len/question_len}). Contains padding.\n",
    "          {context/qn}_mask: Numpy arrays, same shape as _ids.\n",
    "            Contains 1s where there is real data, 0s where there is padding.\n",
    "          {context/qn/ans}_tokens: Lists length batch_size, containing lists (unpadded) of tokens (strings)\n",
    "          ans_span: numpy array, shape (batch_size, 2)\n",
    "          uuid: a list (length batch_size) of strings.\n",
    "            Not needed for training. Used by official_eval mode.\n",
    "        \"\"\"\n",
    "        self.names = names\n",
    "        self.context_word_index_padded = context_word_index_padded\n",
    "        self.context_word_mask = context_word_mask\n",
    "#         self.context_tokens = context_tokens\n",
    "\n",
    "        self.question_word_index_padded = question_word_index_padded\n",
    "        self.question_word_mask = question_word_mask\n",
    "#         self.qn_tokens = qn_tokens\n",
    "\n",
    "        self.answer_start = answer_start\n",
    "        self.answer_end = answer_end\n",
    "\n",
    "#         self.uuids = uuids\n",
    "\n",
    "        self.batch_size = len(self.context_word_index_padded)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def refill_batches(batches,batch_size,names, max_context_length, max_question_length,answer_start, answer_end ):\n",
    "#     refill_batches(batches, word_to_index, context_file, qn_file, ans_file, batch_size, context_len, question_len, discard_long):\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    Adds more batches into the \"batches\" list.\n",
    "    Inputs:\n",
    "      batches: list to add batches to\n",
    "     \n",
    "      names: list containing strings of file names [\"train_context\",\"train_question\"] or [\"validation_context\",\"validation_question\"]\n",
    "      data_dir : paths to {train/dev}.{context/question/answer} data files\n",
    "      batch_size: integer ==> how big to make the batches\n",
    "      max_context_length, max_question_length: max length of context and question respectively\n",
    "      \n",
    "    \"\"\"\n",
    "    print (\"Refilling batches...\")\n",
    "    tic = time.time()\n",
    "    examples = [] # list of (qn_ids, context_ids, ans_span, ans_tokens) triples\n",
    "    \n",
    "    word_index_padded =[os.path.join(data_dir + name + \"_word_index_padded.pkl\")  for name in self.names ]\n",
    "    with open(word_index_padded[0], \"rb\") as input_file:\n",
    "        context_word_index_padded = pickle.load(input_file)\n",
    "    with open(word_index_padded[1], \"rb\") as input_file:\n",
    "        question_word_index_padded = pickle.load(input_file)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "#     context_line, qn_line, ans_line = context_file.readline(), qn_file.readline(), ans_file.readline() # read the next line from each\n",
    "\n",
    "    while True: # while you haven't reached the end\n",
    "        \n",
    "        # add to examples\n",
    "        examples.append((context_word_index_padded, question_word_index_padded, answer_start, answer_end))\n",
    "\n",
    "        # stop refilling if you have 1 batch : change it later \n",
    "        ################## add number of batches you need ###########################33\n",
    "        if len(examples) == batch_size * 1 :\n",
    "            break\n",
    "\n",
    "\n",
    "\n",
    "    # Make into batches and append to the list batches\n",
    "    for batch_start in xrange(0, len(examples), batch_size):\n",
    "\n",
    "        # Note: each of these is a list length batch_size of lists of ints (except on last iter when it might be less than batch_size)\n",
    "        context_word_index_padded, question_word_index_padded, answer_start, answer_end = zip(*examples[batch_start:batch_start+batch_size])\n",
    "\n",
    "        batches.append((context_word_index_padded, question_word_index_padded, answer_start, answer_end))\n",
    "\n",
    "    # shuffle the batches\n",
    "    random.shuffle(batches)\n",
    "\n",
    "    toc = time.time()\n",
    "    print (\"Refilling batches took %.2f seconds\" % (toc-tic))\n",
    "    return\n",
    "\n",
    "\n",
    "def get_batch_generator(data_dir, names, batch_size, max_context_length, max_question_length, answer_start, answer_end):\n",
    "    \"\"\"\n",
    "    This function returns a generator object that yields batches.\n",
    "    The last batch in the dataset will be a partial batch.\n",
    "    Read this to understand generators and the yield keyword in Python: https://stackoverflow.com/questions/231767/what-does-the-yield-keyword-do\n",
    "    Inputs:\n",
    "      names: list containing strings of file names = [\"train_context\",\"train_question\"] or [\"validation_context\",\"validation_question\"]\n",
    "      data_dir : paths to {train/dev}.{context/question/answer} data files\n",
    "      batch_size: integer ==> how big to make the batches\n",
    "      max_context_length, max_question_length: max length of context and question respectively\n",
    "     \n",
    "    \"\"\"\n",
    "    \n",
    "#     with open(r\"E:\\\\Internships_19\\\\Internship(Summer_19)\\\\Q&A_Toolkit\\\\Dataset_analysis\\\\SQuAD\\\\glove_word_embeddings.pkl\", \"rb\") as input_file:\n",
    "#         emb_matrix = pickle.load(input_file)\n",
    "    \n",
    "   \n",
    "    \n",
    "    batches = []\n",
    "\n",
    "    while True:\n",
    "        if len(batches) == 0: # add more batches\n",
    "            refill_batches(batches,batch_size,names, max_context_length, max_question_length,answer_start, answer_end)\n",
    "        if len(batches) == 0:\n",
    "            break\n",
    "\n",
    "        # Get next batch. These are all lists length batch_size\n",
    "        (context_word_index_padded, question_word_index_padded, answer_start, answer_end) = batches.pop(0)\n",
    "\n",
    "#         # Pad context_ids and qn_ids\n",
    "#         qn_ids = padded(qn_ids, question_len) # pad questions to length question_len\n",
    "#         context_ids = padded(context_ids, context_len) # pad contexts to length context_len\n",
    "\n",
    "        # Make qn_ids into a np array and create qn_mask\n",
    "#         qn_ids = np.array(qn_ids) # shape (batch_size, question_len)\n",
    "#         qn_mask = (qn_ids != PAD_ID).astype(np.int32) # shape (batch_size, question_len)\n",
    "\n",
    "#         # Make context_ids into a np array and create context_mask\n",
    "#         context_ids = np.array(context_ids) # shape (batch_size, context_len)\n",
    "#         context_mask = (context_ids != PAD_ID).astype(np.int32) # shape (batch_size, context_len)\n",
    "\n",
    "\n",
    "        context_word_mask = (context_word_index_padded != 0).type(torch.int32) \n",
    "        question_word_mask = (question_word_index_padded != 0).type(torch.int32)\n",
    "        # Make ans_span into a np array\n",
    "#         ans_span = np.array(ans_span) # shape (batch_size, 2)\n",
    "\n",
    "        # Make into a Batch object\n",
    "        batch = Batch(context_word_index_padded, context_word_mask, question_word_index_padded, question_word_mask, answer_start, answer_end)\n",
    "\n",
    "        yield batch\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\"train_context\",\"train_question\"]\n",
    "data_dir = \"E:\\\\Internships_19\\\\Internship(Summer_19)\\\\Q&A_Toolkit\\\\Dataset_analysis\\\\SQuAD\\\\\"\n",
    "max_context_length = 400\n",
    "max_question_length = 30\n",
    "answers = [\"train.answer_start\", \"train.answer_end\"]\n",
    "answers_indexes =[os.path.join(data_dir + answer )  for answer in answers ]\n",
    "with open(answers_indexes[0], \"r\" ,encoding=\"utf-8\") as input_file:\n",
    "    answer_start = input_file\n",
    "    lines  = []\n",
    "    for line in answer_start:\n",
    "        lines.append(line)\n",
    "    write_path_train_word = os.path.join(data_dir, \"answer_start\" + \"_pkl.pkl\")\n",
    "    write_file_train_word = open(write_path_train_word, \"wb\")\n",
    "    pickle.dump(lines, write_file_train_word)\n",
    "with open(answers_indexes[1], \"r\" ,encoding=\"utf-8\") as input_file:\n",
    "    answer_start = input_file\n",
    "    lines  = []\n",
    "    for line in answer_start:\n",
    "        lines.append(line)\n",
    "    write_path_train_word = os.path.join(data_dir, \"answer_end\" + \"_pkl.pkl\")\n",
    "    write_file_train_word = open(write_path_train_word, \"wb\")\n",
    "    pickle.dump(lines, write_file_train_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'_io.TextIOWrapper' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-34d00d3912ce>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswers_indexes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"r\"\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"utf-8\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0minput_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: '_io.TextIOWrapper' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "with open(data_dir + \"\\\\\" + prefix + \"_word_index.context_pkl.pkl\", \"rb\") as input_file:\n",
    "            context_word_index = pickle.load(input_file)\n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_batch_generator(data_dir, names, batch_size, max_context_length, max_question_length, answer_start, answer_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Batch(names, context_word_index_padded, context_word_mask, question_word_index_padded, question_word_mask, answer_start, answer_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
