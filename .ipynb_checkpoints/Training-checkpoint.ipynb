{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import logging\n",
    "import code\n",
    "import gc\n",
    "from torch.optim import Adam\n",
    "# from "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "There are two ways of letting the model know your intention i.e do you want to train the model or do you want to use the model to evaluate. In case of model.train() the model knows it has to learn the layers and when we use model.eval() it indicates the model that nothing new is to be learnt and the model is used for testing. model.eval() is also necessary because in pytorch if we are using batchnorm and during test if we want to just pass a single image, pytorch throws an error if model.eval() is not specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Train_Model:\n",
    "\n",
    "    def __init__(self, config, model):\n",
    "        self.config = config\n",
    "        self.model = model\n",
    "        self.loss_function = torch.nn.CrossEntropyLoss()\n",
    "        self.parameters_trainable = list(filter(lambda p: p.requires_grad, self.model.parameters()))\n",
    "        self.optimizer = optim.Adam(self.parameters_trainable, lr=self.config.lr)\n",
    "\n",
    "        self.glove_path = os.path.join(config.data_dir, \"glove_word_embeddings.pkl\")\n",
    "        self.num_epochs = config.num_epochs\n",
    "        self.data_dir = config.data_dir\n",
    "        self.names = config.names\n",
    "        self.batch_size = config.batch_size\n",
    "        self.print_every = config.print_every\n",
    "        self.max_context_length = config.max_context_length\n",
    "        self.max_question_length = config.max_question_length\n",
    "        self.model_dir = config.model_dir\n",
    "        self.early_stop = config.early_stop\n",
    "        self.print_and_validate_every = config.print_and_validate_every\n",
    "        \n",
    "    def save_model(self, model, optimizer, loss, global_step, epoch ,prefix):\n",
    "        # A state_dict is simply a Python dictionary object that maps each layer to its parameter tensor\n",
    "        \n",
    "        if(prefix == \"best_model\"):\n",
    "            model_state = model.state_dict()\n",
    "            model_state = {k: v for k, v in model_state.items() if 'embedding' not in k}\n",
    "\n",
    "            state = {\n",
    "                'global_step': global_step,\n",
    "                'epoch': epoch,\n",
    "                'model': model_state,\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'current_loss': loss\n",
    "            }\n",
    "            model_save_path = os.path.join(self.model_dir, 'best_model_%d_%d_%d' % (global_step, epoch, int(time.time())))\n",
    "            torch.save(state, model_save_path)\n",
    "            \n",
    "        elif(prefix == \"last_model\"):\n",
    "            model_state = model.state_dict()\n",
    "            model_state = {k: v for k, v in model_state.items() if 'embedding' not in k}\n",
    "\n",
    "            state = {\n",
    "                'global_step': global_step,\n",
    "                'epoch': epoch,\n",
    "                'model': model_state,\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'current_loss': loss\n",
    "            }\n",
    "            model_save_path = os.path.join(self.model_dir, 'last_model_%d_%d_%d' % (global_step, epoch, int(time.time())))\n",
    "            torch.save(state, model_save_path)\n",
    "            \n",
    "            \n",
    "    def get_f1_em_score(self, prefix, num_samples=100):\n",
    "\n",
    "\n",
    "        f1_total = 0.\n",
    "        em_total = 0.\n",
    "        example_num = 0\n",
    "\n",
    "#         tic = time.time()\n",
    "\n",
    "        for batch in get_batch_generator(self.data_dir, self.names, self.batch_size, self.max_context_length, self.max_question_length,prefix):\n",
    "\n",
    "            _,start_pos_prediction, end_pos_prediction = self.test_one_batch(batch)\n",
    "\n",
    "            start_pos_prediction = start_pos_prediction.tolist()\n",
    "            end_pos_prediction = end_pos_prediction.tolist()\n",
    "\n",
    "            for index, (pred_answer_start, pred_answer_end, true_answer_tokens) in enumerate(zip(start_pos_prediction, end_pos_prediction, batch.answer_tokens_batch)):\n",
    "                    \n",
    "                example_num += 1\n",
    "                pred_answer_tokens = batch.context_tokens_batch[index][pred_answer_start : pred_answer_end + 1]\n",
    "                pred_answer = \" \".join(pred_answer_tokens)\n",
    "\n",
    "                true_answer = \" \".join(true_answer_tokens)\n",
    "\n",
    "                f1 = f1_score(pred_answer, true_answer)\n",
    "                em = exact_match_score(pred_answer, true_answer)\n",
    "                f1_total += f1\n",
    "                em_total += em\n",
    "\n",
    "                \n",
    "\n",
    "                if num_samples != 0 and example_num >= num_samples:\n",
    "                    break\n",
    "\n",
    "            if num_samples != 0 and example_num >= num_samples:\n",
    "                break\n",
    "\n",
    "        f1_total /= example_num\n",
    "        em_total /= example_num\n",
    "\n",
    "#         toc = time.time()\n",
    "#         logging.info(\"Calculating F1/EM for %i examples in %s set took %.2f seconds\" % (example_num, dataset, toc-tic))\n",
    "\n",
    "        return f1_total, em_total\n",
    "    def get_validation_loss(self,prefix):\n",
    "#         logging.info(\"Calculating dev loss...\")\n",
    "#         tic = time.time()\n",
    "#         loss_per_batch, batch_lengths = [], []\n",
    "        total_validation_loss = 0.0\n",
    "        validation_set_size = 0\n",
    "        for batch in get_batch_generator(self.data_dir, self.names, self.batch_size, self.max_context_length, self.max_question_length,prefix):\n",
    "\n",
    "            validation_batch_loss, _, _ = self.test_one_batch(batch)\n",
    "            validation_set_size += batch.batch_size\n",
    "            total_validation_loss += validation_batch_loss\n",
    "#             batch_lengths.append(curr_batch_size)\n",
    "#             i += 1\n",
    "#             if i == 10:\n",
    "#                 break\n",
    "#         total_num_examples = sum(batch_lengths)\n",
    "#         toc = time.time()\n",
    "#         print(validation_set_size)\n",
    "\n",
    "        validation_loss = total_validation_loss / validation_set_size\n",
    "#         print \"Computed validation loss = %f \" % (validation_loss)\n",
    "\n",
    "        return validation_loss\n",
    "\n",
    "    def get_data(self, batch, is_train=True):\n",
    "        \n",
    "        question_word_index_batch = batch.question_word_index_batch\n",
    "\n",
    "        context_word_index_batch = batch.context_word_index_batch\n",
    "        \n",
    "        span_tensor_batch = batch.span_tensor_batch\n",
    "\n",
    "        if is_train:\n",
    "            return context_word_index_batch, question_word_index_batch,span_tensor_batch\n",
    "        else:\n",
    "            return context_word_index_batch, question_word_index_batch\n",
    "      \n",
    "    def get_grad_norm(self, parameters, norm_type=2):\n",
    "        parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
    "        total_norm = 0\n",
    "        for p in parameters:\n",
    "            param_norm = p.grad.data.norm(norm_type)\n",
    "            total_norm += param_norm ** norm_type\n",
    "        total_norm = total_norm ** (1. / norm_type)\n",
    "        return total_norm\n",
    "\n",
    "    def get_param_norm(self, parameters, norm_type=2):\n",
    "        total_norm = 0\n",
    "        for p in parameters:\n",
    "            param_norm = p.data.norm(norm_type)\n",
    "            total_norm += param_norm ** norm_type\n",
    "        total_norm = total_norm ** (1. / norm_type)\n",
    "        return total_norm\n",
    "    \n",
    "    def test_one_batch(self, batch):\n",
    "        \n",
    "        self.model.eval()\n",
    "\n",
    "        context_word_index_batch, question_word_index_batch,  span_tensor_batch = self.get_data(batch)\n",
    "\n",
    "#         print(context_word_index_batch)\n",
    "\n",
    "\n",
    "        context_word_index_padded_per_batch = Variable(pad_data(context_word_index_batch))\n",
    "#         print(context_word_index_padded_per_batch)\n",
    "        context_word_index_padded_per_batch.requires_grad = False\n",
    "        question_word_index_padded_per_batch = Variable(pad_data(question_word_index_batch))\n",
    "        question_word_index_padded_per_batch.requires_grad = False\n",
    "\n",
    "\n",
    "        context_ids = np.array(context_word_index_padded_per_batch) # shape (batch_size, context_len)\n",
    "        context_mask_per_batch = (context_ids != 0).astype(np.int32) # shape (batch_size, context_len)\n",
    "        context_word_mask_per_batch_new = Variable(torch.from_numpy(context_mask_per_batch))\n",
    "        context_word_mask_per_batch_new.requires_grad = False\n",
    "\n",
    "        question_ids = np.array(question_word_index_padded_per_batch) # shape (batch_size, context_len)\n",
    "        question_mask_per_batch = (question_ids != 0).astype(np.int32) # shape (batch_size, context_len)\n",
    "        question_word_mask_per_batch_new = Variable(torch.from_numpy(question_mask_per_batch))\n",
    "        question_word_mask_per_batch_new.requires_grad = False\n",
    "\n",
    "        span_tensor_batch = Variable(span_tensor_batch)\n",
    "        \n",
    "        span_tensor_batch.requires_grad = False\n",
    "\n",
    "        start_index_prediction, end_index_prediction, loss = self.model(context_word_index_padded_per_batch,context_word_mask_per_batch_new, question_word_index_padded_per_batch, question_word_mask_per_batch_new, span_tensor_batch)\n",
    "    \n",
    "        self.model.train()\n",
    "\n",
    "        return loss.item(),start_index_prediction, end_index_prediction\n",
    "        \n",
    "    def train_one_batch(self, batch):\n",
    "\n",
    "\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        context_word_index_batch, question_word_index_batch,  span_tensor_batch = self.get_data(batch)\n",
    "\n",
    "\n",
    "\n",
    "        context_word_index_padded_per_batch = Variable(pad_data(context_word_index_batch))\n",
    "        context_word_index_padded_per_batch.requires_grad = False\n",
    "        question_word_index_padded_per_batch = Variable(pad_data(question_word_index_batch))\n",
    "        question_word_index_padded_per_batch.requires_grad = False\n",
    "\n",
    "\n",
    "        context_ids = np.array(context_word_index_padded_per_batch) # shape (batch_size, context_len)\n",
    "        context_mask_per_batch = (context_ids != 0).astype(np.int32) # shape (batch_size, context_len)\n",
    "        context_word_mask_per_batch_new = Variable(torch.from_numpy(context_mask_per_batch))\n",
    "        context_word_mask_per_batch_new.requires_grad = False\n",
    "\n",
    "        question_ids = np.array(question_word_index_padded_per_batch) # shape (batch_size, context_len)\n",
    "        question_mask_per_batch = (question_ids != 0).astype(np.int32) # shape (batch_size, context_len)\n",
    "        question_word_mask_per_batch_new = Variable(torch.from_numpy(question_mask_per_batch))\n",
    "        question_word_mask_per_batch_new.requires_grad = False\n",
    "\n",
    "        span_tensor_batch = Variable(span_tensor_batch)\n",
    "        \n",
    "        span_tensor_batch.requires_grad = False\n",
    "\n",
    "        _, _, loss = self.model(context_word_index_padded_per_batch,context_word_mask_per_batch_new, question_word_index_padded_per_batch, question_word_mask_per_batch_new, span_tensor_batch)\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "#         l2_reg = None\n",
    "#         for W in self.parameters:\n",
    "            \n",
    "#             if l2_reg is None:\n",
    "#                 l2_reg = W.norm(2)\n",
    "#             else:\n",
    "#                 l2_reg = l2_reg + W.norm(2)\n",
    "#         loss = loss + config.reg_lambda * l2_reg\n",
    "        \n",
    "\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "\n",
    "        \n",
    "        param_norm = self.get_param_norm(self.parameters_trainable)\n",
    "        grad_norm = self.get_grad_norm(self.parameters_trainable)\n",
    "        \n",
    "        \n",
    "#         clip_grad_norm_(parameters, config.max_grad_norm)\n",
    "        self.optimizer.step()\n",
    "    \n",
    "\n",
    "        return loss.item(), param_norm, grad_norm\n",
    "    \n",
    "    \n",
    "    def train(self):\n",
    "\n",
    "\n",
    "        num_parameters = sum(p.numel() for p in self.parameters_trainable)\n",
    "\n",
    "        best_validation_f1, best_validation_em = None, None\n",
    "        best_validation_epoch = 0\n",
    "        epoch = 0\n",
    "        global_step = 0\n",
    "\n",
    "        loss_array = []\n",
    "        logging.info(\"Beginning training loop...\")\n",
    "        for epoch in range(200):\n",
    "            total_loss = 0.0\n",
    "            epoch_tic = time.time()\n",
    "            for batch in get_batch_generator(self.data_dir, self.names, self.batch_size, self.max_context_length, self.max_question_length,\"train\"):\n",
    "                \n",
    "                global_step += 1\n",
    "                iter_tic = time.time()\n",
    "                \n",
    "                train_batch_loss, param_norm, grad_norm = self.train_one_batch(batch)\n",
    "\n",
    "#                 total_loss = total_loss + loss\n",
    "#                 loss_array.append(total_loss)\n",
    "    \n",
    "                iter_toc = time.time()\n",
    "                iter_time = iter_toc - iter_tic\n",
    "            \n",
    "                if global_step % self.print_and_validate_every == 0:\n",
    "                    \n",
    "#                     print(self.get_validation_loss(\"validation\"))\n",
    "                    validation_batch_loss = self.get_validation_loss(\"validation\")\n",
    "                    \n",
    "#                     print()\n",
    "\n",
    "\n",
    "#                 if global_step % config.save_every == 0:\n",
    "#                     logging.info(\"Saving to %s...\" % model_dir)\n",
    "#                     self.save_model(model, optimizer, loss, global_step, epoch, model_dir)\n",
    "\n",
    "#                 if global_step % config.validate_every == 0:\n",
    "                    \n",
    "#                     logging.info(\"Epoch %d, Iter %d, dev loss: %f\" % (epoch, global_step, dev_loss))\n",
    "#                     write_summary(dev_loss, \"dev/loss\", summary_writer, global_step)\n",
    "\n",
    "                    train_batch_f1, train_batch_em = self.get_f1_em_score(\"train\", num_samples=100)\n",
    "#                     logging.info(\"Epoch %d, Iter %d, Train F1 score: %f, Train EM score: %f\" % (\n",
    "#                         epoch, global_step, train_f1, train_em))\n",
    "#                     write_summary(train_f1, \"train/F1\", summary_writer, global_step)\n",
    "#                     write_summary(train_em, \"train/EM\", summary_writer, global_step)\n",
    "\n",
    "                    validation_batch_f1, validation_batch_em = self.get_f1_em_score(\"validation\", num_samples=100)\n",
    "#                     logging.info(\n",
    "#                         \"Epoch %d, Iter %d, Dev F1 score: %f, Dev EM score: %f\" % (epoch, global_step, dev_f1, dev_em))\n",
    "#                     write_summary(dev_f1, \"dev/F1\", summary_writer, global_step)\n",
    "#                     write_summary(dev_em, \"dev/EM\", summary_writer, global_step)\n",
    "\n",
    "\n",
    "\n",
    "                    if best_validation_f1 is None or validation_batch_f1 > best_validation_f1:\n",
    "                        best_validation_f1 = validation_batch_f1\n",
    "\n",
    "                    if best_validation_em is None or validation_batch_em > best_validation_em:\n",
    "                        best_validation_em = validation_batch_em\n",
    "                        best_validation_epoch = epoch+1\n",
    "#                         logging.info(\"Saving to %s...\" % bestmodel_dir)\n",
    "                        self.save_model(self.model, self.optimizer, validation_batch_loss, global_step, epoch, \"best_model\")\n",
    "                        \n",
    "                    print (\"Epoch : {} Step : {} Train_batch Loss : {} Validation_batch Loss :{} \" .format(epoch+1, global_step, train_batch_loss, validation_batch_loss))\n",
    "            \n",
    "                    print(\"Train_batch F1:{} Train_batch EM:{} Validation_batch_F1: {} Best_validation_batch F1:{} Best_validation_batch EM :{} \".format(train_batch_f1,train_batch_em,validation_batch_f1,best_validation_f1,best_validation_em))\n",
    "\n",
    "                \n",
    "            if (epoch - best_validation_epoch > self.early_stop):\n",
    "                break\n",
    "            self.save_model( model, optimizer, loss, global_step, epoch+1 ,\"last_model\")\n",
    "#             self.save_model(model, optimizer, loss, global_step, epoch, bestmodel_dir)\n",
    "#             torch.save(self.model.state_dict(), open(os.path.join(outdir, \"last_model\"), \"wb\"))\n",
    "#             print(\"total loss for epoch number = \" + str(epoch+1) + \" = \" + str(total_loss))\n",
    "\n",
    "            epoch_toc = time.time()\n",
    "            print(\"End of epoch %i.\" % (epoch+1))\n",
    "\n",
    "        sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:54: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1 Step : 2 Train_batch Loss : 10.032332420349121 Validation_batch Loss :0.4691783428192139 \n",
      "Train_batch F1:0.0 Train_batch EM:0.0 Validation_batch_F1: 0.20534146637261289 Best_validation_batch F1:0.20534146637261289 Best_validation_batch EM :0.0 \n",
      "End of epoch 1.\n",
      "Epoch : 2 Step : 4 Train_batch Loss : 10.0269775390625 Validation_batch Loss :0.4699121952056885 \n",
      "Train_batch F1:0.0 Train_batch EM:0.0 Validation_batch_F1: 0.2143947001799035 Best_validation_batch F1:0.2143947001799035 Best_validation_batch EM :0.0 \n",
      "End of epoch 2.\n",
      "Epoch : 3 Step : 6 Train_batch Loss : 10.01665210723877 Validation_batch Loss :0.4710701465606689 \n",
      "Train_batch F1:0.0 Train_batch EM:0.0 Validation_batch_F1: 0.09506970972488216 Best_validation_batch F1:0.2143947001799035 Best_validation_batch EM :0.0 \n",
      "End of epoch 3.\n",
      "Epoch : 4 Step : 8 Train_batch Loss : 9.995808601379395 Validation_batch Loss :0.47323384284973147 \n",
      "Train_batch F1:0.05497076023391813 Train_batch EM:0.0 Validation_batch_F1: 0.07657346912337841 Best_validation_batch F1:0.2143947001799035 Best_validation_batch EM :0.0 \n",
      "End of epoch 4.\n",
      "Epoch : 5 Step : 10 Train_batch Loss : 9.952901840209961 Validation_batch Loss :0.4761983394622803 \n",
      "Train_batch F1:0.0 Train_batch EM:0.0 Validation_batch_F1: 0.05592898933416175 Best_validation_batch F1:0.2143947001799035 Best_validation_batch EM :0.0 \n",
      "End of epoch 5.\n",
      "Epoch : 6 Step : 12 Train_batch Loss : 9.866878509521484 Validation_batch Loss :0.48173675537109373 \n",
      "Train_batch F1:0.09444444444444447 Train_batch EM:0.0 Validation_batch_F1: 0.07592898933416177 Best_validation_batch F1:0.2143947001799035 Best_validation_batch EM :0.0 \n",
      "End of epoch 6.\n",
      "Epoch : 7 Step : 14 Train_batch Loss : 9.699949264526367 Validation_batch Loss :0.4896361827850342 \n",
      "Train_batch F1:0.09444444444444447 Train_batch EM:0.0 Validation_batch_F1: 0.054027777777777786 Best_validation_batch F1:0.2143947001799035 Best_validation_batch EM :0.0 \n",
      "End of epoch 7.\n",
      "Epoch : 8 Step : 16 Train_batch Loss : 9.3748779296875 Validation_batch Loss :0.5077174663543701 \n",
      "Train_batch F1:0.15198512585812357 Train_batch EM:0.0 Validation_batch_F1: 0.09314413484692123 Best_validation_batch F1:0.2143947001799035 Best_validation_batch EM :0.0 \n",
      "End of epoch 8.\n",
      "Epoch : 9 Step : 18 Train_batch Loss : 8.86159896850586 Validation_batch Loss :0.5210849285125733 \n",
      "Train_batch F1:0.15198512585812357 Train_batch EM:0.0 Validation_batch_F1: 0.08124871001031993 Best_validation_batch F1:0.2143947001799035 Best_validation_batch EM :0.0 \n",
      "End of epoch 9.\n",
      "Epoch : 10 Step : 20 Train_batch Loss : 8.458535194396973 Validation_batch Loss :0.607372522354126 \n",
      "Train_batch F1:0.15928228776054862 Train_batch EM:0.0 Validation_batch_F1: 0.13799070352778794 Best_validation_batch F1:0.2143947001799035 Best_validation_batch EM :0.0 \n",
      "End of epoch 10.\n",
      "Epoch : 11 Step : 22 Train_batch Loss : 8.167372703552246 Validation_batch Loss :0.5232369899749756 \n",
      "Train_batch F1:0.22906615451971493 Train_batch EM:0.0 Validation_batch_F1: 0.01213235294117647 Best_validation_batch F1:0.2143947001799035 Best_validation_batch EM :0.0 \n",
      "End of epoch 11.\n",
      "Epoch : 12 Step : 24 Train_batch Loss : 7.940975666046143 Validation_batch Loss :0.5038631916046142 \n",
      "Train_batch F1:0.09444444444444447 Train_batch EM:0.0 Validation_batch_F1: 0.039980158730158734 Best_validation_batch F1:0.2143947001799035 Best_validation_batch EM :0.0 \n",
      "End of epoch 12.\n",
      "Epoch : 13 Step : 26 Train_batch Loss : 7.747241973876953 Validation_batch Loss :0.5410366058349609 \n",
      "Train_batch F1:0.09444444444444447 Train_batch EM:0.0 Validation_batch_F1: 0.03516806722689076 Best_validation_batch F1:0.2143947001799035 Best_validation_batch EM :0.0 \n"
     ]
    }
   ],
   "source": [
    "from torchviz import make_dot\n",
    "hidden_dim = 100\n",
    "maxout_pool_size=16\n",
    "max_number_of_iterations = 3\n",
    "with open(r\"E:\\\\Internships_19\\\\Internship(Summer_19)\\\\Q&A_Toolkit\\\\Dataset_analysis\\\\SQuAD\\\\glove_word_embeddings.pkl\", \"rb\") as input_file:\n",
    "    embedding_matrix = pickle.load(input_file)\n",
    "    \n",
    "with autograd.set_detect_anomaly(True):\n",
    "    model = DCN_Model(hidden_dim, embedding_matrix, dropout_ratio, maxout_pool_size, max_number_of_iterations)\n",
    "\n",
    "    \n",
    "    # model = model.cpu()\n",
    "    train_model = Train_Model(config, model)\n",
    "\n",
    "    train_model.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm as tqdm\n",
    "import torch\n",
    "import random\n",
    "# import nltk\n",
    "import numpy as np\n",
    "import pickle\n",
    "import sys\n",
    "import copy\n",
    "import os.path\n",
    "# import tqdm as tqdm\n",
    "\n",
    "datapath = \"E:\\\\Internships_19\\\\Internship(Summer_19)\\\\Q&A_Toolkit\\\\Dataset_analysis\\\\SQuAD\\\\\"\n",
    "\n",
    "def find_max_length(data):\n",
    "\n",
    "    \"\"\" Finds the maximum sequence length for data \n",
    "        Args:\n",
    "            data: The data from which sequences will be chosen\n",
    "    \"\"\"\n",
    "    temp = 0\n",
    "    index = 0\n",
    "    for i, _ in enumerate(data):\n",
    "\n",
    "        if (len(data[i]) > temp):\n",
    "            temp = len(data[i])\n",
    "            index = i\n",
    "    return temp,index\n",
    "\n",
    "\n",
    "def pad_data(data):\n",
    "\n",
    "    \"\"\" Pad the data to max_length given\n",
    "        Args: \n",
    "            data: Data that needs to be padded\n",
    "            max_length : The length to be achieved with padding\n",
    "        Returns:\n",
    "            padded_data : Each sequence is padded to make it of length\n",
    "                          max_length.\n",
    "    \"\"\"\n",
    "    padded_data = []\n",
    "    max_length,index =  find_max_length(data)\n",
    "\n",
    "    for lines in data:\n",
    "        if (len(lines) < max_length):\n",
    "            temp = np.lib.pad(lines, (0,max_length - len(lines)),\n",
    "                'constant', constant_values=0)\n",
    "        else:\n",
    "            temp = lines[:max_length]\n",
    "        padded_data.append(temp)\n",
    "\n",
    "    padded_data = torch.from_numpy(np.array(padded_data)).type(torch.int64)\n",
    "\n",
    "    return padded_data\n",
    "\n",
    "\n",
    "def index_files_using_word_to_index(filename, _dict, max_words):\n",
    "    \n",
    "    f = open(filename, \"r\", encoding=\"utf-8\")\n",
    "\n",
    "    lines = f.readlines()\n",
    "    lines  = [l.lower() for l in lines]\n",
    "    encoded_lines = []\n",
    "    for l in lines:\n",
    "        tokens = l.split()\n",
    "        tokens = tokens[:max_words]\n",
    "        temp = []\n",
    "        for t in tokens:\n",
    "            if t in _dict:\n",
    "                temp.append(_dict[t])\n",
    "            else:\n",
    "                temp.append(1)\n",
    "\n",
    "        encoded_lines.append(temp[:])\n",
    "\n",
    "    return encoded_lines\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "import sys\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def normalize_answer(s):\n",
    "    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
    "\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "\n",
    "def f1_score(prediction, ground_truth):\n",
    "    prediction_tokens = normalize_answer(prediction).split()\n",
    "    ground_truth_tokens = normalize_answer(ground_truth).split()\n",
    "    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
    "    num_same = sum(common.values())\n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "    precision = 1.0 * num_same / len(prediction_tokens)\n",
    "    recall = 1.0 * num_same / len(ground_truth_tokens)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "\n",
    "def exact_match_score(prediction, ground_truth):\n",
    "    return (normalize_answer(prediction) == normalize_answer(ground_truth))\n",
    "    \n",
    "with open(r\"E:\\\\Internships_19\\\\Internship(Summer_19)\\\\Q&A_Toolkit\\\\Dataset_analysis\\\\SQuAD\\\\dictionaries.pkl\", \"rb\") as input_file:\n",
    "    dictionaries = pickle.load(input_file)\n",
    "word_to_index = dictionaries[\"word_to_index\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "\n",
    "import torch\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "import os\n",
    "import pickle\n",
    "import tqdm as tqdm\n",
    "\n",
    "import numpy as np\n",
    "from six.moves import xrange\n",
    "\n",
    "class Batch():\n",
    "    \"\"\"A class to hold the information needed for a training batch\"\"\"\n",
    "    def __init__(self,names,context_word_index_batch,question_word_index_batch, span_tensor_batch,context_tokens_batch,questions_tokens_batch,answer_tokens_batch):\n",
    "        \n",
    "        self.names = names\n",
    "        self.context_word_index_batch = context_word_index_batch\n",
    "\n",
    "        self.question_word_index_batch = question_word_index_batch\n",
    "        self.span_tensor_batch = span_tensor_batch\n",
    "        self.context_tokens_batch = context_tokens_batch\n",
    "        self.questions_tokens_batch = questions_tokens_batch\n",
    "        self.answer_tokens_batch = answer_tokens_batch\n",
    "        self.batch_size = len(self.context_word_index_batch)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def refill_batches(batches,batch_size,names, max_context_length, max_question_length,context_word_index,question_word_index,span_tensor,context_tokens,question_tokens,answer_tokens):\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    Adds more batches into the \"batches\" list.\n",
    "    Inputs:\n",
    "      batches: list to add batches to\n",
    "\n",
    "      names: list containing strings of file names [\"train_context\",\"train_question\"] or [\"validation_context\",\"validation_question\"]\n",
    "      data_dir : paths to {train/dev}.{context/question/answer} data files\n",
    "      batch_size: integer ==> how big to make the batches\n",
    "      max_context_length, max_question_length: max length of context and question respectively\n",
    "\n",
    "    \"\"\"\n",
    "#     print (\"Refilling batches...\")\n",
    "    tic = time.time()\n",
    "    examples = [] \n",
    "\n",
    "\n",
    "\n",
    "        # add to examples\n",
    "    examples.append((context_word_index, question_word_index, span_tensor,context_tokens,question_tokens,answer_tokens))\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "    # Make into batches and append to the list batches\n",
    "    for batch_start in xrange(0, len(examples[0][0]), batch_size):\n",
    "\n",
    "        # Note: each of these is a list length batch_size of lists of ints (except on last iter when it might be less than batch_size)\n",
    "        context_word_index_batch = examples[0][0][batch_start:batch_start+batch_size]\n",
    "        question_word_index_batch = examples[0][1][batch_start:batch_start+batch_size]\n",
    "        span_tensor_batch = examples[0][2][batch_start:batch_start+batch_size]\n",
    "        context_tokens_batch = examples[0][3][batch_start:batch_start+batch_size]\n",
    "        questions_tokens_batch = examples[0][4][batch_start:batch_start+batch_size]\n",
    "        answer_tokens_batch = examples[0][5][batch_start:batch_start+batch_size]\n",
    "\n",
    "#         print(\"Batch \" + str(batch_start + 1) + \" loaded\")\n",
    "        \n",
    "        batches.append((context_word_index_batch, question_word_index_batch,span_tensor_batch,context_tokens_batch,questions_tokens_batch,answer_tokens_batch))\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    # shuffle the batches\n",
    "    random.shuffle(batches)\n",
    "\n",
    "    toc = time.time()\n",
    "#     print (\"Refilling batches took %.2f seconds\" % (toc-tic))\n",
    "    return batches\n",
    "\n",
    "\n",
    "def get_batch_generator(data_dir,names, batch_size, max_context_length, max_question_length,prefix):\n",
    "    \"\"\"\n",
    "    This function returns a generator object that yields batches.\n",
    "    The last batch in the dataset will be a partial batch.\n",
    "    Read this to understand generators and the yield keyword in Python: https://stackoverflow.com/questions/231767/what-does-the-yield-keyword-do\n",
    "    Inputs:\n",
    "      names: list containing strings of file names = [\"train_context\",\"train_question\"] or [\"validation_context\",\"validation_question\"]\n",
    "      data_dir : paths to {train/dev}.{context/question/answer} data files\n",
    "      batch_size: integer ==> how big to make the batches\n",
    "      max_context_length, max_question_length: max length of context and question respectively\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "#     if(prefix == \"train\"):\n",
    "\n",
    "    context_path_train = os.path.join(data_dir, prefix + \".context\")\n",
    "    question_path_train = os.path.join(data_dir, prefix +  \".question\")\n",
    "    answer_path_train = os.path.join(data_dir, prefix +  \".answer_text\")\n",
    "#     print(os.path.join(data_dir, prefix + \".context\"))\n",
    "\n",
    "    context_tokens = open(context_path_train, \"r\", encoding=\"utf-8\").readlines()\n",
    "    question_tokens =  open(question_path_train, \"r\", encoding=\"utf-8\").readlines()\n",
    "    answer_tokens = open(answer_path_train, \"r\", encoding=\"utf-8\").readlines()\n",
    "#         print(question_tokens)\n",
    "\n",
    "#     lines = f.readlines()\n",
    "#     lines  = [l.lower() for l in lines]\n",
    "\n",
    "    context_word_index_old = index_files_using_word_to_index(context_path_train, word_to_index, max_context_length)\n",
    "    question_word_index_old = index_files_using_word_to_index(question_path_train, word_to_index, max_question_length)\n",
    "    \n",
    "    answer_start_path = os.path.join(data_dir  + prefix +  \".answer_start\")\n",
    "    answer_start_list = open(answer_start_path, \"r\", encoding=\"utf-8\").readlines()\n",
    "    \n",
    "    answer_end_path = os.path.join(data_dir + \"//\" + prefix +  \".answer_end\")\n",
    "    answer_end_list = open(answer_end_path, \"r\", encoding=\"utf-8\").readlines()\n",
    "\n",
    "#     with open(data_dir + \"//\" + prefix +  \"answer_end\", \"r\") as input_file:\n",
    "#         answer_end_pkl = pickle.load(input_file)\n",
    "#         print(\"answer_end_pkl\")\n",
    "#         print(answer_end_pkl)\n",
    "#     with open(data_dir + \"//\" + prefix +  \"answer_start\", \"r\") as input_file:\n",
    "#         answer_start_pkl = pickle.load(input_file)\n",
    "            \n",
    "#     elif(prefix == \"validation\"):\n",
    "#         context_path_train = os.path.join(datapath, \"validation.context\")\n",
    "#         question_path_train = os.path.join(datapath, \"validation.question\")\n",
    "#         answer_path_train = os.path.join(datapath, \"validation.answer_text\")\n",
    "\n",
    "#         context_tokens = open(context_path_train, \"r\", encoding=\"utf-8\").readlines()\n",
    "#         question_tokens =  open(question_path_train, \"r\", encoding=\"utf-8\").readlines()\n",
    "#         answer_tokens = open(answer_path_train, \"r\", encoding=\"utf-8\").readlines()\n",
    "\n",
    "\n",
    "#         context_word_index_old = index_files_using_word_to_index(context_path_train, word_to_index, max_context_length)\n",
    "#         question_word_index_old = index_files_using_word_to_index(question_path_train, word_to_index, max_question_length)\n",
    "\n",
    "\n",
    "#         ### check for answers if they are validation or train\n",
    "#         with open(data_dir + \"//\" + \"answer_end_pkl.pkl\", \"rb\") as input_file:\n",
    "#             answer_end_pkl = pickle.load(input_file)\n",
    "#         with open(data_dir + \"//\" + \"answer_start_pkl.pkl\", \"rb\") as input_file:\n",
    "#             answer_start_pkl = pickle.load(input_file)\n",
    "\n",
    "\n",
    "            \n",
    "    context_tokens = context_tokens[30:50]\n",
    "    question_tokens = question_tokens[30:50]\n",
    "    answer_tokens = answer_tokens[30:50]\n",
    "\n",
    "    answer_end = torch.from_numpy(np.array([int(i) for i in answer_end_list])).long()\n",
    "    answer_start = torch.from_numpy(np.array([int(i) for i in answer_start_list])).long()              \n",
    "    answer_start = torch.unsqueeze(answer_start, 1)\n",
    "    answer_end = torch.unsqueeze(answer_end, 1)\n",
    "\n",
    "    span_tensor_old = torch.cat((answer_start, answer_end), 1)\n",
    "    span_tensor = span_tensor_old[30:50]\n",
    "    context_word_index = context_word_index_old[30:50]\n",
    "    question_word_index = question_word_index_old[30:50]\n",
    "\n",
    "\n",
    "\n",
    "    batches = []\n",
    "    count = 0\n",
    "\n",
    "    while (True):\n",
    "        count = count + 1\n",
    "        if len(batches) == 0: # add more batches\n",
    "            if(count > 2):\n",
    "                break\n",
    "            batches = refill_batches(batches,batch_size,names, max_context_length, max_question_length,context_word_index,question_word_index,span_tensor,context_tokens,question_tokens,answer_tokens)\n",
    "        if len(batches) == 0:\n",
    "            break\n",
    "\n",
    "        # Get next batch. These are all lists length batch_size\n",
    "        (context_word_index_batch, question_word_index_batch,span_tensor_batch,context_tokens,question_tokens,answer_tokens) = batches.pop(0)\n",
    "        \n",
    "\n",
    "        if(len(context_word_index_batch) == 0):\n",
    "            break\n",
    "            \n",
    "\n",
    "\n",
    "        # Make into a Batch object\n",
    "        batch = Batch(names,context_word_index_batch, question_word_index_batch, span_tensor_batch,context_tokens,question_tokens,answer_tokens)\n",
    "\n",
    "        yield batch\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import logging\n",
    "import code\n",
    "import pickle\n",
    "import os\n",
    "from torch import autograd\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import Embedding\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "\n",
    "class DCN_Model(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_dim, embedding_matrix, dropout_ratio, maxout_pool_size, max_number_of_iterations):\n",
    "        super(DCN_Model, self).__init__()\n",
    "\n",
    "        self.encoder = Word_Level_Encoder(hidden_dim, emb_matrix, dropout_ratio)\n",
    "        self.coattention_encoder = Coattention_Encoder(hidden_dim, maxout_pool_size, embedding_matrix, max_number_of_iterations, dropout_ratio)\n",
    "        self.decoder = Dynamic_Decoder(hidden_dim, maxout_pool_size, max_number_of_iterations, dropout_ratio)\n",
    "\n",
    "    def forward(self, context_word_indexes, context_word_mask, question_word_indexes, question_word_mask,span_tensor):\n",
    "        passage_representation = self.encoder.forward(context_word_indexes, context_word_mask)\n",
    "\n",
    "        question_representation = self.encoder.forward(question_word_indexes, question_word_mask)\n",
    "       \n",
    "\n",
    "        U_matrix = self.coattention_encoder.forward(question_representation, passage_representation,context_word_mask)\n",
    "\n",
    "        loss, index_start, index_end = self.decoder.forward(U_matrix, context_word_mask, span_tensor)\n",
    "\n",
    "        return index_start, index_end, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "\n",
    "with open(r\"E:\\\\Internships_19\\\\Internship(Summer_19)\\\\Q&A_Toolkit\\\\Dataset_analysis\\\\SQuAD\\\\glove_word_embeddings.pkl\", \"rb\") as input_file:\n",
    "    emb_matrix = pickle.load(input_file)\n",
    "    \n",
    "names = [\"validation_context\",\"train_context\",\"validation_question\",\"train_question\"]\n",
    "data_dir = \"E:\\\\Internships_19\\\\Internship(Summer_19)\\\\Q&A_Toolkit\\\\Dataset_analysis\\\\SQuAD\\\\\"\n",
    "\n",
    "\n",
    "\n",
    "def get_pretrained_embedding(embedding_matrix):\n",
    "    embedding = nn.Embedding(*embedding_matrix.shape)\n",
    "    embedding.weight = nn.Parameter(torch.from_numpy(embedding_matrix).float())\n",
    "    embedding.weight.requires_grad = False\n",
    "    return embedding\n",
    "\n",
    "\n",
    "class Word_Level_Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_dim, embedding_matrix, dropout_ratio):\n",
    "        super(Word_Level_Encoder, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.embedding = get_pretrained_embedding(embedding_matrix)\n",
    "        self.embedding_dim = self.embedding.embedding_dim\n",
    "\n",
    "        # batch_first = True\n",
    "        # Input: has a dimension of B x m x embedding_dim\n",
    "        # Function parameters: input_size, hidden_size, num_layers_of_LSTM = 1(here)\n",
    "        self.encoder = nn.LSTM(self.embedding_dim, self.hidden_dim, 1, batch_first=True,\n",
    "                              bidirectional=False, dropout=dropout_ratio) \n",
    "                                     \n",
    "#         self.dropout_emb = nn.Dropout(p=dropout_ratio)\n",
    "        \n",
    "        # creates a random vector with size= hidden_dim\n",
    "        self.sentinel = nn.Parameter(torch.rand(hidden_dim,))\n",
    "        \n",
    "    def initHidden(self,batch_size):\n",
    "        h0 = Variable(torch.zeros(1, batch_size, self.hidden_dim), requires_grad = False) # Initial hidden state\n",
    "        c0 = Variable(torch.zeros(1, batch_size, self.hidden_dim), requires_grad = False) # Initial cell state\n",
    "        return h0, c0\n",
    "\n",
    "\n",
    "    def forward(self, word_sequence_indexes, word_sequence_mask):\n",
    "        \n",
    "        # stores length of per instance for context/question\n",
    "        # tensor of size = B\n",
    "        length_per_instance = torch.sum(word_sequence_mask, 1)\n",
    "\n",
    "\n",
    "        initial_hidden_states = self.initHidden(len(length_per_instance))\n",
    "        # returns the word_sequences_embeddings with the embeddings for each token/word from word_sequence_indexes\n",
    "        # word_sequence_embeddings is a tensor of dimension of B x m x l\n",
    "        word_sequence_embeddings = self.embedding(word_sequence_indexes)\n",
    "        \n",
    "        # All RNN modules accept packed sequences as inputs.\n",
    "        # Input: word_sequence_embeddings has a dimension of B x m x l (l is the size of the glove_embedding/ pre-trained embedding/embedding_dim)\n",
    "        packed_word_sequence_embeddings = pack_padded_sequence(word_sequence_embeddings,length_per_instance,batch_first=True,enforce_sorted=False)\n",
    "\n",
    "        \n",
    "        \n",
    "        # nn.LSTM encoder gets an input of pack_padded_sequence of dimensions\n",
    "        # since the input was a packed sequence, the output will also be a packed sequence\n",
    "        output, _ = self.encoder(packed_word_sequence_embeddings,initial_hidden_states)\n",
    "       \n",
    "        \n",
    "        # Pads a packed batch of variable length sequences.\n",
    "        # It is an inverse operation to pack_padded_sequence().\n",
    "        # dimension:  B x m x l\n",
    "        output_to_LSTM_padded, _ = pad_packed_sequence(output, batch_first=True)\n",
    "        \n",
    "        \n",
    "\n",
    "        # list() creates a list of elements if an iterable is passed\n",
    "        # batch_size is a scalar which stores the value of batch size. (batch_size = B)\n",
    "        batch_size, _ = list(word_sequence_mask.size())\n",
    "        \n",
    "        \n",
    "        # dimension of sentinel matrix =  B x 1 x l (replicates or expands along given dimension)\n",
    "        length_per_instance_new_dim = length_per_instance.unsqueeze(1).expand(batch_size, self.hidden_dim).unsqueeze(1)\n",
    "        \n",
    "\n",
    "        # sentinel to be concatenated to the data\n",
    "        # dimension of sentinel_zero =  B x 1 x l\n",
    "        sentinel_zero = torch.zeros(batch_size, 1, self.hidden_dim)\n",
    "        \n",
    "        # copy sentinel vector at the end\n",
    "        # dimension of output_to_LSTM_padded_with_sentinel =  B x (m + 1) x l\n",
    "        output_to_LSTM_padded_with_sentinel = torch.cat([output_to_LSTM_padded, sentinel_zero], 1)  \n",
    "        \n",
    "        \n",
    "        \n",
    "        return output_to_LSTM_padded_with_sentinel\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "class Highway_Maxout_Network(nn.Module):\n",
    "    def __init__(self, hidden_dim, maxout_pool_size, dropout_ratio):\n",
    "        super(Highway_Maxout_Network, self).__init__()\n",
    "        self.hidden_dim = hidden_dim # l\n",
    "        self.maxout_pool_size = maxout_pool_size # p\n",
    "\n",
    "        # Affine mapping from 5l ==> l\n",
    "        self.r = nn.Linear(5 * hidden_dim, hidden_dim, bias=False) \n",
    "       \n",
    "\n",
    "        # Affine mapping from 3*l ==> l*p\n",
    "        self.max_out_layer1 = nn.Linear(3 * hidden_dim, hidden_dim*maxout_pool_size)\n",
    "        \n",
    "        # Affine mapping from l ==> l*p\n",
    "        self.max_out_layer2 = nn.Linear(hidden_dim, hidden_dim*maxout_pool_size)\n",
    "       \n",
    "        # Affine mapping from 2*l ==> p\n",
    "        self.max_out_layer3 = nn.Linear(2 * hidden_dim, maxout_pool_size)\n",
    "\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "        \n",
    "    \n",
    "    def forward(self, h_i, U, curr_mask_vector, index_i_minus_1, u_concatenated, mask_matrix, target=None):\n",
    "        batch_size, max_word_length , _ = list(U.size())\n",
    "\n",
    "        # concatenation of ( h_i of dimension = b x l ; u_concatenated of dimension = b x 4l ) along dimension 1 = gives b x 5l\n",
    "        # self.r(b x 5l) ====> b x l (change of vector space)\n",
    "        r = torch.tanh(self.r(torch.cat((h_i.view(-1, self.hidden_dim), u_concatenated), 1)))  # b x 5l => b x l\n",
    "       \n",
    "\n",
    "        # hidden_dim = l\n",
    "        r_expanded = r.unsqueeze(1).expand(batch_size, max_word_length, self.hidden_dim).contiguous()  # b x m x l\n",
    "\n",
    "        m_t1_input = torch.cat((U, r_expanded), 2).view(-1, 3*self.hidden_dim)  # b*m x 3l\n",
    "\n",
    "        m_t1_output = self.max_out_layer1(m_t1_input)  # b*m x p*l\n",
    "        \n",
    "        m_t1_output_resized, _ = m_t1_output.view(-1, self.hidden_dim, self.maxout_pool_size).max(2) # b*m x l\n",
    "\n",
    "        # m_t2_input =  m_t1_output_resized\n",
    "        m_t2_output = self.max_out_layer2(m_t1_output_resized)  # b*m x l*p\n",
    "        \n",
    "        m_t2_output_resized, _ = m_t2_output.view(-1, self.hidden_dim, self.maxout_pool_size).max(2)  # b*m x l\n",
    "\n",
    "        m_t3_input = torch.cat((m_t1_output_resized, m_t2_output_resized), 1)  # b*m x 2l\n",
    "        alpha1 = self.max_out_layer3(m_t3_input)  # b * m x p\n",
    "        alpha2, _ = alpha1.max(1)  # b*m\n",
    "        alpha3 = alpha2.view(-1, max_word_length) # b x m\n",
    "\n",
    "#         print(\"alpha \" + str(alpha3.size()) )\n",
    "#         print(\"mask matrix \" + str(mask_matrix.size()))\n",
    "        alpha3 = alpha3 + mask_matrix  # b x m\n",
    "#         print(\"alpha3\")\n",
    "#         print(alpha3)\n",
    "        \n",
    "        # alpha can be treated as probabilities that assign probability masses todifferent words in context. The word with\n",
    "        # maximum weight(probability) becomes the index(start/end)\n",
    "        alpha4 = F.softmax(alpha3, 1)  # b x m\n",
    "        _, index_i = torch.max(alpha4, dim=1) # b\n",
    "\n",
    "        if curr_mask_vector is None:\n",
    "            curr_mask_vector = (index_i == index_i) # b\n",
    "        else:\n",
    "            index_i = index_i*curr_mask_vector.long()  # b\n",
    "            index_i_minus_1 = index_i_minus_1*curr_mask_vector.long()  # b\n",
    "            curr_mask_vector = (index_i != index_i_minus_1) # b\n",
    "\n",
    "        step_loss = None\n",
    "        \n",
    "        \n",
    "\n",
    "#         target[target < 0] = 0\n",
    "        \n",
    "        \n",
    "        ## loss is only calculated only on that the predicted index at i_th time-step which varies \n",
    "        ## from the predicted index at time-step (i-1)_th time-step\n",
    "#         print(target)\n",
    "        if target is not None:\n",
    "            step_loss = self.loss(alpha3, target)  # b\n",
    "#             print(\"step_loss\")\n",
    "#             print(step_loss)\n",
    "# #             step_loss1 = step_loss * curr_mask_vector.float() # b\n",
    "#             print(\"step_loss1\")\n",
    "#             print(step_loss1)\n",
    "\n",
    "        return index_i, curr_mask_vector, step_loss # all have dimension: b\n",
    "\n",
    "class Dynamic_Decoder(nn.Module):\n",
    "    def __init__(self, hidden_dim, maxout_pool_size, max_number_of_iterations, dropout_ratio):\n",
    "        super(Dynamic_Decoder, self).__init__()\n",
    "        self.max_number_of_iterations = max_number_of_iterations\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        # batch_first = True\n",
    "        # Input: has a dimension of B * m * embedding_dim\n",
    "        # Function parameters: input_size, hidden_size, num_layers_of_LSTM = 1(here)\n",
    "        self.decoder = nn.LSTM(4 * hidden_dim, hidden_dim, 1, batch_first=True, bidirectional=False)\n",
    "\n",
    "        self.maxout_start = Highway_Maxout_Network(hidden_dim, maxout_pool_size, dropout_ratio)\n",
    "        self.maxout_end = Highway_Maxout_Network(hidden_dim, maxout_pool_size, dropout_ratio)\n",
    "        \n",
    "    def initHidden(self,batch_size):\n",
    "        h0 = Variable(torch.zeros(1, batch_size, self.hidden_dim), requires_grad = False) # Initial hidden state\n",
    "        c0 = Variable(torch.zeros(1, batch_size, self.hidden_dim), requires_grad = False) # Initial cell state\n",
    "        return h0, c0\n",
    "\n",
    "\n",
    "    def forward(self, U, document_word_sequence_mask,span_tensor):\n",
    "        batch_size, max_word_length, _ = list(U.size()) # U has dimension : B x m x 2l\n",
    "\n",
    "        curr_mask_start,  curr_mask_end = None, None\n",
    "        results_mask_start, results_start = [], []\n",
    "        results_mask_end, results_end = [], []\n",
    "        step_losses = []\n",
    "        \n",
    "\n",
    "        # dimension = B x m\n",
    "        mask_matrix = (1.0 - document_word_sequence_mask.float()) * (-1e30)\n",
    "        \n",
    "        # dimension = B\n",
    "        indices = torch.arange(0, batch_size)\n",
    "\n",
    "        \n",
    "        # initialize start_i_minus_1, end_i_minus_1: these are the initial values of start and end indices\n",
    "        # start_i_minus_1 = the first index for the context/question \n",
    "        # end_i_minus_1 = the last index for the context/question \n",
    "        \n",
    "        # dimension = B\n",
    "        start_i_minus_1 = torch.zeros(batch_size).long()\n",
    "        \n",
    "        # dimension = B\n",
    "        end_i_minus_1 = torch.sum(document_word_sequence_mask, 1) - 1\n",
    "\n",
    "        \n",
    "\n",
    "        # After every iteration the hidden and current state \n",
    "        # at t = length of the sequence (for the one-directional lstm) will\n",
    "        # be returned by the lstm\n",
    "        # the hidden_state_i(h_i) will serve as an input to next lstm\n",
    "        hidden_and_current_state_i = self.initHidden(batch_size)\n",
    "        start_target = None\n",
    "        end_target = None\n",
    "        \n",
    "        # this sets the start and end target (ie. the y_label) for an answer\n",
    "        if span_tensor is not None:\n",
    "            # Dimension = B\n",
    "            start_target = span_tensor[:,0]\n",
    "            \n",
    "            \n",
    "            # Dimension = B\n",
    "            end_target = span_tensor[:,1]\n",
    "            \n",
    "    \n",
    "            \n",
    "        # this is just an initialization of u_start_i_minus_1\n",
    "        # u_start_i_minus_1 is essentially u_start_zero outside the loop\n",
    "        u_start_i_minus_1 = U[indices, start_i_minus_1, :]  # B x 2l\n",
    "        \n",
    "        # Why do we need an iterative procedure to predict the start and end indices for an answer ? \n",
    "        # Solution: there may exist several intuitive answer spans within the document, each corresponding to a\n",
    "        # local maxima. An iterative technique to select an answer span by alternating between\n",
    "        # predicting the start point and predicting the end point. This iterative procedure allows the model to\n",
    "        # recover from initial local maxima corresponding to incorrect answer spans.\n",
    "        for _ in range(self.max_number_of_iterations):\n",
    "            u_end_i_minus_1 = U[indices, end_i_minus_1, :]  # B x 2l\n",
    "            \n",
    "            # u_concatenated is fed to the lstm\n",
    "            u_concatenated = torch.cat((u_start_i_minus_1, u_end_i_minus_1), 1)  # B x 4l\n",
    "            \n",
    "        \n",
    "\n",
    "            # the hidden_and_current_state_i = h_i,c_i are essentially hidden and current cell states \n",
    "            # for t = length of the sequence (for the one-directional lstm) after every iteration\n",
    "            # u_concatenated.unsqueeze(1) has a dimension : B x 1 x 4l\n",
    "            lstm_output, hidden_and_current_state_i = self.decoder(u_concatenated.unsqueeze(1), hidden_and_current_state_i)\n",
    "            \n",
    "            # h_i has dimension = 1 x B x l\n",
    "            # c_i has dimension = 1 x B x l\n",
    "            h_i, c_i = hidden_and_current_state_i\n",
    "            \n",
    "            \n",
    "\n",
    "            # Inputs to the Highway_Maxout_Network(to find start index) are: hidden_state_i(h_i), start_i_minus_1(index), u_concatenated ==>(u_start_i_minus_1;u_end_i_minus_1) \n",
    "            start_i_minus_1, curr_mask_start, step_loss_start = self.maxout_start(h_i, U, curr_mask_start, start_i_minus_1,\n",
    "                                                                u_concatenated, mask_matrix, start_target)\n",
    "            \n",
    "            \n",
    "            \n",
    "            u_start_i_minus_1 = U[indices, start_i_minus_1, :]  # B x 2l\n",
    "\n",
    "            u_concatenated = torch.cat((u_start_i_minus_1, u_end_i_minus_1), 1)  # b x 4l\n",
    "\n",
    "            # Inputs to the Highway_Maxout_Network(to find end index) are: hidden_state_i(h_i), end_i_minus_1(index), u_concatenated ==>(u_start_i_minus_1;u_end_i_minus_1) \n",
    "            end_i_minus_1, curr_mask_end, step_loss_end = self.maxout_end(h_i, U, curr_mask_end, end_i_minus_1,\n",
    "                                                              u_concatenated, mask_matrix, end_target)\n",
    "\n",
    "            # we minimize the cumulative softmax cross entropy of the start and end points across all iterations.\n",
    "            if span_tensor is not None:\n",
    "                step_loss = step_loss_start + step_loss_end\n",
    "#                 print(step_loss)\n",
    "                step_losses.append(step_loss)\n",
    "            \n",
    "            results_mask_start.append(curr_mask_start) # appends all the curr_mask_start ==> dimension: num_iterations x B\n",
    "            results_start.append(start_i_minus_1) # appends all the start_indexes ==> dimension: num_iterations x B\n",
    "            results_mask_end.append(curr_mask_end) # appends all the curr_mask_end ==> dimension: num_iterations x B\n",
    "            results_end.append(end_i_minus_1) # appends all the end_indexes ==> dimension: num_iterations x B\n",
    "\n",
    "        \n",
    "        \n",
    "        # Dimension = B\n",
    "        result_pos_start1 = torch.sum(torch.stack(results_mask_start, 1), 1).long()\n",
    "        result_pos_start = result_pos_start1 - 1\n",
    "        \n",
    "        # Dimension = B\n",
    "        index_start = torch.gather(torch.stack(results_start, 1), 1, result_pos_start.unsqueeze(1)).squeeze()\n",
    "\n",
    "        # Dimension = B\n",
    "        result_pos_end1 = torch.sum(torch.stack(results_mask_end, 1), 1).long()\n",
    "        result_pos_end = result_pos_end1 - 1\n",
    "        \n",
    "        # Dimension = B\n",
    "        index_end = torch.gather(torch.stack(results_end, 1), 1, result_pos_end.unsqueeze(1)).squeeze()\n",
    "\n",
    "        loss = None\n",
    "\n",
    "#         print(\"step_losses\")\n",
    "#         print(sum(step_losses))\n",
    "        if span_tensor is not None:\n",
    "            # step losses has dimension = num_iterations x B\n",
    "            sum_losses = sum(step_losses)\n",
    "            batch_avg_loss = sum_losses / self.max_number_of_iterations\n",
    "            loss = batch_avg_loss\n",
    "\n",
    "            \n",
    "        return loss, index_start, index_end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "\n",
    "class Coattention_Encoder(nn.Module):\n",
    "    def __init__(self, hidden_dim, maxout_pool_size, embedding_matrix, max_number_of_iterations, dropout_ratio):\n",
    "        super(Coattention_Encoder, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        ## nn.Linear(input_dim, output_dim)\n",
    "        # Affine mapping from l ==> l\n",
    "        self.question_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        \n",
    "        self.fusion_bilstm = Fusion_BiLSTM(hidden_dim, dropout_ratio)\n",
    "#         self.dropout = nn.Dropout(p=dropout_ratio)\n",
    "\n",
    "    def forward(self, question_representation, context_representation,document_word_sequence_mask):\n",
    "        \n",
    "        ############## m = max length of instances in one batch of document ;  n= max length of instances in one batch of question ############################33\n",
    "        Q = question_representation # B x (n + 1) x l\n",
    "        D = context_representation  # B x (m + 1) x l\n",
    "        \n",
    "#         print(\"question_representation.(Output to Encoder Layer) ==  \" + str(Q.size()))\n",
    "#         print(\"context_representation. (Output to Encoder Layer)  ==  \" + str(D.size()))\n",
    "\n",
    "        # view function is meant to reshape the tensor.(Similar to reshape function in numpy)\n",
    "        # view( row_size = -1 ,means that number of rows are unknown, column_size)\n",
    "        # pass the Q tensor through a non-linearity \n",
    "        Q2 = torch.tanh(self.question_proj(Q.view(-1, self.hidden_dim))).view(Q.size()) #B x (n + 1) x l\n",
    "\n",
    "        ##################################   Co-Attention starts here  #######################################\n",
    "        \n",
    "        ########################################   Step - 1  ##################################################\n",
    "        # transpose(tensor, first_dimension to be transposed, second_dimension to be transposed)\n",
    "        Q_transpose = torch.transpose(Q2, 1, 2) #dimension: B x l x (n + 1)\n",
    "        \n",
    "        # Performs a batch matrix-matrix product of matrices stored in batch1 and batch2.\n",
    "        # batch1 and batch2 must be 3-D tensors each containing the same number of matrices.\n",
    "        L = torch.bmm(D, Q_transpose) # dimension of L : B x (m + 1) x (n + 1)\n",
    "\n",
    "        ####################################### Step-2 ######################################################\n",
    "        A_Q = F.softmax(L, dim=2) # B x (m + 1) x (n + 1)\n",
    "\n",
    "\n",
    "        D_transpose = torch.transpose(D, 1, 2) #dimension: B x l x (m + 1)\n",
    "        C_Q = torch.bmm(D_transpose, A_Q) # (B x l x (m + 1)) x (B x (m + 1) x (n + 1)) => B x l x (n + 1)\n",
    "\n",
    "        ####################################### Step-3 #######################################################\n",
    "        L_tranpose = torch.transpose(L,1,2)\n",
    "        A_D = F.softmax(L_tranpose, dim=2)  # B x (n + 1) x (m + 1)\n",
    "        \n",
    "        \n",
    "        # concatenation along dimension=1:(B x l x (n + 1) ; B x l x (n + 1)  -----> B x 2l x (n + 1) ) x (B x (n + 1) x (m + 1)) ====> B x 2l x (m + 1)\n",
    "        C_D = torch.bmm(torch.cat((Q_transpose, C_Q), 1), A_D) # B x 2l x (m + 1)\n",
    "        C_D_transpose = torch.transpose(C_D, 1, 2)  # B x (m + 1) x 2l\n",
    "\n",
    "        \n",
    "        #######################################  Step-4 ##########################################################\n",
    "        #fusion BiLSTM\n",
    "        # concatenation along dimension = 2:  (B x (m + 1) x 2l ; B x (m + 1) x l  -----> B x (m + 1) x 3l )\n",
    "        bi_lstm_input = torch.cat((C_D_transpose, D), 2) # B x (m + 1) x 3l\n",
    "       \n",
    "        U = self.fusion_bilstm(bi_lstm_input, document_word_sequence_mask) # B x m x 2l\n",
    "        \n",
    "#         print(\"size of U.(U is output of Co-attention encoder) ==  \" + str(U.size()))\n",
    "        \n",
    "        return U\n",
    "\n",
    "\n",
    "class Fusion_BiLSTM(nn.Module):\n",
    "    def __init__(self, hidden_dim, dropout_ratio):\n",
    "        super(Fusion_BiLSTM, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "         # batch_first = True\n",
    "        # Input: has a dimension of B * m * embedding_dim\n",
    "        # Function parameters: input_size, hidden_size, num_layers_of_LSTM = 1(here)\n",
    "        self.fusion_bilstm = nn.LSTM(3 * hidden_dim, hidden_dim, 1, batch_first=True,\n",
    "                                     bidirectional=True, dropout=dropout_ratio)\n",
    "        \n",
    "#         self.dropout = nn.Dropout(p=dropout_ratio)\n",
    "\n",
    "    def initHidden(self,batch_size):\n",
    "        h0 = Variable(torch.zeros(2, batch_size, self.hidden_dim), requires_grad = False) # Initial hidden state\n",
    "        c0 = Variable(torch.zeros(2, batch_size, self.hidden_dim), requires_grad = False) # Initial cell state\n",
    "        return h0, c0\n",
    "\n",
    "    def forward(self, word_sequence_embeddings, word_sequence_mask):\n",
    "        \n",
    "        # stores length of per instance for context/question\n",
    "        length_per_instance = torch.sum(word_sequence_mask, 1)\n",
    "        \n",
    "        initial_hidden_states = self.initHidden(len(length_per_instance))\n",
    "      \n",
    "        # All RNN modules accept packed sequences as inputs.\n",
    "        # Input: word_sequence_embeddings has a dimension of B x m+1 x 3l (l is the size of the glove_embedding/ pre-trained embedding/embedding_dim)\n",
    "        packed_word_sequence_embeddings = pack_padded_sequence(word_sequence_embeddings, length_per_instance, batch_first=True,enforce_sorted=False)\n",
    "        \n",
    "        # since the input was a packed sequence, the output will also be a packed sequence\n",
    "        output, _ = self.fusion_bilstm(packed_word_sequence_embeddings,initial_hidden_states)\n",
    "        \n",
    "        # Pads a packed batch of variable length sequences.\n",
    "        # It is an inverse operation to pack_padded_sequence().\n",
    "        # dimension:  B x m x 2l\n",
    "        output_to_BiLSTM_padded, _ = pad_packed_sequence(output, batch_first=True)\n",
    "\n",
    "\n",
    "        return output_to_BiLSTM_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "class Config(object):\n",
    "    pass\n",
    "\n",
    "config = Config()\n",
    "config.data_dir = \"E:\\\\Internships_19\\\\Internship(Summer_19)\\\\Q&A_Toolkit\\\\Dataset_analysis\\\\SQuAD\\\\\"\n",
    "config.word_embedding_size = 100\n",
    "config.hidden_dim = 300\n",
    "config.dropout_ratio = 0.15\n",
    "config.max_context_length = 600\n",
    "config.max_question_length = 30\n",
    "config.print_and_validate_every = 2\n",
    "\n",
    "#vector with zeros for unknown words\n",
    "config.num_iterations = 2\n",
    "config.maxout_pool_size=16\n",
    "\n",
    "config.lr = 0.001\n",
    "config.dropout_ratio = 0.15\n",
    "config.early_stop = 10\n",
    "\n",
    "config.max_grad_norm = 5.0\n",
    "config.batch_size = 20\n",
    "config.num_epochs = 2\n",
    "config.model_dir = \"E:\\\\Internships_19\\\\Internship(Summer_19)\\\\Q&A_Toolkit\\\\Dynamic_Coattention_Networks\\\\Models\\\\saved_models\"\n",
    "\n",
    "# config.print_every = 100\n",
    "# config.save_every = 50000000\n",
    "# config.eval_every = 1000\n",
    "\n",
    "# config.model_type = 'co-attention'\n",
    "config.reg_lambda = 0.00007\n",
    "config.names = [\"train_context\",\"train_question\"]\n",
    "config.print_every = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"E:\\\\Internships_19\\\\Internship(Summer_19)\\\\Q&A_Toolkit\\\\Dataset_analysis\\\\SQuAD\\\\train_word_index.context_pkl.pkl\", \"rb\") as input_file:\n",
    "    train_word_index = pickle.load(input_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_word_index = (np.array(train_word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "136"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_word_index[81509])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_word_index = np.sort(train_word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "130319"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len((train_word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(700, 81510)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_max_length(data):\n",
    "\n",
    "    \"\"\" Finds the maximum sequence length for data \n",
    "        Args:\n",
    "            data: The data from which sequences will be chosen\n",
    "\n",
    "    \"\"\"\n",
    "    temp = 0\n",
    "    index = 0\n",
    "    for i, _ in enumerate(data):\n",
    "\n",
    "        if (len(data[i]) > temp):\n",
    "            temp = len(data[i])\n",
    "            index = i\n",
    "\n",
    "    return temp,index\n",
    "find_max_length(train_word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "total loss for epoch number = 1 = 32.17833232879639\n",
    "total loss for epoch number = 2 = 32.168625831604004\n",
    "total loss for epoch number = 3 = 32.14808177947998\n",
    "total loss for epoch number = 4 = 32.09620189666748\n",
    "total loss for epoch number = 5 = 31.954957962036133\n",
    "total loss for epoch number = 6 = 31.559465408325195\n",
    "total loss for epoch number = 7 = 30.727495193481445\n",
    "total loss for epoch number = 8 = 29.533214569091797\n",
    "total loss for epoch number = 9 = 29.055150032043457\n",
    "total loss for epoch number = 10 = 27.7101993560791\n",
    "total loss for epoch number = 11 = 27.483716011047363\n",
    "total loss for epoch number = 12 = 26.665382385253906\n",
    "total loss for epoch number = 13 = 25.93720054626465\n",
    "total loss for epoch number = 14 = 25.415908813476562\n",
    "total loss for epoch number = 15 = 24.633482933044434\n",
    "total loss for epoch number = 16 = 23.936548709869385\n",
    "total loss for epoch number = 17 = 23.339470863342285\n",
    "total loss for epoch number = 18 = 22.724595546722412\n",
    "total loss for epoch number = 19 = 21.955151081085205\n",
    "total loss for epoch number = 20 = 21.523600578308105\n",
    "total loss for epoch number = 21 = 21.19681406021118\n",
    "total loss for epoch number = 22 = 21.83211851119995\n",
    "total loss for epoch number = 23 = 21.1593337059021\n",
    "total loss for epoch number = 24 = 20.900564670562744\n",
    "total loss for epoch number = 25 = 19.70767068862915\n",
    "total loss for epoch number = 26 = 19.67492914199829\n",
    "total loss for epoch number = 27 = 19.265146255493164\n",
    "total loss for epoch number = 28 = 18.597230911254883\n",
    "total loss for epoch number = 29 = 18.266369819641113\n",
    "total loss for epoch number = 30 = 17.68292474746704\n",
    "total loss for epoch number = 31 = 17.3766827583313\n",
    "total loss for epoch number = 32 = 17.918822288513184\n",
    "total loss for epoch number = 33 = 18.077731609344482\n",
    "total loss for epoch number = 34 = 17.40169382095337\n",
    "total loss for epoch number = 35 = 17.27264928817749\n",
    "total loss for epoch number = 36 = 16.654419898986816\n",
    "total loss for epoch number = 37 = 16.463799476623535\n",
    "total loss for epoch number = 38 = 15.577938556671143\n",
    "total loss for epoch number = 39 = 15.450960636138916\n",
    "total loss for epoch number = 40 = 15.45249891281128\n",
    "total loss for epoch number = 41 = 16.389851570129395\n",
    "total loss for epoch number = 42 = 17.3285551071167\n",
    "total loss for epoch number = 43 = 17.434212684631348\n",
    "total loss for epoch number = 44 = 18.02951955795288\n",
    "total loss for epoch number = 45 = 17.89313840866089\n",
    "total loss for epoch number = 46 = 17.27841329574585\n",
    "total loss for epoch number = 47 = 15.964763164520264\n",
    "total loss for epoch number = 48 = 15.166417121887207\n",
    "total loss for epoch number = 49 = 14.629198551177979\n",
    "total loss for epoch number = 50 = 14.122273921966553\n",
    "total loss for epoch number = 51 = 13.258476734161377\n",
    "total loss for epoch number = 52 = 13.108572483062744\n",
    "total loss for epoch number = 53 = 12.255869150161743\n",
    "total loss for epoch number = 54 = 13.896694421768188\n",
    "total loss for epoch number = 55 = 13.541181802749634\n",
    "total loss for epoch number = 56 = 14.647278308868408\n",
    "total loss for epoch number = 57 = 15.043529033660889\n",
    "total loss for epoch number = 58 = 13.394643306732178\n",
    "total loss for epoch number = 59 = 13.43801498413086\n",
    "total loss for epoch number = 60 = 12.894775152206421\n",
    "total loss for epoch number = 61 = 11.573852062225342\n",
    "total loss for epoch number = 62 = 10.52441143989563\n",
    "total loss for epoch number = 63 = 9.848382949829102\n",
    "total loss for epoch number = 64 = 9.332906484603882\n",
    "total loss for epoch number = 65 = 9.508471727371216\n",
    "total loss for epoch number = 66 = 13.369320392608643\n",
    "total loss for epoch number = 67 = 12.665861129760742\n",
    "total loss for epoch number = 68 = 11.980929851531982\n",
    "total loss for epoch number = 69 = 11.748188018798828\n",
    "total loss for epoch number = 70 = 11.962870359420776\n",
    "total loss for epoch number = 71 = 10.069189548492432\n",
    "total loss for epoch number = 72 = 10.383047819137573\n",
    "total loss for epoch number = 73 = 9.056565523147583\n",
    "total loss for epoch number = 74 = 9.438700675964355\n",
    "total loss for epoch number = 75 = 7.341150522232056\n",
    "total loss for epoch number = 76 = 6.8021992444992065\n",
    "total loss for epoch number = 77 = 6.210239887237549\n",
    "total loss for epoch number = 78 = 5.8986111879348755\n",
    "total loss for epoch number = 79 = 5.713133215904236\n",
    "total loss for epoch number = 80 = 4.878859996795654\n",
    "total loss for epoch number = 81 = 5.826797127723694\n",
    "total loss for epoch number = 82 = 5.426213502883911\n",
    "total loss for epoch number = 83 = 6.603458046913147\n",
    "total loss for epoch number = 84 = 15.350074291229248\n",
    "total loss for epoch number = 85 = 10.562259197235107\n",
    "total loss for epoch number = 86 = 12.858104705810547\n",
    "total loss for epoch number = 87 = 12.242384910583496\n",
    "total loss for epoch number = 88 = 11.94776177406311\n",
    "total loss for epoch number = 89 = 11.306705236434937\n",
    "total loss for epoch number = 90 = 10.357141494750977\n",
    "total loss for epoch number = 91 = 9.093875646591187\n",
    "total loss for epoch number = 92 = 7.81508207321167\n",
    "total loss for epoch number = 93 = 6.502181529998779\n",
    "total loss for epoch number = 94 = 5.164236545562744\n",
    "total loss for epoch number = 95 = 3.90993595123291\n",
    "total loss for epoch number = 96 = 2.9373868107795715\n",
    "total loss for epoch number = 97 = 2.7593416571617126\n",
    "total loss for epoch number = 98 = 2.326014459133148\n",
    "total loss for epoch number = 99 = 2.6694695949554443\n",
    "total loss for epoch number = 100 = 3.8264707922935486\n",
    "total loss for epoch number = 101 = 2.2964141964912415\n",
    "total loss for epoch number = 102 = 7.579878032207489\n",
    "total loss for epoch number = 103 = 3.5895921885967255\n",
    "total loss for epoch number = 104 = 2.9978184700012207\n",
    "total loss for epoch number = 105 = 4.063562095165253\n",
    "total loss for epoch number = 106 = 2.9828415513038635\n",
    "total loss for epoch number = 107 = 3.369132161140442\n",
    "total loss for epoch number = 108 = 3.351353645324707\n",
    "total loss for epoch number = 109 = 3.794126808643341\n",
    "total loss for epoch number = 110 = 3.012239456176758\n",
    "total loss for epoch number = 111 = 2.8702157735824585\n",
    "total loss for epoch number = 112 = 1.7207311391830444\n",
    "total loss for epoch number = 113 = 2.0022245794534683\n",
    "total loss for epoch number = 114 = 0.9039437919855118\n",
    "total loss for epoch number = 115 = 0.8274446055293083\n",
    "total loss for epoch number = 116 = 0.5814169459044933\n",
    "total loss for epoch number = 117 = 0.40696873515844345\n",
    "total loss for epoch number = 118 = 0.42903827503323555\n",
    "total loss for epoch number = 119 = 0.3167058974504471\n",
    "total loss for epoch number = 120 = 0.25811220332980156\n",
    "total loss for epoch number = 121 = 0.14187653083354235\n",
    "total loss for epoch number = 122 = 0.16914647724479437\n",
    "total loss for epoch number = 123 = 0.11258036363869905\n",
    "total loss for epoch number = 124 = 0.10236110677942634\n",
    "total loss for epoch number = 125 = 0.09372418699786067\n",
    "total loss for epoch number = 126 = 0.06702486285939813\n",
    "total loss for epoch number = 127 = 0.06078878790140152\n",
    "total loss for epoch number = 128 = 0.05785344308242202\n",
    "total loss for epoch number = 129 = 0.053836439503356814\n",
    "total loss for epoch number = 130 = 0.046016820007935166\n",
    "total loss for epoch number = 131 = 0.041798400692641735\n",
    "total loss for epoch number = 132 = 0.03838780801743269\n",
    "total loss for epoch number = 133 = 0.036418596282601357\n",
    "total loss for epoch number = 134 = 0.033840433694422245\n",
    "total loss for epoch number = 135 = 0.03214098769240081\n",
    "total loss for epoch number = 136 = 0.030217934167012572\n",
    "total loss for epoch number = 137 = 0.028761992580257356\n",
    "total loss for epoch number = 138 = 0.027511407039128244\n",
    "total loss for epoch number = 139 = 0.026002503815107048\n",
    "total loss for epoch number = 140 = 0.024525705841369927\n",
    "total loss for epoch number = 141 = 0.02329673815984279\n",
    "total loss for epoch number = 142 = 0.022294552880339324\n",
    "total loss for epoch number = 143 = 0.02144126920029521\n",
    "total loss for epoch number = 144 = 0.020754242432303727\n",
    "total loss for epoch number = 145 = 0.019760132185183465\n",
    "total loss for epoch number = 146 = 0.018839834607206285\n",
    "total loss for epoch number = 147 = 0.018111356417648494\n",
    "total loss for epoch number = 148 = 0.017471503582783043\n",
    "total loss for epoch number = 149 = 0.01690120669081807\n",
    "total loss for epoch number = 150 = 0.016317367204464972\n",
    "total loss for epoch number = 151 = 0.015561295324005187\n",
    "total loss for epoch number = 152 = 0.01515566511079669\n",
    "total loss for epoch number = 153 = 0.014657465624623\n",
    "total loss for epoch number = 154 = 0.01412404392613098\n",
    "total loss for epoch number = 155 = 0.013657505449373275\n",
    "total loss for epoch number = 156 = 0.013160260918084532\n",
    "total loss for epoch number = 157 = 0.012751896865665913\n",
    "total loss for epoch number = 158 = 0.012330755067523569\n",
    "total loss for epoch number = 159 = 0.011996459681540728\n",
    "total loss for epoch number = 160 = 0.011642901285085827\n",
    "total loss for epoch number = 161 = 0.0114046098315157\n",
    "total loss for epoch number = 162 = 0.011008834990207106\n",
    "total loss for epoch number = 163 = 0.010655211750417948\n",
    "total loss for epoch number = 164 = 0.010334905004128814\n",
    "total loss for epoch number = 165 = 0.010084724926855415\n",
    "total loss for epoch number = 166 = 0.009875933115836233\n",
    "total loss for epoch number = 167 = 0.00957024865783751\n",
    "total loss for epoch number = 168 = 0.009287516411859542\n",
    "total loss for epoch number = 169 = 0.009027163730934262\n",
    "total loss for epoch number = 170 = 0.008858680666889995\n",
    "total loss for epoch number = 171 = 0.008627891773357987\n",
    "total loss for epoch number = 172 = 0.008426157059147954\n",
    "total loss for epoch number = 173 = 0.008204332960303873\n",
    "total loss for epoch number = 174 = 0.007964134390931576\n",
    "total loss for epoch number = 175 = 0.00777797686168924\n",
    "total loss for epoch number = 176 = 0.007617823255714029\n",
    "total loss for epoch number = 177 = 0.00746854132739827\n",
    "total loss for epoch number = 178 = 0.007293765025679022\n",
    "total loss for epoch number = 179 = 0.007106908306013793\n",
    "total loss for epoch number = 180 = 0.006946817971765995\n",
    "total loss for epoch number = 181 = 0.006788825936382636\n",
    "total loss for epoch number = 182 = 0.0066535948717501014\n",
    "total loss for epoch number = 183 = 0.0065050766279455274\n",
    "total loss for epoch number = 184 = 0.006362724088830873\n",
    "total loss for epoch number = 185 = 0.0062629701278638095\n",
    "total loss for epoch number = 186 = 0.006134414579719305\n",
    "total loss for epoch number = 187 = 0.005990854842821136\n",
    "total loss for epoch number = 188 = 0.005842717480845749\n",
    "total loss for epoch number = 189 = 0.005730247619794682\n",
    "total loss for epoch number = 190 = 0.005645497585646808\n",
    "total loss for epoch number = 191 = 0.005493609030963853\n",
    "total loss for epoch number = 192 = 0.005418459681095555\n",
    "total loss for epoch number = 193 = 0.005294799600960687\n",
    "total loss for epoch number = 194 = 0.005178833031095564\n",
    "total loss for epoch number = 195 = 0.005076535482658073\n",
    "total loss for epoch number = 196 = 0.005006662831874564\n",
    "total loss for epoch number = 197 = 0.004907608381472528\n",
    "total loss for epoch number = 198 = 0.004784520482644439\n",
    "total loss for epoch number = 199 = 0.004718717187643051\n",
    "total loss for epoch number = 200 = 0.004635111283278093"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"E:\\\\Internships_19\\\\Internship(Summer_19)\\\\Q&A_Toolkit\\\\Dataset_analysis\\\\SQuAD\\\\\"\n",
    "prefix = \"validation\"\n",
    "context_path_train = os.path.join(data_dir, prefix + \".context\")\n",
    "question_path_train = os.path.join(data_dir, prefix +  \".question\")\n",
    "answer_path_train = os.path.join(data_dir, prefix +  \".answer_text\")\n",
    "\n",
    "context_tokens = open(context_path_train, \"r\", encoding=\"utf-8\").readlines()\n",
    "question_tokens =  open(question_path_train, \"r\", encoding=\"utf-8\").readlines()\n",
    "answer_tokens = open(answer_path_train, \"r\", encoding=\"utf-8\").readlines()\n",
    "#         print(question_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['france\\n',\n",
       " '10th and 11th centuries\\n',\n",
       " 'denmark , iceland and norway\\n',\n",
       " 'rollo\\n',\n",
       " '10th century\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " 'william the conqueror\\n',\n",
       " 'richard i\\n',\n",
       " 'catholic\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " 'viking\\n',\n",
       " '9th century\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '911\\n',\n",
       " 'king charles iii\\n',\n",
       " 'seine\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " 'rollo\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " 'catholicism\\n',\n",
       " 'north\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " 'fighting horsemen\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " 'seljuk turks\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '1050s\\n',\n",
       " '1060s\\n',\n",
       " 'alexius komnenos\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " 'afranji\\n',\n",
       " 'oursel\\n',\n",
       " 'turkish forces\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " 'norman mercenary\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " 'robert guiscard\\n',\n",
       " '1082\\n',\n",
       " '30,000\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " 'deabolis\\n',\n",
       " 'bohemond\\n',\n",
       " 'deabolis\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '1185\\n',\n",
       " 'dyrrachium\\n',\n",
       " 'the adriatic\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " 'king ethelred ii\\n',\n",
       " 'duke richard ii\\n',\n",
       " 'normandy\\n',\n",
       " 'sweyn forkbeard\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " 'harthacnut\\n',\n",
       " '1041\\n',\n",
       " 'robert of jumiges\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " 'battle of hastings\\n',\n",
       " 'william ii\\n',\n",
       " '1066\\n',\n",
       " 'anglo-saxons\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " 'modern english\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '1169\\n',\n",
       " 'ireland\\n',\n",
       " 'irish\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " 'edgar\\n',\n",
       " 'king malcolm iii of scotland\\n',\n",
       " '1072\\n',\n",
       " 'duncan\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " 'sybilla of normandy\\n',\n",
       " 'norman\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " 'hereford\\n',\n",
       " 'the welsh\\n',\n",
       " 'edward the confessor\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " 'wales\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '1018\\n',\n",
       " 'william of montreuil\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '1097\\n',\n",
       " 'tancred\\n',\n",
       " 'jerusalem\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '380 years\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " 'a storm\\n',\n",
       " 'berengaria\\n',\n",
       " '1191\\n',\n",
       " 'isaac komnenos\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " 'conrad of montferrat\\n',\n",
       " 'silver\\n',\n",
       " 'guy de lusignan\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " 'africa\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " 'bethencourt\\n',\n",
       " 'enrique prez de guzmn\\n',\n",
       " 'maciot de bethencourt\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " 'channel islands\\n',\n",
       " 'two\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " 'romanesque\\n',\n",
       " 'rounded\\n',\n",
       " '< u n k >\\n',\n",
       " 'early gothic\\n',\n",
       " 'anglo-saxon\\n',\n",
       " 'sicily\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " 'early 11th century\\n',\n",
       " 'dukes\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '16th century\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " 'embroidery\\n',\n",
       " 'bayeux tapestry\\n',\n",
       " 'odo\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " 'mosaics\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '11th\\n',\n",
       " 'william of volpiano and john of ravenna\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " 'southern italy\\n',\n",
       " \"latin monastery at sant'eufemia .\\n\",\n",
       " 'robert guiscard\\n',\n",
       " 'singing\\n',\n",
       " '< u n k >\\n',\n",
       " 'computational complexity theory\\n',\n",
       " 'inherent difficulty\\n',\n",
       " 'computational problems\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " 'if its solution requires significant resources\\n',\n",
       " 'mathematical models of computation\\n',\n",
       " 'time and storage\\n',\n",
       " 'number of gates in a circuit\\n',\n",
       " 'determine the practical limits on what computers can and can not do\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " 'analysis of algorithms and computability theory\\n',\n",
       " 'analysis of algorithms\\n',\n",
       " 'computational complexity theory\\n',\n",
       " 'computability theory\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " 'problem instance\\n',\n",
       " 'the problem\\n',\n",
       " 'concrete\\n',\n",
       " 'instances\\n',\n",
       " 'solution\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '2000\\n',\n",
       " 'round trip through all sites in milan\\n',\n",
       " 'computational problems\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " 'problem instance\\n',\n",
       " 'binary alphabet\\n',\n",
       " 'bitstrings\\n',\n",
       " 'binary notation\\n',\n",
       " 'adjacency matrices\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " 'decision problems\\n',\n",
       " 'yes or no\\n',\n",
       " '1 or 0\\n',\n",
       " 'yes\\n',\n",
       " 'yes\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " 'arbitrary graph\\n',\n",
       " 'formal language\\n',\n",
       " 'how graphs are encoded as binary strings\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " 'a computational problem\\n',\n",
       " 'a single output\\n',\n",
       " 'a function problem\\n',\n",
       " 'the integer factorization problem\\n',\n",
       " 'complex\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " 'decision problems\\n',\n",
       " 'set of triples\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " 'how much time the best algorithm requires to solve the problem\\n',\n",
       " 'the instance\\n',\n",
       " 'as a function of the size of the instance\\n',\n",
       " 'bits\\n',\n",
       " 'an increase in the input size\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " \"cobham 's thesis\\n\",\n",
       " 'the time taken\\n',\n",
       " 'worst-case time complexity\\n',\n",
       " 't ( n )\\n',\n",
       " 'polynomial time algorithm\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " 'a turing machine\\n',\n",
       " 'an algorithm\\n',\n",
       " 'the turing machine\\n',\n",
       " 'symbols\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " 'a deterministic turing machine\\n',\n",
       " 'rules\\n',\n",
       " 'a probabilistic turing machine\\n',\n",
       " 'a non-deterministic turing machine\\n',\n",
       " 'randomized algorithms\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " 'complexity classes\\n',\n",
       " 'time or space\\n',\n",
       " 'probabilistic turing machines , non-deterministic turing machines\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " 'random access machines\\n',\n",
       " 'computational power\\n',\n",
       " 'time and memory\\n',\n",
       " 'the machines operate deterministically\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " 'non-deterministic\\n',\n",
       " 'unusual resources\\n',\n",
       " 'mathematical models\\n',\n",
       " 'time\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " 'state transitions\\n',\n",
       " 'difficulty\\n',\n",
       " 'dtime ( f ( n ) )\\n',\n",
       " 'time\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " 'complexity resources\\n',\n",
       " 'computational resource\\n',\n",
       " 'blum complexity axioms\\n',\n",
       " 'complexity measures\\n',\n",
       " 'complexity measures\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " 'best , worst and average\\n',\n",
       " 'complexity measure\\n',\n",
       " 'time\\n',\n",
       " 'inputs\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " 'deterministic sorting algorithm quicksort\\n',\n",
       " 'worst-case\\n',\n",
       " 'o ( n2 )\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " 'the most efficient algorithm\\n',\n",
       " 'analysis of algorithms\\n',\n",
       " 'lower bounds\\n',\n",
       " 'upper bound\\n',\n",
       " 'all possible algorithms\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " 'big o notation\\n',\n",
       " 'constant factors and smaller terms\\n',\n",
       " 't ( n ) = o ( n2 )\\n',\n",
       " 'the computational model\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " 'complexity classes\\n',\n",
       " 'framework\\n',\n",
       " 'complicated definitions\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " 'chosen machine model\\n',\n",
       " 'linear time\\n',\n",
       " 'single-tape turing machines\\n',\n",
       " 'cobham-edmonds thesis\\n',\n",
       " 'complexity class p\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " 'time or space\\n',\n",
       " 'bounding\\n',\n",
       " 'complexity classes\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " 'bpp , zpp and rp\\n',\n",
       " 'boolean\\n',\n",
       " 'quantum\\n',\n",
       " '# p\\n',\n",
       " 'interactive\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " 'computation time\\n',\n",
       " 'dtime ( n2 )\\n',\n",
       " 'time and space hierarchy theorems\\n',\n",
       " 'a proper hierarchy on the classes defined\\n',\n",
       " 'quantitative statements\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " 'time and space hierarchy theorems\\n',\n",
       " 'exptime\\n',\n",
       " 'pspace\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " 'reduction\\n',\n",
       " 'another problem\\n',\n",
       " 'reduces\\n',\n",
       " 'karp reductions and levin reductions\\n',\n",
       " 'the bound on the complexity of reductions\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " 'polynomial-time reduction\\n',\n",
       " 'multiplying two integers\\n',\n",
       " 'polynomial time\\n',\n",
       " 'input\\n',\n",
       " 'multiplication\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " 'the type of reduction being used\\n',\n",
       " 'if every problem in c can be reduced to x\\n',\n",
       " 'solve any problem in c\\n',\n",
       " 'np-hard\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " 'np-complete\\n',\n",
       " 'np\\n',\n",
       " 'there is no known polynomial-time solution\\n',\n",
       " 'np\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " 'p\\n',\n",
       " 'cobhamedmonds thesis\\n',\n",
       " 'np\\n',\n",
       " 'boolean satisfiability problem\\n',\n",
       " 'turing machines\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " 'more efficient solutions\\n',\n",
       " 'protein structure prediction\\n',\n",
       " '$ 1,000,000\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " 'ladner\\n',\n",
       " 'np-intermediate problems\\n',\n",
       " 'graph isomorphism problem\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " 'the graph isomorphism problem\\n',\n",
       " 'np-complete\\n',\n",
       " 'polynomial time hierarchy\\n',\n",
       " 'second level\\n',\n",
       " 'laszlo babai and eugene luks\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " 'the integer factorization problem\\n',\n",
       " 'k\\n',\n",
       " 'modern cryptographic systems\\n',\n",
       " 'the general number field sieve\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " 'suspected to be unequal\\n',\n",
       " 'p  np  pp  pspace\\n',\n",
       " 'between p and pspace\\n',\n",
       " 'proving that any of these classes are unequal\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " 'co-np\\n',\n",
       " 'reversed\\n',\n",
       " 'not equal\\n',\n",
       " 'p is not equal to np\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " 'l\\n',\n",
       " 'strictly contained in p or equal to p\\n',\n",
       " 'complexity classes\\n',\n",
       " 'nl and nc\\n',\n",
       " 'if they are distinct or equal classes\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " 'intractable problems\\n',\n",
       " 'exponential-time algorithms\\n',\n",
       " 'np-complete problems\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " 'presburger arithmetic\\n',\n",
       " 'algorithms have been written\\n',\n",
       " 'np-complete knapsack problem\\n',\n",
       " 'in less than quadratic time\\n',\n",
       " 'np-complete boolean satisfiability problem\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " 'foundations were laid out\\n',\n",
       " 'alan turing\\n',\n",
       " 'turing machines\\n',\n",
       " '1936\\n',\n",
       " 'a computer\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " 'on the computational complexity of algorithms\\n',\n",
       " 'juris hartmanis and richard stearns\\n',\n",
       " '1965\\n',\n",
       " 'time and space\\n',\n",
       " '1965\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " 'john myhill\\n',\n",
       " '1961\\n',\n",
       " 'hisao yamada\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " 'input encoding\\n',\n",
       " 'encoding\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " 'manuel blum\\n',\n",
       " 'speed-up theorem\\n',\n",
       " \"`` reducibility among combinatorial problems ''\\n\",\n",
       " '21\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " 'socal\\n',\n",
       " '10 counties\\n',\n",
       " 'economic center\\n',\n",
       " 'demographics and economic ties\\n',\n",
       " 'historical political divisions\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " 'southern california megaregion\\n',\n",
       " '11\\n',\n",
       " 'nevada\\n',\n",
       " 'mexican\\n',\n",
       " 'tijuana\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " 'pacific\\n',\n",
       " 'seven\\n',\n",
       " '12 million\\n',\n",
       " 'san diego\\n',\n",
       " '17.5 million\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " 'colorado river\\n',\n",
       " 'colorado desert\\n',\n",
       " 'mojave desert\\n',\n",
       " 'mexicounited states border\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " 'california\\n',\n",
       " '3,792,621\\n',\n",
       " 'los angeles\\n',\n",
       " 'san diego\\n',\n",
       " 'south\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " 'los angeles\\n',\n",
       " 'united states\\n',\n",
       " 'counties\\n',\n",
       " '15\\n',\n",
       " 'counties\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " 'hollywood\\n',\n",
       " 'los angeles\\n',\n",
       " 'the walt disney company\\n',\n",
       " 'music\\n',\n",
       " 'sony\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " 'skateboard\\n',\n",
       " 'tony hawk\\n',\n",
       " 'shaun white\\n',\n",
       " 'oahu\\n',\n",
       " 'transpac\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " 'palm springs\\n',\n",
       " 'beaches\\n',\n",
       " 'southern\\n',\n",
       " 'open spaces\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " \"37 9 ' 58.23 ''\\n\",\n",
       " '11\\n',\n",
       " 'ten\\n',\n",
       " 'tehachapi mountains\\n',\n",
       " 'northern\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " 'mexico\\n',\n",
       " 'alta california\\n',\n",
       " 'monterey\\n',\n",
       " 'the missouri compromise\\n',\n",
       " 'free\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " 'inequitable taxes\\n',\n",
       " 'cow counties\\n',\n",
       " 'three\\n',\n",
       " '75\\n',\n",
       " 'milton latham\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " 'los angeles times\\n',\n",
       " '1900\\n',\n",
       " '1999\\n',\n",
       " 'imperial\\n',\n",
       " 'seven\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " 'regional tourism groups\\n',\n",
       " 'california state automobile association\\n',\n",
       " 'three-region\\n',\n",
       " 'tehachapis\\n',\n",
       " 'southern\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " 'third\\n',\n",
       " 'vast areas\\n',\n",
       " 'suburban\\n',\n",
       " 'highways\\n',\n",
       " 'international metropolitan\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " 'camp pendleton\\n',\n",
       " 'inland empire\\n',\n",
       " 'united states census bureau\\n',\n",
       " 'orange\\n',\n",
       " '1990s\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " 'mediterranean\\n',\n",
       " 'infrequent rain\\n',\n",
       " \"60 's\\n\",\n",
       " 'very rare\\n',\n",
       " '70\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " 'varied\\n',\n",
       " 'pacific ocean\\n',\n",
       " 'topographic\\n',\n",
       " 'peninsular\\n',\n",
       " 'valleys\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '10,000\\n',\n",
       " 'small\\n',\n",
       " '6.7\\n',\n",
       " 'property damage\\n',\n",
       " '$ 20 billion\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " 'san andreas\\n',\n",
       " '6.7\\n',\n",
       " 'puente hills\\n',\n",
       " 'usgs\\n',\n",
       " 'occurrence\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " 'economically\\n',\n",
       " 'global\\n',\n",
       " 'economic\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '2010\\n',\n",
       " 'high growth rates\\n',\n",
       " '10.0 %\\n',\n",
       " 'tech-oriented\\n',\n",
       " 'greater sacramento\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " 'metropolitan statistical areas\\n',\n",
       " 'two\\n',\n",
       " 'five million\\n',\n",
       " 'southern border region\\n',\n",
       " '17,786,419\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " 'los angeles\\n',\n",
       " '1.3 million\\n',\n",
       " 'twelve\\n',\n",
       " '100,000\\n',\n",
       " 'riverside\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " 'petroleum\\n',\n",
       " 'hollywood\\n',\n",
       " 'the housing bubble\\n',\n",
       " 'diverse\\n',\n",
       " 'heavily impacted\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '1920s\\n',\n",
       " 'richest\\n',\n",
       " 'cattle\\n',\n",
       " 'citrus\\n',\n",
       " 'aerospace\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " 'business\\n',\n",
       " 'central business districts\\n',\n",
       " 'south coast metro\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " 'business\\n',\n",
       " 'los angeles area\\n',\n",
       " 'san fernando valley\\n',\n",
       " 'los angeles\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " 'business\\n',\n",
       " 'riverside\\n',\n",
       " 'hospitality business/financial centre\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " 'orange\\n',\n",
       " 'university of california , irvine\\n',\n",
       " 'west irvine\\n',\n",
       " 'south coast metro\\n',\n",
       " 'rapidly\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " 'downtown san diego\\n',\n",
       " 'northern san diego\\n',\n",
       " 'north county\\n',\n",
       " 'san diego\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " 'los angeles international airport\\n',\n",
       " 'passenger volume\\n',\n",
       " 'third\\n',\n",
       " 'san diego international airport\\n',\n",
       " 'van nuys airport\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " 'metrolink\\n',\n",
       " 'seven\\n',\n",
       " 'six\\n',\n",
       " 'orange\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " 'port of los angeles\\n',\n",
       " 'port of san diego\\n',\n",
       " 'southern\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " 'the tech coast\\n',\n",
       " 'research\\n',\n",
       " 'private\\n',\n",
       " '5\\n',\n",
       " '12\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " 'nfl\\n',\n",
       " 'nba\\n',\n",
       " 'mlb\\n',\n",
       " 'los angeles kings\\n',\n",
       " 'la galaxy\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " 'chivas usa\\n',\n",
       " 'two\\n',\n",
       " '2014\\n',\n",
       " 'stubhub center\\n',\n",
       " '2018\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " 'college\\n',\n",
       " 'ucla\\n',\n",
       " 'trojans\\n',\n",
       " 'pac-12\\n',\n",
       " 'division i\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " 'rugby\\n',\n",
       " 'high school\\n',\n",
       " 'an official school sport\\n',\n",
       " '< u n k >\\n',\n",
       " 'bskyb\\n',\n",
       " 'bskyb\\n',\n",
       " '2014\\n',\n",
       " 'sky plc\\n',\n",
       " 'sky uk limited\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '2006\\n',\n",
       " 'two\\n',\n",
       " 'sky\\n',\n",
       " '1.3bn\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " 'ondigital\\n',\n",
       " 'freeview\\n',\n",
       " 'three\\n',\n",
       " 'sky three\\n',\n",
       " 'pick tv\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " 'sky+ pvr\\n',\n",
       " 'september 2007\\n',\n",
       " 'monthly fee\\n',\n",
       " 'january 2010\\n',\n",
       " 'sky+hd box\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " 'videoguard\\n',\n",
       " 'nds\\n',\n",
       " 'cisco systems\\n',\n",
       " 'bskyb\\n',\n",
       " 'sky+\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '< u n k >\\n',\n",
       " '2007\\n',\n",
       " ...]"
      ]
     },
     "execution_count": 528,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['40\\n',\n",
       " '29\\n',\n",
       " '84\\n',\n",
       " '23\\n',\n",
       " '42\\n',\n",
       " '50\\n',\n",
       " '80\\n',\n",
       " '56\\n',\n",
       " '42\\n',\n",
       " '45\\n',\n",
       " '80\\n',\n",
       " '84\\n',\n",
       " '93\\n',\n",
       " '45\\n',\n",
       " '80\\n',\n",
       " '31\\n',\n",
       " '55\\n',\n",
       " '88\\n',\n",
       " '38\\n',\n",
       " '92\\n',\n",
       " '27\\n',\n",
       " '63\\n',\n",
       " '8\\n',\n",
       " '16\\n',\n",
       " '38\\n',\n",
       " '55\\n',\n",
       " '72\\n',\n",
       " '15\\n',\n",
       " '155\\n',\n",
       " '164\\n',\n",
       " '118\\n',\n",
       " '164\\n',\n",
       " '4\\n',\n",
       " '161\\n',\n",
       " '59\\n',\n",
       " '69\\n',\n",
       " '59\\n',\n",
       " '88\\n',\n",
       " '164\\n',\n",
       " '51\\n',\n",
       " '34\\n',\n",
       " '55\\n',\n",
       " '92\\n',\n",
       " '25\\n',\n",
       " '17\\n',\n",
       " '41\\n',\n",
       " '85\\n',\n",
       " '25\\n',\n",
       " '19\\n',\n",
       " '41\\n',\n",
       " '85\\n',\n",
       " '92\\n',\n",
       " '8\\n',\n",
       " '24\\n',\n",
       " '78\\n',\n",
       " '22\\n',\n",
       " '114\\n',\n",
       " '74\\n',\n",
       " '60\\n',\n",
       " '8\\n',\n",
       " '24\\n',\n",
       " '55\\n',\n",
       " '114\\n',\n",
       " '52\\n',\n",
       " '95\\n',\n",
       " '156\\n',\n",
       " '52\\n',\n",
       " '94\\n',\n",
       " '212\\n',\n",
       " '156\\n',\n",
       " '2\\n',\n",
       " '3\\n',\n",
       " '33\\n',\n",
       " '52\\n',\n",
       " '94\\n',\n",
       " '191\\n',\n",
       " '39\\n',\n",
       " '146\\n',\n",
       " '212\\n',\n",
       " '10\\n',\n",
       " '17\\n',\n",
       " '39\\n',\n",
       " '147\\n',\n",
       " '212\\n',\n",
       " '17\\n',\n",
       " '39\\n',\n",
       " '58\\n',\n",
       " '121\\n',\n",
       " '212\\n',\n",
       " '25\\n',\n",
       " '48\\n',\n",
       " '116\\n',\n",
       " '28\\n',\n",
       " '62\\n',\n",
       " '116\\n',\n",
       " '18\\n',\n",
       " '23\\n',\n",
       " '116\\n',\n",
       " '18\\n',\n",
       " '7\\n',\n",
       " '34\\n',\n",
       " '55\\n',\n",
       " '127\\n',\n",
       " '86\\n',\n",
       " '92\\n',\n",
       " '19\\n',\n",
       " '58\\n',\n",
       " '92\\n',\n",
       " '98\\n',\n",
       " '173\\n',\n",
       " '13\\n',\n",
       " '57\\n',\n",
       " '73\\n',\n",
       " '116\\n',\n",
       " '138\\n',\n",
       " '19\\n',\n",
       " '36\\n',\n",
       " '71\\n',\n",
       " '74\\n',\n",
       " '91\\n',\n",
       " '19\\n',\n",
       " '10\\n',\n",
       " '40\\n',\n",
       " '73\\n',\n",
       " '74\\n',\n",
       " '23\\n',\n",
       " '36\\n',\n",
       " '74\\n',\n",
       " '83\\n',\n",
       " '24\\n',\n",
       " '9\\n',\n",
       " '36\\n',\n",
       " '23\\n',\n",
       " '180\\n',\n",
       " '9\\n',\n",
       " '42\\n',\n",
       " '83\\n',\n",
       " '180\\n',\n",
       " '121\\n',\n",
       " '86\\n',\n",
       " '204\\n',\n",
       " '2\\n',\n",
       " '112\\n',\n",
       " '156\\n',\n",
       " '203\\n',\n",
       " '8\\n',\n",
       " '18\\n',\n",
       " '43\\n',\n",
       " '141\\n',\n",
       " '23\\n',\n",
       " '50\\n',\n",
       " '57\\n',\n",
       " '57\\n',\n",
       " '19\\n',\n",
       " '52\\n',\n",
       " '56\\n',\n",
       " '5\\n',\n",
       " '23\\n',\n",
       " '52\\n',\n",
       " '101\\n',\n",
       " '12\\n',\n",
       " '30\\n',\n",
       " '30\\n',\n",
       " '90\\n',\n",
       " '109\\n',\n",
       " '21\\n',\n",
       " '94\\n',\n",
       " '148\\n',\n",
       " '12\\n',\n",
       " '71\\n',\n",
       " '80\\n',\n",
       " '94\\n',\n",
       " '148\\n',\n",
       " '7\\n',\n",
       " '46\\n',\n",
       " '153\\n',\n",
       " '263\\n',\n",
       " '312\\n',\n",
       " '14\\n',\n",
       " '290\\n',\n",
       " '263\\n',\n",
       " '312\\n',\n",
       " '2\\n',\n",
       " '7\\n',\n",
       " '29\\n',\n",
       " '103\\n',\n",
       " '271\\n',\n",
       " '11\\n',\n",
       " '67\\n',\n",
       " '88\\n',\n",
       " '106\\n',\n",
       " '173\\n',\n",
       " '67\\n',\n",
       " '107\\n',\n",
       " '110\\n',\n",
       " '146\\n',\n",
       " '11\\n',\n",
       " '67\\n',\n",
       " '90\\n',\n",
       " '107\\n',\n",
       " '9\\n',\n",
       " '45\\n',\n",
       " '65\\n',\n",
       " '83\\n',\n",
       " '93\\n',\n",
       " '9\\n',\n",
       " '69\\n',\n",
       " '65\\n',\n",
       " '93\\n',\n",
       " '45\\n',\n",
       " '9\\n',\n",
       " '45\\n',\n",
       " '65\\n',\n",
       " '93\\n',\n",
       " '11\\n',\n",
       " '11\\n',\n",
       " '50\\n',\n",
       " '11\\n',\n",
       " '13\\n',\n",
       " '25\\n',\n",
       " '41\\n',\n",
       " '3\\n',\n",
       " '13\\n',\n",
       " '30\\n',\n",
       " '41\\n',\n",
       " '2\\n',\n",
       " '61\\n',\n",
       " '82\\n',\n",
       " '51\\n',\n",
       " '100\\n',\n",
       " '3\\n',\n",
       " '2\\n',\n",
       " '51\\n',\n",
       " '84\\n',\n",
       " '23\\n',\n",
       " '6\\n',\n",
       " '61\\n',\n",
       " '79\\n',\n",
       " '11\\n',\n",
       " '68\\n',\n",
       " '111\\n',\n",
       " '129\\n',\n",
       " '9\\n',\n",
       " '17\\n',\n",
       " '115\\n',\n",
       " '11\\n',\n",
       " '5\\n',\n",
       " '9\\n',\n",
       " '17\\n',\n",
       " '101\\n',\n",
       " '129\\n',\n",
       " '2\\n',\n",
       " '18\\n',\n",
       " '14\\n',\n",
       " '33\\n',\n",
       " '2\\n',\n",
       " '14\\n',\n",
       " '24\\n',\n",
       " '30\\n',\n",
       " '2\\n',\n",
       " '14\\n',\n",
       " '18\\n',\n",
       " '33\\n',\n",
       " '30\\n',\n",
       " '14\\n',\n",
       " '104\\n',\n",
       " '182\\n',\n",
       " '2\\n',\n",
       " '26\\n',\n",
       " '39\\n',\n",
       " '79\\n',\n",
       " '2\\n',\n",
       " '26\\n',\n",
       " '28\\n',\n",
       " '53\\n',\n",
       " '104\\n',\n",
       " '19\\n',\n",
       " '4\\n',\n",
       " '103\\n',\n",
       " '85\\n',\n",
       " '11\\n',\n",
       " '72\\n',\n",
       " '77\\n',\n",
       " '103\\n',\n",
       " '4\\n',\n",
       " '19\\n',\n",
       " '53\\n',\n",
       " '73\\n',\n",
       " '77\\n',\n",
       " '14\\n',\n",
       " '2\\n',\n",
       " '14\\n',\n",
       " '123\\n',\n",
       " '213\\n',\n",
       " '235\\n',\n",
       " '2\\n',\n",
       " '70\\n',\n",
       " '120\\n',\n",
       " '152\\n',\n",
       " '18\\n',\n",
       " '51\\n',\n",
       " '66\\n',\n",
       " '129\\n',\n",
       " '18\\n',\n",
       " '51\\n',\n",
       " '129\\n',\n",
       " '14\\n',\n",
       " '66\\n',\n",
       " '129\\n',\n",
       " '129\\n',\n",
       " '14\\n',\n",
       " '18\\n',\n",
       " '51\\n',\n",
       " '66\\n',\n",
       " '129\\n',\n",
       " '24\\n',\n",
       " '27\\n',\n",
       " '2\\n',\n",
       " '19\\n',\n",
       " '24\\n',\n",
       " '21\\n',\n",
       " '2\\n',\n",
       " '24\\n',\n",
       " '79\\n',\n",
       " '144\\n',\n",
       " '105\\n',\n",
       " '11\\n',\n",
       " '58\\n',\n",
       " '79\\n',\n",
       " '126\\n',\n",
       " '144\\n',\n",
       " '18\\n',\n",
       " '58\\n',\n",
       " '79\\n',\n",
       " '105\\n',\n",
       " '160\\n',\n",
       " '8\\n',\n",
       " '64\\n',\n",
       " '92\\n',\n",
       " '7\\n',\n",
       " '74\\n',\n",
       " '105\\n',\n",
       " '122\\n',\n",
       " '17\\n',\n",
       " '7\\n',\n",
       " '17\\n',\n",
       " '92\\n',\n",
       " '122\\n',\n",
       " '40\\n',\n",
       " '31\\n',\n",
       " '11\\n',\n",
       " '15\\n',\n",
       " '31\\n',\n",
       " '11\\n',\n",
       " '77\\n',\n",
       " '2\\n',\n",
       " '11\\n',\n",
       " '31\\n',\n",
       " '59\\n',\n",
       " '77\\n',\n",
       " '144\\n',\n",
       " '20\\n',\n",
       " '66\\n',\n",
       " '113\\n',\n",
       " '136\\n',\n",
       " '20\\n',\n",
       " '20\\n',\n",
       " '31\\n',\n",
       " '75\\n',\n",
       " '113\\n',\n",
       " '6\\n',\n",
       " '88\\n',\n",
       " '6\\n',\n",
       " '8\\n',\n",
       " '88\\n',\n",
       " '78\\n',\n",
       " '99\\n',\n",
       " '64\\n',\n",
       " '88\\n',\n",
       " '9\\n',\n",
       " '67\\n',\n",
       " '32\\n',\n",
       " '57\\n',\n",
       " '2\\n',\n",
       " '22\\n',\n",
       " '38\\n',\n",
       " '67\\n',\n",
       " '9\\n',\n",
       " '22\\n',\n",
       " '37\\n',\n",
       " '32\\n',\n",
       " '5\\n',\n",
       " '22\\n',\n",
       " '5\\n',\n",
       " '30\\n',\n",
       " '41\\n',\n",
       " '174\\n',\n",
       " '248\\n',\n",
       " '326\\n',\n",
       " '41\\n',\n",
       " '1\\n',\n",
       " '188\\n',\n",
       " '283\\n',\n",
       " '326\\n',\n",
       " '1\\n',\n",
       " '264\\n',\n",
       " '241\\n',\n",
       " '205\\n',\n",
       " '5\\n',\n",
       " '7\\n',\n",
       " '67\\n',\n",
       " '85\\n',\n",
       " '5\\n',\n",
       " '74\\n',\n",
       " '115\\n',\n",
       " '141\\n',\n",
       " '5\\n',\n",
       " '54\\n',\n",
       " '101\\n',\n",
       " '114\\n',\n",
       " '5\\n',\n",
       " '10\\n',\n",
       " '49\\n',\n",
       " '64\\n',\n",
       " '78\\n',\n",
       " '5\\n',\n",
       " '44\\n',\n",
       " '49\\n',\n",
       " '57\\n',\n",
       " '5\\n',\n",
       " '49\\n',\n",
       " '78\\n',\n",
       " '66\\n',\n",
       " '82\\n',\n",
       " '53\\n',\n",
       " '26\\n',\n",
       " '48\\n',\n",
       " '59\\n',\n",
       " '89\\n',\n",
       " '31\\n',\n",
       " '59\\n',\n",
       " '89\\n',\n",
       " '34\\n',\n",
       " '24\\n',\n",
       " '74\\n',\n",
       " '112\\n',\n",
       " '2\\n',\n",
       " '37\\n',\n",
       " '99\\n',\n",
       " '37\\n',\n",
       " '16\\n',\n",
       " '23\\n',\n",
       " '44\\n',\n",
       " '105\\n',\n",
       " '3\\n',\n",
       " '11\\n',\n",
       " '3\\n',\n",
       " '100\\n',\n",
       " '3\\n',\n",
       " '92\\n',\n",
       " '3\\n',\n",
       " '3\\n',\n",
       " '60\\n',\n",
       " '67\\n',\n",
       " '92\\n',\n",
       " '2\\n',\n",
       " '23\\n',\n",
       " '65\\n',\n",
       " '19\\n',\n",
       " '69\\n',\n",
       " '36\\n',\n",
       " '23\\n',\n",
       " '33\\n',\n",
       " '13\\n',\n",
       " '106\\n',\n",
       " '27\\n',\n",
       " '13\\n',\n",
       " '35\\n',\n",
       " '102\\n',\n",
       " '114\\n',\n",
       " '47\\n',\n",
       " '11\\n',\n",
       " '27\\n',\n",
       " '77\\n',\n",
       " '111\\n',\n",
       " '9\\n',\n",
       " '32\\n',\n",
       " '36\\n',\n",
       " '2\\n',\n",
       " '9\\n',\n",
       " '9\\n',\n",
       " '51\\n',\n",
       " '9\\n',\n",
       " '9\\n',\n",
       " '18\\n',\n",
       " '32\\n',\n",
       " '50\\n',\n",
       " '7\\n',\n",
       " '75\\n',\n",
       " '7\\n',\n",
       " '14\\n',\n",
       " '82\\n',\n",
       " '66\\n',\n",
       " '14\\n',\n",
       " '75\\n',\n",
       " '23\\n',\n",
       " '64\\n',\n",
       " '86\\n',\n",
       " '122\\n',\n",
       " '41\\n',\n",
       " '115\\n',\n",
       " '82\\n',\n",
       " '78\\n',\n",
       " '67\\n',\n",
       " '93\\n',\n",
       " '9\\n',\n",
       " '15\\n',\n",
       " '68\\n',\n",
       " '83\\n',\n",
       " '68\\n',\n",
       " '68\\n',\n",
       " '91\\n',\n",
       " '15\\n',\n",
       " '68\\n',\n",
       " '91\\n',\n",
       " '41\\n',\n",
       " '8\\n',\n",
       " '11\\n',\n",
       " '25\\n',\n",
       " '39\\n',\n",
       " '59\\n',\n",
       " '25\\n',\n",
       " '35\\n',\n",
       " '40\\n',\n",
       " '63\\n',\n",
       " '3\\n',\n",
       " '25\\n',\n",
       " '29\\n',\n",
       " '79\\n',\n",
       " '20\\n',\n",
       " '81\\n',\n",
       " '29\\n',\n",
       " '61\\n',\n",
       " '81\\n',\n",
       " '81\\n',\n",
       " '84\\n',\n",
       " '1\\n',\n",
       " '17\\n',\n",
       " '53\\n',\n",
       " '2\\n',\n",
       " '17\\n',\n",
       " '2\\n',\n",
       " '17\\n',\n",
       " '32\\n',\n",
       " '26\\n',\n",
       " '40\\n',\n",
       " '42\\n',\n",
       " '2\\n',\n",
       " '17\\n',\n",
       " '40\\n',\n",
       " '41\\n',\n",
       " '40\\n',\n",
       " '26\\n',\n",
       " '37\\n',\n",
       " '14\\n',\n",
       " '16\\n',\n",
       " '54\\n',\n",
       " '97\\n',\n",
       " '70\\n',\n",
       " '6\\n',\n",
       " '16\\n',\n",
       " '54\\n',\n",
       " '63\\n',\n",
       " '79\\n',\n",
       " '35\\n',\n",
       " '119\\n',\n",
       " '188\\n',\n",
       " '39\\n',\n",
       " '119\\n',\n",
       " '188\\n',\n",
       " '126\\n',\n",
       " '7\\n',\n",
       " '35\\n',\n",
       " '123\\n',\n",
       " '126\\n',\n",
       " '188\\n',\n",
       " '41\\n',\n",
       " '56\\n',\n",
       " '84\\n',\n",
       " '83\\n',\n",
       " '41\\n',\n",
       " '2\\n',\n",
       " '75\\n',\n",
       " '41\\n',\n",
       " '52\\n',\n",
       " '56\\n',\n",
       " '75\\n',\n",
       " '77\\n',\n",
       " '5\\n',\n",
       " '43\\n',\n",
       " '58\\n',\n",
       " '164\\n',\n",
       " '183\\n',\n",
       " '5\\n',\n",
       " '42\\n',\n",
       " '57\\n',\n",
       " '126\\n',\n",
       " '29\\n',\n",
       " '126\\n',\n",
       " '160\\n',\n",
       " '14\\n',\n",
       " '22\\n',\n",
       " '66\\n',\n",
       " '28\\n',\n",
       " '1\\n',\n",
       " '14\\n',\n",
       " '22\\n',\n",
       " '28\\n',\n",
       " '184\\n',\n",
       " '66\\n',\n",
       " '13\\n',\n",
       " '21\\n',\n",
       " '45\\n',\n",
       " '66\\n',\n",
       " '183\\n',\n",
       " '4\\n',\n",
       " '29\\n',\n",
       " '41\\n',\n",
       " '109\\n',\n",
       " '169\\n',\n",
       " '4\\n',\n",
       " '41\\n',\n",
       " '56\\n',\n",
       " '128\\n',\n",
       " '4\\n',\n",
       " '41\\n',\n",
       " '43\\n',\n",
       " '137\\n',\n",
       " '5\\n',\n",
       " '29\\n',\n",
       " '34\\n',\n",
       " '87\\n',\n",
       " '7\\n',\n",
       " '17\\n',\n",
       " '32\\n',\n",
       " '35\\n',\n",
       " '5\\n',\n",
       " '28\\n',\n",
       " '34\\n',\n",
       " '71\\n',\n",
       " '5\\n',\n",
       " '1\\n',\n",
       " '40\\n',\n",
       " '96\\n',\n",
       " '125\\n',\n",
       " '40\\n',\n",
       " '78\\n',\n",
       " '89\\n',\n",
       " '109\\n',\n",
       " '30\\n',\n",
       " '42\\n',\n",
       " '40\\n',\n",
       " '109\\n',\n",
       " '78\\n',\n",
       " '6\\n',\n",
       " '74\\n',\n",
       " '28\\n',\n",
       " '40\\n',\n",
       " '6\\n',\n",
       " '6\\n',\n",
       " '19\\n',\n",
       " '28\\n',\n",
       " '47\\n',\n",
       " '20\\n',\n",
       " '31\\n',\n",
       " '46\\n',\n",
       " '111\\n',\n",
       " '22\\n',\n",
       " '113\\n',\n",
       " '13\\n',\n",
       " '22\\n',\n",
       " '19\\n',\n",
       " '46\\n',\n",
       " '2\\n',\n",
       " '37\\n',\n",
       " '2\\n',\n",
       " '20\\n',\n",
       " '124\\n',\n",
       " '23\\n',\n",
       " '45\\n',\n",
       " '37\\n',\n",
       " '117\\n',\n",
       " '3\\n',\n",
       " '35\\n',\n",
       " '65\\n',\n",
       " '103\\n',\n",
       " '126\\n',\n",
       " '3\\n',\n",
       " '15\\n',\n",
       " '34\\n',\n",
       " '89\\n',\n",
       " '3\\n',\n",
       " '82\\n',\n",
       " '11\\n',\n",
       " '117\\n',\n",
       " '49\\n',\n",
       " '43\\n',\n",
       " '60\\n',\n",
       " '98\\n',\n",
       " '7\\n',\n",
       " '29\\n',\n",
       " '60\\n',\n",
       " '107\\n',\n",
       " '87\\n',\n",
       " '19\\n',\n",
       " '42\\n',\n",
       " '25\\n',\n",
       " '22\\n',\n",
       " '49\\n',\n",
       " '19\\n',\n",
       " '25\\n',\n",
       " '11\\n',\n",
       " '2\\n',\n",
       " '30\\n',\n",
       " '49\\n',\n",
       " '10\\n',\n",
       " '29\\n',\n",
       " '34\\n',\n",
       " '11\\n',\n",
       " '29\\n',\n",
       " '48\\n',\n",
       " '4\\n',\n",
       " '61\\n',\n",
       " '127\\n',\n",
       " '43\\n',\n",
       " '56\\n',\n",
       " '140\\n',\n",
       " '9\\n',\n",
       " '4\\n',\n",
       " '56\\n',\n",
       " '127\\n',\n",
       " '48\\n',\n",
       " '68\\n',\n",
       " '263\\n',\n",
       " '134\\n',\n",
       " '218\\n',\n",
       " '48\\n',\n",
       " '106\\n',\n",
       " '240\\n',\n",
       " '20\\n',\n",
       " '48\\n',\n",
       " '68\\n',\n",
       " '91\\n',\n",
       " '27\\n',\n",
       " '43\\n',\n",
       " '50\\n',\n",
       " '92\\n',\n",
       " '130\\n',\n",
       " '15\\n',\n",
       " '43\\n',\n",
       " '94\\n',\n",
       " '50\\n',\n",
       " '130\\n',\n",
       " '17\\n',\n",
       " '21\\n',\n",
       " '50\\n',\n",
       " '43\\n',\n",
       " '15\\n',\n",
       " '94\\n',\n",
       " '130\\n',\n",
       " '43\\n',\n",
       " '19\\n',\n",
       " '5\\n',\n",
       " '23\\n',\n",
       " '71\\n',\n",
       " '105\\n',\n",
       " '160\\n',\n",
       " '9\\n',\n",
       " '23\\n',\n",
       " '56\\n',\n",
       " '71\\n',\n",
       " '81\\n',\n",
       " '5\\n',\n",
       " '23\\n',\n",
       " '71\\n',\n",
       " '130\\n',\n",
       " '71\\n',\n",
       " '130\\n',\n",
       " '160\\n',\n",
       " '7\\n',\n",
       " '55\\n',\n",
       " '92\\n',\n",
       " '47\\n',\n",
       " '110\\n',\n",
       " '7\\n',\n",
       " '55\\n',\n",
       " '47\\n',\n",
       " '92\\n',\n",
       " '28\\n',\n",
       " '7\\n',\n",
       " '28\\n',\n",
       " '47\\n',\n",
       " '92\\n',\n",
       " '7\\n',\n",
       " '28\\n',\n",
       " '110\\n',\n",
       " '30\\n',\n",
       " '34\\n',\n",
       " '53\\n',\n",
       " '67\\n',\n",
       " '6\\n',\n",
       " '4\\n',\n",
       " '32\\n',\n",
       " '53\\n',\n",
       " '6\\n',\n",
       " '32\\n',\n",
       " '53\\n',\n",
       " '6\\n',\n",
       " '37\\n',\n",
       " '66\\n',\n",
       " '49\\n',\n",
       " '11\\n',\n",
       " '6\\n',\n",
       " '28\\n',\n",
       " '37\\n',\n",
       " '66\\n',\n",
       " '49\\n',\n",
       " '28\\n',\n",
       " '37\\n',\n",
       " '66\\n',\n",
       " '49\\n',\n",
       " '6\\n',\n",
       " '28\\n',\n",
       " '33\\n',\n",
       " '55\\n',\n",
       " '27\\n',\n",
       " '3\\n',\n",
       " '9\\n',\n",
       " '33\\n',\n",
       " '78\\n',\n",
       " '99\\n',\n",
       " '3\\n',\n",
       " '9\\n',\n",
       " '99\\n',\n",
       " '2\\n',\n",
       " '21\\n',\n",
       " '38\\n',\n",
       " '45\\n',\n",
       " '79\\n',\n",
       " '2\\n',\n",
       " '24\\n',\n",
       " '45\\n',\n",
       " '52\\n',\n",
       " '4\\n',\n",
       " '21\\n',\n",
       " '45\\n',\n",
       " '52\\n',\n",
       " '30\\n',\n",
       " '25\\n',\n",
       " '30\\n',\n",
       " '67\\n',\n",
       " '69\\n',\n",
       " '80\\n',\n",
       " '25\\n',\n",
       " '30\\n',\n",
       " '67\\n',\n",
       " '66\\n',\n",
       " '18\\n',\n",
       " '25\\n',\n",
       " '30\\n',\n",
       " '67\\n',\n",
       " '25\\n',\n",
       " '2\\n',\n",
       " '29\\n',\n",
       " '23\\n',\n",
       " '56\\n',\n",
       " '93\\n',\n",
       " '2\\n',\n",
       " '23\\n',\n",
       " '69\\n',\n",
       " '81\\n',\n",
       " '85\\n',\n",
       " '56\\n',\n",
       " '69\\n',\n",
       " '81\\n',\n",
       " '85\\n',\n",
       " '3\\n',\n",
       " '40\\n',\n",
       " '83\\n',\n",
       " '121\\n',\n",
       " '141\\n',\n",
       " '20\\n',\n",
       " '40\\n',\n",
       " '83\\n',\n",
       " '133\\n',\n",
       " '147\\n',\n",
       " '20\\n',\n",
       " '40\\n',\n",
       " '83\\n',\n",
       " '132\\n',\n",
       " '147\\n',\n",
       " '27\\n',\n",
       " '21\\n',\n",
       " '68\\n',\n",
       " '75\\n',\n",
       " '27\\n',\n",
       " '41\\n',\n",
       " '27\\n',\n",
       " '57\\n',\n",
       " '21\\n',\n",
       " '2\\n',\n",
       " '43\\n',\n",
       " '54\\n',\n",
       " '74\\n',\n",
       " '84\\n',\n",
       " '2\\n',\n",
       " '54\\n',\n",
       " '62\\n',\n",
       " '84\\n',\n",
       " '10\\n',\n",
       " '54\\n',\n",
       " '68\\n',\n",
       " '84\\n',\n",
       " '65\\n',\n",
       " '15\\n',\n",
       " '58\\n',\n",
       " '10\\n',\n",
       " '58\\n',\n",
       " '97\\n',\n",
       " '126\\n',\n",
       " '15\\n',\n",
       " '58\\n',\n",
       " '10\\n',\n",
       " '97\\n',\n",
       " '1\\n',\n",
       " '16\\n',\n",
       " '22\\n',\n",
       " '48\\n',\n",
       " '75\\n',\n",
       " '2\\n',\n",
       " '16\\n',\n",
       " '22\\n',\n",
       " '48\\n',\n",
       " '75\\n',\n",
       " '3\\n',\n",
       " '16\\n',\n",
       " '15\\n',\n",
       " '22\\n',\n",
       " '38\\n',\n",
       " '19\\n',\n",
       " '47\\n',\n",
       " '66\\n',\n",
       " '147\\n',\n",
       " '19\\n',\n",
       " '63\\n',\n",
       " '137\\n',\n",
       " '131\\n',\n",
       " '89\\n',\n",
       " '8\\n',\n",
       " '63\\n',\n",
       " '66\\n',\n",
       " '48\\n',\n",
       " '18\\n',\n",
       " '46\\n',\n",
       " '60\\n',\n",
       " '28\\n',\n",
       " '28\\n',\n",
       " '42\\n',\n",
       " '52\\n',\n",
       " '58\\n',\n",
       " '12\\n',\n",
       " '42\\n',\n",
       " '20\\n',\n",
       " '86\\n',\n",
       " '96\\n',\n",
       " '22\\n',\n",
       " '31\\n',\n",
       " '43\\n',\n",
       " '61\\n',\n",
       " '23\\n',\n",
       " '7\\n',\n",
       " '23\\n',\n",
       " '43\\n',\n",
       " '74\\n',\n",
       " '14\\n',\n",
       " '27\\n',\n",
       " '31\\n',\n",
       " '74\\n',\n",
       " '23\\n',\n",
       " '2\\n',\n",
       " '56\\n',\n",
       " '71\\n',\n",
       " '56\\n',\n",
       " '35\\n',\n",
       " '56\\n',\n",
       " '58\\n',\n",
       " '23\\n',\n",
       " '34\\n',\n",
       " '59\\n',\n",
       " '23\\n',\n",
       " ...]"
      ]
     },
     "execution_count": 554,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ontext_path_train = os.path.join(data_dir,\"train.answer_start\")\n",
    "    \n",
    "answer_tokens = open(ontext_path_train, \"r\", encoding=\"utf-8\").readlines()\n",
    "answer_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FINAL TO_DOs\n",
    "1)keep a cap on vocabulary(50k), initialize other words randomly\n",
    "2)integrate the code to pickle vocab and answer/context/question tokens/indices in the end-to-end model\n",
    "3) break train file into train and validation, and use validation file as test file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
