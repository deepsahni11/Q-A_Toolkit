{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset iterator file creates batches of training data and yields a batch: (\"yields\" returns a generator element where \"generators\" are iterables which can loop over its elements only once and then they are destroyed from memory.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "\n",
    "import torch\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "import os\n",
    "import pickle\n",
    "import tqdm as tqdm\n",
    "\n",
    "import numpy as np\n",
    "from six.moves import xrange\n",
    "# from data_util.vocab import PAD_ID, UNK_ID\n",
    "\n",
    "\"\"\"This file contains code to read tokenized data from file,\n",
    "truncate, pad and process it into batches ready for training\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Batch(object):\n",
    "    \"\"\"A class to hold the information needed for a training batch\"\"\"\n",
    "# Batch(names,context_word_index_padded, context_word_mask, question_word_index_padded, question_word_mask, answer_start, answer_end)\n",
    "    def __init__(self,names, context_word_index_padded, context_word_mask, question_word_index_padded, question_word_mask, answer_start, answer_end):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "          {context/qn}_ids: Numpy arrays.\n",
    "            Shape (batch_size, {context_len/question_len}). Contains padding.\n",
    "          {context/qn}_mask: Numpy arrays, same shape as _ids.\n",
    "            Contains 1s where there is real data, 0s where there is padding.\n",
    "          {context/qn/ans}_tokens: Lists length batch_size, containing lists (unpadded) of tokens (strings)\n",
    "          ans_span: numpy array, shape (batch_size, 2)\n",
    "          uuid: a list (length batch_size) of strings.\n",
    "            Not needed for training. Used by official_eval mode.\n",
    "        \"\"\"\n",
    "        self.names = names\n",
    "        self.context_word_index_padded = context_word_index_padded\n",
    "        self.context_word_mask = context_word_mask\n",
    "#         self.context_tokens = context_tokens\n",
    "\n",
    "        self.question_word_index_padded = question_word_index_padded\n",
    "        self.question_word_mask = question_word_mask\n",
    "#         self.qn_tokens = qn_tokens\n",
    "\n",
    "        self.answer_start = answer_start\n",
    "        self.answer_end = answer_end\n",
    "\n",
    "#         self.uuids = uuids\n",
    "\n",
    "        self.batch_size = len(self.context_word_index_padded)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def refill_batches(batches,batch_size,names, max_context_length, max_question_length,context_word_index_padded,question_word_index_padded, answer_start, answer_end ):\n",
    "#     refill_batches(batches, word_to_index, context_file, qn_file, ans_file, batch_size, context_len, question_len, discard_long):\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    Adds more batches into the \"batches\" list.\n",
    "    Inputs:\n",
    "      batches: list to add batches to\n",
    "     \n",
    "      names: list containing strings of file names [\"train_context\",\"train_question\"] or [\"validation_context\",\"validation_question\"]\n",
    "      data_dir : paths to {train/dev}.{context/question/answer} data files\n",
    "      batch_size: integer ==> how big to make the batches\n",
    "      max_context_length, max_question_length: max length of context and question respectively\n",
    "      \n",
    "    \"\"\"\n",
    "    print (\"Refilling batches...\")\n",
    "    tic = time.time()\n",
    "    examples = [] # list of (qn_ids, context_ids, ans_span, ans_tokens) triples\n",
    "    \n",
    "   \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "#     context_line, qn_line, ans_line = context_file.readline(), qn_file.readline(), ans_file.readline() # read the next line from each\n",
    "\n",
    "    while True: # while you haven't reached the end\n",
    "        \n",
    "        # add to examples\n",
    "        examples.append((context_word_index_padded, question_word_index_padded, answer_start, answer_end))\n",
    "\n",
    "        # stop refilling if you have 1 batch : change it later \n",
    "        ################## add number of batches you need ###########################33\n",
    "        if len(examples) == batch_size * 1 :\n",
    "            break\n",
    "\n",
    "\n",
    "\n",
    "    # Make into batches and append to the list batches\n",
    "    for batch_start in xrange(0, len(examples), batch_size):\n",
    "\n",
    "        # Note: each of these is a list length batch_size of lists of ints (except on last iter when it might be less than batch_size)\n",
    "        context_word_index_padded, question_word_index_padded, answer_start, answer_end = zip(*examples[batch_start:batch_start+batch_size])\n",
    "\n",
    "        batches.append((context_word_index_padded, question_word_index_padded, answer_start, answer_end))\n",
    "\n",
    "    # shuffle the batches\n",
    "    random.shuffle(batches)\n",
    "\n",
    "    toc = time.time()\n",
    "    print (\"Refilling batches took %.2f seconds\" % (toc-tic))\n",
    "    return\n",
    "\n",
    "\n",
    "def get_batch_generator(data_dir, names, batch_size, max_context_length, max_question_length,context_word_index_padded,question_word_index_padded,answer_start, answer_end):\n",
    "    \"\"\"\n",
    "    This function returns a generator object that yields batches.\n",
    "    The last batch in the dataset will be a partial batch.\n",
    "    Read this to understand generators and the yield keyword in Python: https://stackoverflow.com/questions/231767/what-does-the-yield-keyword-do\n",
    "    Inputs:\n",
    "      names: list containing strings of file names = [\"train_context\",\"train_question\"] or [\"validation_context\",\"validation_question\"]\n",
    "      data_dir : paths to {train/dev}.{context/question/answer} data files\n",
    "      batch_size: integer ==> how big to make the batches\n",
    "      max_context_length, max_question_length: max length of context and question respectively\n",
    "     \n",
    "    \"\"\"\n",
    "    \n",
    "#     with open(r\"E:\\\\Internships_19\\\\Internship(Summer_19)\\\\Q&A_Toolkit\\\\Dataset_analysis\\\\SQuAD\\\\glove_word_embeddings.pkl\", \"rb\") as input_file:\n",
    "#         emb_matrix = pickle.load(input_file)\n",
    "#     with open(data_dir + \"//\" + \"answer_end_pkl.pkl\", \"rb\") as input_file:\n",
    "#         answer_end_pkl = pickle.load(input_file)\n",
    "#     with open(data_dir + \"//\" + \"answer_start_pkl.pkl\", \"rb\") as input_file:\n",
    "#         answer_start_pkl = pickle.load(input_file)\n",
    "\n",
    "#     answer_end = torch.from_numpy(np.array([int(i) for i in answer_end_pkl])).int()\n",
    "#     answer_start = torch.from_numpy(np.array([int(i) for i in answer_start_pkl])).int()              \n",
    "   \n",
    "    \n",
    "    batches = []\n",
    "\n",
    "    while True:\n",
    "        if len(batches) == 0: # add more batches\n",
    "            refill_batches(batches,batch_size,names, max_context_length, max_question_length,context_word_index_padded,question_word_index_padded,answer_start, answer_end)\n",
    "        if len(batches) == 0:\n",
    "            break\n",
    "\n",
    "        # Get next batch. These are all lists length batch_size\n",
    "        (context_word_index_padded, question_word_index_padded, answer_start, answer_end) = batches.pop(0)\n",
    "\n",
    "#         # Pad context_ids and qn_ids\n",
    "#         qn_ids = padded(qn_ids, question_len) # pad questions to length question_len\n",
    "#         context_ids = padded(context_ids, context_len) # pad contexts to length context_len\n",
    "\n",
    "        # Make qn_ids into a np array and create qn_mask\n",
    "#         qn_ids = np.array(qn_ids) # shape (batch_size, question_len)\n",
    "#         qn_mask = (qn_ids != PAD_ID).astype(np.int32) # shape (batch_size, question_len)\n",
    "\n",
    "#         # Make context_ids into a np array and create context_mask\n",
    "#         context_ids = np.array(context_ids) # shape (batch_size, context_len)\n",
    "#         context_mask = (context_ids != PAD_ID).astype(np.int32) # shape (batch_size, context_len)\n",
    "\n",
    "\n",
    "        print(context_word_index_padded[0])\n",
    "        print((context_word_index_padded[0] != 0).type(torch.int32) )\n",
    "        context_word_mask = int(context_word_index_padded != 0)\n",
    "#     .int()\n",
    "#     .astype(torch.int32) \n",
    "        question_word_mask = int(question_word_index_padded != 0)\n",
    "#     .int()\n",
    "#     astype(torch.int32)\n",
    "        # Make ans_span into a np array\n",
    "#         ans_span = np.array(ans_span) # shape (batch_size, 2)\n",
    "\n",
    "        # Make into a Batch object\n",
    "        batch = Batch(names,context_word_index_padded, context_word_mask, question_word_index_padded, question_word_mask, answer_start, answer_end)\n",
    "\n",
    "        yield batch\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "answers = [\"train.answer_start\", \"train.answer_end\"]\n",
    "answers_indexes =[os.path.join(data_dir + answer )  for answer in answers ]\n",
    "with open(answers_indexes[0], \"r\" ,encoding=\"utf-8\") as input_file:\n",
    "    answer_start = input_file\n",
    "    lines  = []\n",
    "    for line in answer_start:\n",
    "        lines.append(line)\n",
    "    write_path_train_word = os.path.join(data_dir, \"answer_start\" + \"_pkl.pkl\")\n",
    "    write_file_train_word = open(write_path_train_word, \"wb\")\n",
    "    pickle.dump(lines, write_file_train_word)\n",
    "with open(answers_indexes[1], \"r\" ,encoding=\"utf-8\") as input_file:\n",
    "    answer_start = input_file\n",
    "    lines  = []\n",
    "    for line in answer_start:\n",
    "        lines.append(line)\n",
    "    write_path_train_word = os.path.join(data_dir, \"answer_end\" + \"_pkl.pkl\")\n",
    "    write_file_train_word = open(write_path_train_word, \"wb\")\n",
    "    pickle.dump(lines, write_file_train_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index_padded =[os.path.join(data_dir + name + \"_word_index_padded.pkl\")  for name in names ]\n",
    "with open(word_index_padded[0], \"rb\") as input_file:\n",
    "    context_word_index_padded = pickle.load(input_file)\n",
    "with open(word_index_padded[1], \"rb\") as input_file:\n",
    "    question_word_index_padded = pickle.load(input_file)\n",
    "\n",
    "with open(data_dir + \"//\" + \"answer_end_pkl.pkl\", \"rb\") as input_file:\n",
    "    answer_end_pkl = pickle.load(input_file)\n",
    "with open(data_dir + \"//\" + \"answer_start_pkl.pkl\", \"rb\") as input_file:\n",
    "    answer_start_pkl = pickle.load(input_file)\n",
    "                \n",
    "answer_end = torch.from_numpy(np.array([int(i) for i in answer_end_pkl])).int()\n",
    "answer_start = torch.from_numpy(np.array([int(i) for i in answer_start_pkl])).int()              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([130319])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_end.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refilling batches...\n",
      "Refilling batches took 0.00 seconds\n",
      "tensor([[  702, 26517, 38840,  ...,     0,     0,     0],\n",
      "        [  702, 26517, 38840,  ...,     0,     0,     0],\n",
      "        [  702, 26517, 38840,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [    2,   213,    23,  ...,     0,     0,     0],\n",
      "        [    2,   213,    23,  ...,     0,     0,     0],\n",
      "        [    2,   213,    23,  ...,     0,     0,     0]])\n",
      "tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], dtype=torch.int32)\n",
      "7\n",
      "Refilling batches...\n",
      "Refilling batches took 0.00 seconds\n",
      "(tensor([[  702, 26517, 38840,  ...,     0,     0,     0],\n",
      "        [  702, 26517, 38840,  ...,     0,     0,     0],\n",
      "        [  702, 26517, 38840,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [    2,   213,    23,  ...,     0,     0,     0],\n",
      "        [    2,   213,    23,  ...,     0,     0,     0],\n",
      "        [    2,   213,    23,  ...,     0,     0,     0]]), tensor([[  702, 26517, 38840,  ...,     0,     0,     0],\n",
      "        [  702, 26517, 38840,  ...,     0,     0,     0],\n",
      "        [  702, 26517, 38840,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [    2,   213,    23,  ...,     0,     0,     0],\n",
      "        [    2,   213,    23,  ...,     0,     0,     0],\n",
      "        [    2,   213,    23,  ...,     0,     0,     0]]), tensor([[  702, 26517, 38840,  ...,     0,     0,     0],\n",
      "        [  702, 26517, 38840,  ...,     0,     0,     0],\n",
      "        [  702, 26517, 38840,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [    2,   213,    23,  ...,     0,     0,     0],\n",
      "        [    2,   213,    23,  ...,     0,     0,     0],\n",
      "        [    2,   213,    23,  ...,     0,     0,     0]]), tensor([[  702, 26517, 38840,  ...,     0,     0,     0],\n",
      "        [  702, 26517, 38840,  ...,     0,     0,     0],\n",
      "        [  702, 26517, 38840,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [    2,   213,    23,  ...,     0,     0,     0],\n",
      "        [    2,   213,    23,  ...,     0,     0,     0],\n",
      "        [    2,   213,    23,  ...,     0,     0,     0]]), tensor([[  702, 26517, 38840,  ...,     0,     0,     0],\n",
      "        [  702, 26517, 38840,  ...,     0,     0,     0],\n",
      "        [  702, 26517, 38840,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [    2,   213,    23,  ...,     0,     0,     0],\n",
      "        [    2,   213,    23,  ...,     0,     0,     0],\n",
      "        [    2,   213,    23,  ...,     0,     0,     0]]), tensor([[  702, 26517, 38840,  ...,     0,     0,     0],\n",
      "        [  702, 26517, 38840,  ...,     0,     0,     0],\n",
      "        [  702, 26517, 38840,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [    2,   213,    23,  ...,     0,     0,     0],\n",
      "        [    2,   213,    23,  ...,     0,     0,     0],\n",
      "        [    2,   213,    23,  ...,     0,     0,     0]]), tensor([[  702, 26517, 38840,  ...,     0,     0,     0],\n",
      "        [  702, 26517, 38840,  ...,     0,     0,     0],\n",
      "        [  702, 26517, 38840,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [    2,   213,    23,  ...,     0,     0,     0],\n",
      "        [    2,   213,    23,  ...,     0,     0,     0],\n",
      "        [    2,   213,    23,  ...,     0,     0,     0]]))\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'bool' object has no attribute 'type'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-153-866349a82af7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-148-08859f3e83e6>\u001b[0m in \u001b[0;36mget_batch_generator\u001b[1;34m(data_dir, names, batch_size, max_context_length, max_question_length, context_word_index_padded, question_word_index_padded, answer_start, answer_end)\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontext_word_index_padded\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 160\u001b[1;33m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontext_word_index_padded\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    161\u001b[0m         \u001b[0mcontext_word_mask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontext_word_index_padded\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[1;31m#     .int()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'bool' object has no attribute 'type'"
     ]
    }
   ],
   "source": [
    "a = get_batch_generator(data_dir, names, batch_size, max_context_length, max_question_length,context_word_index_padded,question_word_index_padded,answer_start, answer_end)\n",
    "for i in a:\n",
    "    print(i.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3])\n",
      "torch.Size([3, 1])\n"
     ]
    }
   ],
   "source": [
    "x = torch.Tensor([1, 2, 3])\n",
    "r = torch.unsqueeze(x, 0)  \n",
    "print(r.size())# Size: 1x3\n",
    "r = torch.unsqueeze(x, 1)    \n",
    "print(r.size())# Size: 1x3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Tensor([1, 2, 3]).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_dir + \"//\" + \"answer_end_pkl.pkl\", \"rb\") as input_file:\n",
    "    answer_end_pkl = pickle.load(input_file)\n",
    "with open(data_dir + \"//\" + \"answer_start_pkl.pkl\", \"rb\") as input_file:\n",
    "    answer_start_pkl = pickle.load(input_file)\n",
    "                \n",
    "answer_end = torch.from_numpy(np.array([int(i) for i in answer_end_pkl])).int()\n",
    "answer_start = torch.from_numpy(np.array([int(i) for i in answer_start_pkl])).int()              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([130319, 1])\n"
     ]
    }
   ],
   "source": [
    "answer_end = torch.unsqueeze(answer_end, 1)    \n",
    "answer_start = torch.unsqueeze(answer_start, 1)\n",
    "print(answer_start.size())\n",
    "span = torch.cat((answer_start,answer_end),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([39, 28, 83,  ..., -1, -1, -1], dtype=torch.int32)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "span[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "\n",
    "import torch\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "import os\n",
    "import pickle\n",
    "import tqdm as tqdm\n",
    "\n",
    "import numpy as np\n",
    "from six.moves import xrange\n",
    "\n",
    "class Batch():\n",
    "    \"\"\"A class to hold the information needed for a training batch\"\"\"\n",
    "    def __init__(self,names,context_word_index_padded, context_word_mask, question_word_index_padded, question_word_mask, span_tensor):\n",
    "        \n",
    "        self.names = names\n",
    "        self.context_word_index_padded = context_word_index_padded\n",
    "        self.context_word_mask = context_word_mask\n",
    "\n",
    "\n",
    "        self.question_word_index_padded = question_word_index_padded\n",
    "        self.question_word_mask = question_word_mask\n",
    "        self.span_tensor = span_tensor\n",
    "        self.batch_size = len(self.context_word_index_padded)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def refill_batches(batches,batch_size,names, max_context_length, max_question_length,context_word_index_padded,question_word_index_padded,span_tensor):\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    Adds more batches into the \"batches\" list.\n",
    "    Inputs:\n",
    "      batches: list to add batches to\n",
    "\n",
    "      names: list containing strings of file names [\"train_context\",\"train_question\"] or [\"validation_context\",\"validation_question\"]\n",
    "      data_dir : paths to {train/dev}.{context/question/answer} data files\n",
    "      batch_size: integer ==> how big to make the batches\n",
    "      max_context_length, max_question_length: max length of context and question respectively\n",
    "\n",
    "    \"\"\"\n",
    "    print (\"Refilling batches...\")\n",
    "    tic = time.time()\n",
    "    examples = [] \n",
    "\n",
    "\n",
    "\n",
    "        # add to examples\n",
    "    examples.append((context_word_index_padded, question_word_index_padded, span_tensor))\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "    # Make into batches and append to the list batches\n",
    "    for batch_start in xrange(0, len(examples[0][0]), batch_size):\n",
    "\n",
    "        # Note: each of these is a list length batch_size of lists of ints (except on last iter when it might be less than batch_size)\n",
    "        context_word_index_padded = examples[0][0][batch_start:batch_start+batch_size]\n",
    "        question_word_index_padded = examples[0][1][batch_start:batch_start+batch_size]\n",
    "        span_tensor = examples[0][2][batch_start:batch_start+batch_size]\n",
    "\n",
    "        batches.append((context_word_index_padded, question_word_index_padded,span_tensor))\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    # shuffle the batches\n",
    "    random.shuffle(batches)\n",
    "\n",
    "    toc = time.time()\n",
    "    print (\"Refilling batches took %.2f seconds\" % (toc-tic))\n",
    "    return batches\n",
    "\n",
    "\n",
    "def get_batch_generator(data_dir, names, batch_size, max_context_length, max_question_length):\n",
    "    \"\"\"\n",
    "    This function returns a generator object that yields batches.\n",
    "    The last batch in the dataset will be a partial batch.\n",
    "    Read this to understand generators and the yield keyword in Python: https://stackoverflow.com/questions/231767/what-does-the-yield-keyword-do\n",
    "    Inputs:\n",
    "      names: list containing strings of file names = [\"train_context\",\"train_question\"] or [\"validation_context\",\"validation_question\"]\n",
    "      data_dir : paths to {train/dev}.{context/question/answer} data files\n",
    "      batch_size: integer ==> how big to make the batches\n",
    "      max_context_length, max_question_length: max length of context and question respectively\n",
    "\n",
    "    \"\"\"\n",
    "    word_index_padded =[os.path.join(data_dir + name + \"_word_index_padded.pkl\")  for name in names ]\n",
    "    with open(word_index_padded[0], \"rb\") as input_file:\n",
    "        context_word_index_padded = pickle.load(input_file)\n",
    "    with open(word_index_padded[1], \"rb\") as input_file:\n",
    "        question_word_index_padded = pickle.load(input_file)\n",
    "\n",
    "    with open(data_dir + \"//\" + \"answer_end_pkl.pkl\", \"rb\") as input_file:\n",
    "        answer_end_pkl = pickle.load(input_file)\n",
    "    with open(data_dir + \"//\" + \"answer_start_pkl.pkl\", \"rb\") as input_file:\n",
    "        answer_start_pkl = pickle.load(input_file)\n",
    "\n",
    "    context_word_index_padded = context_word_index_padded[10:33]\n",
    "    question_word_index_padded = question_word_index_padded[10:33]\n",
    "\n",
    "    answer_end = torch.from_numpy(np.array([int(i) for i in answer_end_pkl])).long()\n",
    "    answer_start = torch.from_numpy(np.array([int(i) for i in answer_start_pkl])).long()              \n",
    "    answer_start = torch.unsqueeze(answer_start, 1)\n",
    "    answer_end = torch.unsqueeze(answer_end, 1)\n",
    "\n",
    "    span_tensor = torch.cat((answer_start, answer_end), 1)\n",
    "    span_tensor = span_tensor[10:33]\n",
    "\n",
    "\n",
    "\n",
    "    batches = []\n",
    "    count = 0\n",
    "\n",
    "    while (True):\n",
    "        count = count + 1\n",
    "        if len(batches) == 0: # add more batches\n",
    "            if(count > 2):\n",
    "                break\n",
    "            batches = refill_batches(batches,batch_size,names, max_context_length, max_question_length,context_word_index_padded,question_word_index_padded,span_tensor)\n",
    "        if len(batches) == 0:\n",
    "            break\n",
    "\n",
    "        # Get next batch. These are all lists length batch_size\n",
    "        (context_word_index_padded_per_batch, question_word_index_padded_per_batch, span_tensor_per_batch) = batches.pop(0)\n",
    "\n",
    "        if(len(context_word_index_padded_per_batch) == 0):\n",
    "            break\n",
    "            \n",
    "        context_ids = np.array(context_word_index_padded_per_batch[0]) # shape (batch_size, context_len)\n",
    "        context_mask = (context_ids != 0).astype(np.int32) # shape (batch_size, context_len)\n",
    "        context_word_mask = torch.from_numpy(context_mask)\n",
    "\n",
    "        question_ids = np.array(question_word_index_padded_per_batch[0]) # shape (batch_size, context_len)\n",
    "        question_mask = (question_ids != 0).astype(np.int32) # shape (batch_size, context_len)\n",
    "        question_word_mask = torch.from_numpy(question_mask)\n",
    "\n",
    "        # Make into a Batch object\n",
    "        batch = Batch(names,context_word_index_padded_per_batch, context_word_mask, question_word_index_padded_per_batch, question_word_mask, span_tensor_per_batch)\n",
    "\n",
    "        yield batch\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refilling batches...\n",
      "Refilling batches took 0.00 seconds\n",
      "5\n",
      "tensor([[30, 30],\n",
      "        [54, 55],\n",
      "        [87, 87],\n",
      "        [37, 37],\n",
      "        [91, 91]])\n",
      "5\n",
      "tensor([[26, 26],\n",
      "        [62, 63],\n",
      "        [ 7,  8],\n",
      "        [15, 15],\n",
      "        [37, 37]])\n",
      "3\n",
      "tensor([[117, 117],\n",
      "        [163, 163],\n",
      "        [  3,   4]])\n",
      "5\n",
      "tensor([[79, 81],\n",
      "        [83, 83],\n",
      "        [92, 92],\n",
      "        [44, 45],\n",
      "        [79, 81]])\n",
      "5\n",
      "tensor([[ 54,  55],\n",
      "        [ 71,  72],\n",
      "        [ 14,  17],\n",
      "        [154, 154],\n",
      "        [163, 163]])\n"
     ]
    }
   ],
   "source": [
    "names = [\"train_context\",\"train_question\"]\n",
    "data_dir = \"E:\\\\Internships_19\\\\Internship(Summer_19)\\\\Q&A_Toolkit\\\\Dataset_analysis\\\\SQuAD\\\\\"\n",
    "max_context_length = 400\n",
    "max_question_length = 30\n",
    "batch_size = 5\n",
    "# object = Batch(names)\n",
    "for batch in get_batch_generator(data_dir, names, batch_size, max_context_length, max_question_length):\n",
    "    print(batch.batch_size)\n",
    "    print(batch.span_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
