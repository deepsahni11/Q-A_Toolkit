{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "class Highway_Maxout_Network(nn.Module):\n",
    "    def __init__(self, hidden_dim, maxout_pool_size, dropout_ratio):\n",
    "        super(Highway_Maxout_Network, self).__init__()\n",
    "        self.hidden_dim = hidden_dim # l\n",
    "        self.maxout_pool_size = maxout_pool_size # p\n",
    "\n",
    "        # Affine mapping from 5l ==> l\n",
    "        self.r = nn.Linear(5 * hidden_dim, hidden_dim, bias=False) \n",
    "       \n",
    "\n",
    "        # Affine mapping from 3*l ==> l*p\n",
    "        self.max_out_layer1 = nn.Linear(3 * hidden_dim, hidden_dim*maxout_pool_size)\n",
    "        \n",
    "        # Affine mapping from l ==> l*p\n",
    "        self.max_out_layer2 = nn.Linear(hidden_dim, hidden_dim*maxout_pool_size)\n",
    "       \n",
    "        # Affine mapping from 2*l ==> p\n",
    "        self.max_out_layer3 = nn.Linear(2 * hidden_dim, maxout_pool_size)\n",
    "\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, h_i, U, curr_mask_vector, index_i_minus_1, u_concatenated, mask_matrix, target=None):\n",
    "        batch_size, max_word_length , _ = list(U.size())\n",
    "\n",
    "        # concatenation of ( h_i of dimension = b x l ; u_concatenated of dimension = b x 4l ) along dimension 1 = gives b x 5l\n",
    "        # self.r(b x 5l) ====> b x l (change of vector space)\n",
    "        r = F.tanh(self.r(torch.cat((h_i.view(-1, self.hidden_dim), u_concatenated), 1)))  # b x 5l => b x l\n",
    "       \n",
    "\n",
    "        # hidden_dim = l\n",
    "        r_expanded = r.unsqueeze(1).expand(b, m, self.hidden_dim).contiguous()  # b x m x l\n",
    "\n",
    "        m_t1_input = torch.cat((U, r_expanded), 2).view(-1, 3*self.hidden_dim)  # b*m x 3l\n",
    "\n",
    "        m_t1_output = self.max_out_layer1(m_t1_input)  # b*m x p*l\n",
    "        \n",
    "        m_t1_output_resized, _ = m_t1_output.view(-1, self.hidden_dim, self.maxout_pool_size).max(2) # b*m x l\n",
    "\n",
    "        # m_t2_input =  m_t1_output_resized\n",
    "        m_t2_output = self.max_out_layer2(m_t1_output_resized)  # b*m x l*p\n",
    "        \n",
    "        m_t2_output_resized, _ = m_t2_output.view(-1, self.hidden_dim, self.maxout_pool_size).max(2)  # b*m x l\n",
    "\n",
    "        m_t3_input = torch.cat((m_t1_output_resized, m_t2_output_resized), 1)  # b*m x 2l\n",
    "        alpha = self.max_out_layer3(m_t3_input)  # b * m x p\n",
    "        alpha, _ = alpha.max(1)  # b*m\n",
    "        alpha = alpha.view(-1, m) # b x m\n",
    "\n",
    "        \n",
    "        alpha = alpha + mask_matrix  # b x m\n",
    "        \n",
    "        # alpha can be treated as probabilities that assign probability masses todifferent words in context. The word with\n",
    "        # maximum weight(probability) becomes the index(start/end)\n",
    "        alpha = F.log_softmax(alpha, 1)  # b x m\n",
    "        _, index_i = torch.max(alpha, dim=1) # b\n",
    "\n",
    "        if curr_mask_vector is None:\n",
    "            curr_mask_vector = (index_i == index_i) # b\n",
    "        else:\n",
    "            index_i = index_i*curr_mask_vector.long()  # b\n",
    "            index_i_minus_1 = index_i_minus_1*curr_mask_vector.long()  # b\n",
    "            curr_mask_vector = (index_i != index_i_minus_1) # b\n",
    "\n",
    "        step_loss = None\n",
    "\n",
    "        ## loss is only calculated only on that the predicted index at i_th time-step which varies \n",
    "        ## from the predicted index at time-step (i-1)_th time-step\n",
    "        if target is not None:\n",
    "            step_loss = self.loss(alpha, target)  # b\n",
    "            step_loss = step_loss * curr_mask_vector.float() # b\n",
    "\n",
    "        return index_i, curr_mask_vector, step_loss # all have dimension: b\n",
    "\n",
    "class DynamicDecoder(nn.Module):\n",
    "    def __init__(self, hidden_dim, maxout_pool_size, max_number_of_iterations, dropout_ratio):\n",
    "        super(DynamicDecoder, self).__init__()\n",
    "        self.max_number_of_iterations = max_number_of_iterations\n",
    "        \n",
    "        # batch_first = True\n",
    "        # Input: has a dimension of B * m * embedding_dim\n",
    "        # Function parameters: input_size, hidden_size, num_layers_of_LSTM = 1(here)\n",
    "        self.decoder = nn.LSTM(4 * hidden_dim, hidden_dim, 1, batch_first=True, bidirectional=False)\n",
    "        init_lstm_forget_bias(self.decoder)\n",
    "\n",
    "        self.maxout_start = MaxOutHighway(hidden_dim, maxout_pool_size, dropout_ratio)\n",
    "        self.maxout_end = MaxOutHighway(hidden_dim, maxout_pool_size, dropout_ratio)\n",
    "\n",
    "    def forward(self, U, document_word_sequence_mask, answer_start,answer_end):\n",
    "        batch_size, max_word_length, _ = list(U.size()) # U has dimension : B x m x 2l\n",
    "\n",
    "        curr_mask_start,  curr_mask_end = None, None\n",
    "        results_mask_start, results_start = [], []\n",
    "        results_mask_end, results_end = [], []\n",
    "        step_losses = []\n",
    "\n",
    "        mask_matrix = (1.0 - document_word_sequence_mask.float()) * (-1e30)\n",
    "        indices = torch.arange(0, batch_size, out=torch.LongTensor(batch_size))\n",
    "\n",
    "        # initialize start_zero, end_zero: these are the initial values of start and end indices\n",
    "        # start_i_minus_1 = the first index for the context/question \n",
    "        # end_i_minus_1 = the last index for the context/question \n",
    "        start_i_minus_1 = torch.zeros(batch_size, ).long()\n",
    "        end_i_minus_1 = torch.sum(document_word_sequence_mask, 1) - 1\n",
    "\n",
    "        \n",
    "\n",
    "        # After every iteration the hidden and current state \n",
    "        # at t = length of the sequence (for the one-directional lstm) will\n",
    "        # be returned by the lstm\n",
    "        # the hidden_state_i(h_i) will serve as an input to next lstm\n",
    "        hidden_and_current_state_i = None\n",
    "        start_target = None\n",
    "        end_target = None\n",
    "        \n",
    "        # this sets the start and end target (ie. the y_label) for an answer\n",
    "        if answer_start is not None:\n",
    "            start_target = answer_start\n",
    "            end_target = answer_end\n",
    "            \n",
    "        # this is just an initialization of u_start\n",
    "        # u_start_i_minus_1 is essentially u_start_zero outside the loop\n",
    "        u_start_i_minus_1 = U[indices, start_i_minus_1, :]  # b x 2l\n",
    "        \n",
    "        # Why do we need an iterative procedure to predict the start and end indices for na answer ? \n",
    "        # Solution: there may exist several intuitive answer spans within the document, each corresponding to a\n",
    "        # local maxima. An iterative technique to select an answer span by alternating between\n",
    "        # predicting the start point and predicting the end point. This iterative procedure allows the model to\n",
    "        # recover from initial local maxima corresponding to incorrect answer spans.\n",
    "        for _ in range(self.max_number_of_iterations):\n",
    "            u_end_i_minus_1 = U[indices, end_i_minus_1, :]  # b x 2l\n",
    "            \n",
    "            # u_concatenated is fed to the lstm\n",
    "            u_concatenated = torch.cat((u_start_i_minus_1, u_end_i_minus_1), 1)  # b x 4l\n",
    "\n",
    "            # the hidden_and_current_state_i = h_i,c_i are essentially hidden and current cell states \n",
    "            # for t = length of the sequence (for the one-directional lstm) after every iteration\n",
    "            lstm_output, hidden_and_current_state_i = self.decoder(u_concatenated.unsqueeze(1), hidden_and_current_state_i)\n",
    "            h_i, c_i = hidden_and_current_state_i\n",
    "\n",
    "            # Inputs to the Highway_Maxout_Network(to find start index) are: hidden_state_i(h_i), start_i_minus_1(index), u_concatenated ==>(u_start_i_minus_1;u_end_i_minus_1) \n",
    "            start_i_minus_1, curr_mask_start, step_loss_start = self.maxout_start(h_i, U, curr_mask_start, start_i_minus_1,\n",
    "                                                                u_concatenated, mask_matrix, start_target)\n",
    "            u_start_i_minus_1 = U[indices, start_i_minus_1, :]  # b x 2l\n",
    "            u_concatenated = torch.cat((start_i_minus_1, u_end_i_minus_1), 1)  # b x 4l\n",
    "\n",
    "            # Inputs to the Highway_Maxout_Network(to find end index) are: hidden_state_i(h_i), end_i_minus_1(index), u_concatenated ==>(u_start_i_minus_1;u_end_i_minus_1) \n",
    "            end_i_minus_1, curr_mask_end, step_loss_end = self.maxout_end(h_i, U, curr_mask_end, end_i_minus_1,\n",
    "                                                              u_concatenated, mask_matrix, end_target)\n",
    "\n",
    "            # we minimize the cumulative softmax cross entropy of the start and end points across all iterations.\n",
    "            if answer_start is not None:\n",
    "                step_loss = step_loss_start + step_loss_end\n",
    "                step_losses.append(step_loss)\n",
    "\n",
    "            \n",
    "            results_mask_start.append(curr_mask_start) # appends all the curr_mask_start ==> dimension: b x num_iterations\n",
    "            results_start.append(start_i_minus_1) # appends all the start_indexes ==> dimension: b x num_iterations\n",
    "            results_mask_end.append(curr_mask_end) # appends all the curr_mask_end ==> dimension: b x num_iterations\n",
    "            results_end.append(end_i_minus_1) # appends all the end_indexes ==> dimension: b x num_iterations\n",
    "\n",
    "        result_pos_start = torch.sum(torch.stack(results_mask_start, 1), 1).long()\n",
    "        result_pos_start = result_pos_start - 1\n",
    "        index_start = torch.gather(torch.stack(results_start, 1), 1, result_pos_start.unsqueeze(1)).squeeze()\n",
    "\n",
    "        result_pos_end = torch.sum(torch.stack(results_mask_end, 1), 1).long()\n",
    "        result_pos_end = result_pos_end - 1\n",
    "        index_end = torch.gather(torch.stack(results_end, 1), 1, result_pos_end.unsqueeze(1)).squeeze()\n",
    "\n",
    "        loss = None\n",
    "\n",
    "        if answer_start is not None:\n",
    "            sum_losses = torch.sum(torch.stack(step_losses, 1), 1)\n",
    "            batch_avg_loss = sum_losses / self.max_number_of_iterations\n",
    "            loss = torch.mean(batch_avg_loss)\n",
    "\n",
    "        return loss, index_start, index_end\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
